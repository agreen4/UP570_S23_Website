[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neighborhood Analysis",
    "section": "",
    "text": "Neighborhood Analysis Spring 2023\nLearn to tell stories about neighborhoods for decision-making, public deliberation, and accountability using  and principles of reproducible data analysis.\n  \n\n\n\nInstructor\n\n   Dr. Andrew J. Greenlee\n   M210 Temple Buell Hall\n   agreen4@illinois.edu\n   Github\n   urbprof\n   https://fediscience.org/@urbprof\n   Schedule an appointment\n\n\n\nTeaching Assistant\n\n   Ouafa Benkraouda\n   ouafab2@illinois.edu\n   Github\n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   11:00 AM - 12:20 PM\n   Temple Buell Hall 223\n   Slack"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Neighborhood Analysis! We are excited to teach and learn with you this semester."
  },
  {
    "objectID": "syllabus/syllabus.html#course-overview-and-objectives",
    "href": "syllabus/syllabus.html#course-overview-and-objectives",
    "title": "Syllabus",
    "section": "Course Overview and Objectives",
    "text": "Course Overview and Objectives\nThis course teaches techniques for analyzing the demographic, economic, physical, and social conditions that exist at the neighborhood and local government scale. While our focus will be on analyzing current conditions, we will also learn how to tell stories about neighborhood change, and will learn how to project and forecast future trends. We will learn how to describe community characteristics with small area census data, work with local administrative data, and will think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nBy the end of this course, we will:\n\nBecome familiar with common sources of information used to describe neighborhoods and neighborhood characteristics;\nLearn how to use R, RStudio, and Github to create reproducible analysis;\nLearn how to work collaboratively to tell compelling stories for deliberation and decision-making."
  },
  {
    "objectID": "syllabus/syllabus.html#course-format-and-expectations",
    "href": "syllabus/syllabus.html#course-format-and-expectations",
    "title": "Syllabus",
    "section": "Course Format and Expectations",
    "text": "Course Format and Expectations\nWe will learn together using a combination of direct instruction, lab sessions, and tutorials. We will use reading and reflection to help us contextualize our understanding of neighborhoods, however, our focus will be on learning by doing. Our class will meet in person twice per week - Tuesdays will typically be devoted to introducing and discussing new information, and Thursdays will typically be devoted to labs and independent work.\nUnless otherwise noted, please plan on bringing to class a computer that will run R and RStudio (available for PC/Mac/Linux) and for which you have administrative privileges.\n\nWhat to Expect from Me\n\nOffice Hours and Availability: Please make an appointment to speak one-on-one using my Calendly page. Of course, I’m also happy to chat either before or after our class if i’m available.\nEmail: The easiest way to communicate with me outside of class is via email. I try to respond to emails sent during the week within 24 hours. Emails sent over the weekend will receive a response within 48 hours. If you don’t hear from me after that amount of time, it’s okay to nudge me to respond.\nTroubleshooting: Plan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. The Resources page provides thoughts and resources for troubleshooting. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\n\n\n\nWhat I Expect From You\n\nBe Present: I expect that you’ll engage fully in our course sessions and in our class community.\nActively Support Each Other: I expect that each of you will take on individual leadership roles within our class, that includes actively supporting our learning community over the course of the semester. This class assumes collaboration and sharing as part of our learning model.\nRead with Care: This course focuses on learning by doing, however, there are important details contained within the documentation on our course website and within reading selections. Details matter in this class - be intentional about reading carefully and completely important course documents (including this syllabus).\nAct with Integrity: I expect that you will act with integrity in all that you do in this class. The class contract grading system places trust in you to not just meet the nominal standards contained within the contract, but to push yourself to produce your best work.\nSeek Balance: I expect that you’ll actively work to find balance between the many demands in your life. This means budgeting adequate time to engage fully in our course but also budgeting time for adequate rest and sleep, exercise, and other actions that support your mental and physical health.\n\n\n\nCourse Prerequisites\nUP 570: Neighborhood Analysis is open to students with graduate-level standing. The course assumes that you have previously taken UP 517: Data Science for Planners or have substantial experience with manipulating data in R.\nPlease talk with me if you have any questions regarding whether this course is right for you."
  },
  {
    "objectID": "syllabus/syllabus.html#assignments-evaluation-and-grading-contract",
    "href": "syllabus/syllabus.html#assignments-evaluation-and-grading-contract",
    "title": "Syllabus",
    "section": "Assignments, Evaluation, and Grading Contract",
    "text": "Assignments, Evaluation, and Grading Contract\nAlongside our learning of techniques and perspectives on working with small area data, a primary goal of this class is to learn to think critically about the ways in which those data and our analysis embody power and exist within power relationships. One implicit goal of our learning this semester is to embody through our work alternative ways to share power and create meaning.\n\nAssignments\nAssignments You will find detailed information on assignments in the Assignments section."
  },
  {
    "objectID": "syllabus/syllabus.html#honor-code",
    "href": "syllabus/syllabus.html#honor-code",
    "title": "Syllabus",
    "section": "Honor Code",
    "text": "Honor Code\nThe Illinois Student Code states: “It is the responsibility of the student to refrain from infractions of academic integrity, from conduct that may lead to suspicion of such infractions, and from conduct that aids others in such infractions.” Note that you are subject to the Honor Code, as well as procedures for addressing violations to the Code, regardless of whether you have read it and understand it. According to the Code, “ignorance is no excuse.”\nTo meet this standard in this course, note the following: in written work, all ideas (as well as data or other information) that are not your own must be cited. Note that ideas that require citation may not have been published or written down anywhere. While you are free—and indeed encouraged—to discuss assignments with your peers, all of your analysis, and writing should be your own. The consequence for violating these expectations may include receiving no credit for the assignment in question, and may include automatic failure of the course.\n\n\n\n\n\n\nA Simple Standard\n\n\n\nPut simply, don’t cheat and give credit where credit is due."
  },
  {
    "objectID": "syllabus/syllabus.html#learning-environment-and-support",
    "href": "syllabus/syllabus.html#learning-environment-and-support",
    "title": "Syllabus",
    "section": "Learning Environment and Support",
    "text": "Learning Environment and Support\n\nOur Learning Environment\nThe Department of Urban and Regional Planning is committed to maintaining a learning environment that is rooted in the goals and responsibilities of professional planners. By enrolling in a class offered by the Department of Urban and Regional Planning, students agree to be responsible for maintaining an atmosphere of mutual respect in all activities, including lectures, discussions, labs, projects, and extracurricular opportunities. See Student Code Article 1-Student Rights and Responsibilities, Part 1. Student Rights: §1-102.\n\n\nOur Class Environment\nAs part of our classroom and university community, you have an obligation to do the following:\n\nIf you feel ill, do not come to class. If you test positive or have an exposure that requires testing or quarantine, do not come to class.\nPlease be respectful of all members of our learning community and their decisions regarding health and safety precautions."
  },
  {
    "objectID": "syllabus/syllabus.html#your-wellness",
    "href": "syllabus/syllabus.html#your-wellness",
    "title": "Syllabus",
    "section": "Your Wellness",
    "text": "Your Wellness\nWellness at Illinois: Throughout the semester, you may need assistance coping with emotional, interpersonal, or academic concerns. wellness.illinois.edu is a good resource to identify help for yourself or others who may need assistance. Please do not hesitate to reach out or request assistance.\n\nLearning R and Your Wellness\nAs you have learned throughout your journey learning R, you will face challenges related to working with the software this semester. That’s a given, and is an expected part of learning in this class. Part of the goal is to teach you how to understand the intentionality and logic behind the software so that you can anticipate where errors are likely to occur.\n\n\n\n\n\n\nExperiential Learning\n\n\n\nThe only way to do this is to encounter errors - and there will be many!\n\n\nYou are entering an intermediate to advanced stage of learning a new language, it’s grammar, and its application. While this will be frustrating at times, there is a major payoff in the capacity you will gain in analytic skills and problem-solving. This payoff will come slowly over time - do not expect it to come easily.\n\nYou are not alone in this struggle. In addition to your classmates and others who are going through the same thing, there is a large R user community, and lots of existing documentation and troubleshooting. Any problem you will encounter has likely been encountered and addressed before.\n\n\n\n\n\n\nTip\n\n\n\nWhen I run into an error, after an initial check for simple issues like closing parentheses and spelling errors, I copy and paste error codes directly into a web search to see how others have dealt with similar problems. I encourage you to do the same.\n\n\nYou got this, and there will be a payoff, so long as you use the tools consistently - I promise!"
  },
  {
    "objectID": "syllabus/syllabus.html#were-here-for-you",
    "href": "syllabus/syllabus.html#were-here-for-you",
    "title": "Syllabus",
    "section": "We’re Here For You",
    "text": "We’re Here For You\nWe’ve been living in particularly abnormal times for the last few years - while for many, things are improving, it would be irresponsible to expect that teaching and learning would occur “normally” right now. We continue to teach and learn under emergency circumstances.\nAs you face challenges this semester (and beyond) I need you to communicate with me, either during our course sessions or individually. You can schedule an appointment with me at your convenience via my Calendly page. I promise to listen, to be a resource, and to help in any way that I can - if I can’t help you, I will find someone who can."
  },
  {
    "objectID": "assignments/01_place_selection.html",
    "href": "assignments/01_place_selection.html",
    "title": "Place Selection Memorandum",
    "section": "",
    "text": "A very important early task in Neighborhood Analysis is to select the place that you will spend time examining in depth this semester. The place you select will be the location that you focus your social, economic, and demographic analysis on. Given the structure of this class, you need to choose a location in the United States.\nYour work will include thinking about how that place fits within its larger “region.” What do we mean by “place”? It might be a city or town—the City of Champaign—or even twin cities—the cities of Champaign and Urbana. It could also be a county. For our purposes, it is not an entire metropolitan area or a larger macro region (such as the Great Plains or Mississippi Delta)."
  },
  {
    "objectID": "assignments/01_place_selection.html#goals",
    "href": "assignments/01_place_selection.html#goals",
    "title": "Place Selection Memorandum",
    "section": "Goals",
    "text": "Goals\n\nIdentify a particular place that you will focus your analytic work on over the course of the semester\nThink critically about place, geography, and audience"
  },
  {
    "objectID": "assignments/01_place_selection.html#place-selection-considerations",
    "href": "assignments/01_place_selection.html#place-selection-considerations",
    "title": "Place Selection Memorandum",
    "section": "Place Selection Considerations",
    "text": "Place Selection Considerations\n\nBig places—the cities of Chicago, Los Angeles, New York—are interesting. They are socioeconomically, racially, and ethnically diverse and it is usually not hard to find “stories in the data” with respect to neighborhood trends. However, large cities are very complex. Properly understanding the past, present, and future of such places and their neighborhoods can be a herculean task. Unless you are very familiar with a big place, we suggest focusing on a smaller place.\nMany of the techniques you will learn use methods that draw on data available at the census tract level, as census tracts are frequently used as (imperfect) proxies for neighborhoods. As you work to select your place, it may be useful to identify the number of census tracts that overlap. As a rule of thumb, your place should have no fewer than 15 tracts. More is certainly ok, but let’s talk if you are considering a place with fewer than 15 tracts.\nConsider selecting a place in a broader area of particular policy concern, such as an economically distressed region (central Appalachia, the Mississippi Delta, or the Black Belt). Many communities in the Great Plains and Mountain states are not distressed by conventional measures but they face significant out-migration and population decline, which introduce their own challenges at the local level. Places around international borders can also be interesting to examine.\nConsider selecting a place within driving distance of Champaign-Urbana (although you may not select Champaign County or it’s constituent cities as your place). Making a site visit to the county and its principal urban centers could help you understand its conditions and context better, giving you the ability to write about it with more authority.\nHaving some personal experience in the place you select can be helpful. Many students select their hometown or other community/county in which they have lived, provided the criteria above are satisfied. Some students also select places there are interested in living or working in.\nIslands and territories (such as Puerto Rico and Hawaii) and remote areas (such as Alaska) are interesting, but geography and data availability can make them challenging places to analyze. Students have analyzed all of these places in recent years- but with extra effort needed to address data gaps and limitations. You may want to explore these issues before committing to one of these places.\nIf you have any questions about the places you are considering, or you wish to talk through your choices, please talk with me."
  },
  {
    "objectID": "assignments/01_place_selection.html#your-place-selection-memorandum",
    "href": "assignments/01_place_selection.html#your-place-selection-memorandum",
    "title": "Place Selection Memorandum",
    "section": "Your Place Selection Memorandum",
    "text": "Your Place Selection Memorandum\nPrepare a memorandum of around 500 words that identifies the place that you propose focusing your analysis on this semester.\nAt a minimum, the narrative associated with your memorandum should address the following:\n\nWhat is the name of the place which you have selected, and where is it located?\nWhat is the specific geography associated with this place (city, county, etc.)?\nWhat characteristics of this place make its neighborhoods important to examine?\nBased upon your preliminary background research, what types of policy issues may be important to examine through your analysis?"
  },
  {
    "objectID": "assignments/01_place_selection.html#submission-instructions",
    "href": "assignments/01_place_selection.html#submission-instructions",
    "title": "Place Selection Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/02_place_background.html",
    "href": "assignments/02_place_background.html",
    "title": "Place Background Memorandum",
    "section": "",
    "text": "In your previous memorandum, you identified a place to focus your analysis on, and provided some preliminary justification for your choice. In this memorandum, you will leverage additional research to identify priority populations, issues, and neighborhoods, and also to lay out an analysis roadmap that will guide your subsequent analysis."
  },
  {
    "objectID": "assignments/03_population.html",
    "href": "assignments/03_population.html",
    "title": "Population Memorandum",
    "section": "",
    "text": "Your prior two memorandums focused on using secondary sources to scope analysis for your place. In this memorandum you will draw upon multiple sources of available quantitative data to describe the population of your place and their characteristics. This background on the population will help you to identify a specific policy issue or other questions which you will analyze in your next memorandum."
  },
  {
    "objectID": "assignments/04_policy.html",
    "href": "assignments/04_policy.html",
    "title": "Policy Memorandum",
    "section": "",
    "text": "In your prior memorandums you developed a preliminary narrative around your place based upon the characteristics of the population. Simultaneously, in your labs, you’ve been learning about a range of analytical strategies. In this lab, you will identify a specific policy-based problem and will develop and implement an appropriate analysis to better inform the problem. This is the final sub-element of your final policy report which will draw from your prior writing and analysis to present a polished report for your place."
  },
  {
    "objectID": "assignments/05_term.html",
    "href": "assignments/05_term.html",
    "title": "Term Assignment",
    "section": "",
    "text": "For your term assignment, you will independently apply the skills and principles of data analysis we’ve learned over the course of the semester to produce a policy-relevant analysis and report. Building upon your memorandums, produce a publication ready policy report for your place as well as complimentary reproducible documentation.\nAt a minimum, your report to address the following:\n\nContext and Background: Drawing upon secondary sources, craft a narrative context and background for your analysis. This may include in-depth profiling of the neighborhood(s) of interest or may involve broader contextualizing drawing from the history and background of your place. Your narrative does not have to be long, but it should help to contextualize the questions you are engaging with and the methods you use to engage those questions.\nMethods and Approach: Describe your approach to analysis - what is the specific question or problem you are taking on? What precedents or work that others have done is helping to inform your approach? Why is the selected geographic scale of your analysis appropriate?\nData Sources: Describe the data source(s) that you are using for your analysis. Where are these data coming from? What are each contributing? What are the specific indicators you are using from these data?\nData Description, Analysis and Interpretation: Please describe the analysis which you’ve done with the data, as well as your written interpretation based upon your analysis.\nImplications for Policy and Impact: Share your thoughts on how your findings can or should influence policy or otherwise have an impact within your place. You may also wish to provide some thoughts on how you or others may expand upon this work in the future or describe a set of future questions or analyses that build upon what you’ve done.\n\nThe actual structure of your report is up to you, and will likely vary depending upon your place and the questions you seek to engage."
  },
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Neighborhood Analysis! We are excited to teach and learn with you this semester."
  },
  {
    "objectID": "syllabus/index.html#course-overview-and-objectives",
    "href": "syllabus/index.html#course-overview-and-objectives",
    "title": "Syllabus",
    "section": "Course Overview and Objectives",
    "text": "Course Overview and Objectives\nThis course teaches techniques for analyzing the demographic, economic, physical, and social conditions that exist at the neighborhood and local government scale. While our focus will be on analyzing current conditions, we will also learn how to tell stories about neighborhood change, and will learn how to project and forecast future trends. We will learn how to describe community characteristics with small area census data, work with local administrative data, and will think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nBy the end of this course, we will:\n\nBecome familiar with common sources of information used to describe neighborhoods and neighborhood characteristics;\nLearn how to use R, RStudio, and Github to create reproducible analysis;\nLearn how to work collaboratively to tell compelling stories for deliberation and decision-making."
  },
  {
    "objectID": "syllabus/index.html#course-format-and-expectations",
    "href": "syllabus/index.html#course-format-and-expectations",
    "title": "Syllabus",
    "section": "Course Format and Expectations",
    "text": "Course Format and Expectations\nWe will learn together using a combination of direct instruction, lab sessions, and tutorials. We will use reading and reflection to help us contextualize our understanding of neighborhoods, however, our focus will be on learning by doing. Our class will meet in person twice per week - Tuesdays will typically be devoted to introducing and discussing new information, and Thursdays will typically be devoted to labs and independent work.\nUnless otherwise noted, please plan on bringing to class a computer that will run R and RStudio (available for PC/Mac/Linux) and for which you have administrative privileges.\n\nWhat to Expect from Your Instructors\n\nOffice Hours and Availability: We are available to speak one on one - you can book appointments with us via our Calendly pages:\n\nAndrew Greenlee\nOuafa Benkraouda\n\nOf course, we are happy to chat either before or after class if we are available.\nEmail: The easiest way to communicate with me outside of class is via email. I try to respond to emails sent during the week within 24 hours. Emails sent over the weekend will receive a response within 48 hours. If you don’t hear from me after that amount of time, it’s okay to nudge me to respond.\nTroubleshooting: Plan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. The Resources page provides thoughts and resources for troubleshooting. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\n\n\n\nWhat We Expect From You\n\nBe Present: We expect that you’ll engage fully in our course sessions and in our class community.\nActively Support Each Other: We expect that each of you will take on individual leadership roles within our class, that includes actively supporting our learning community over the course of the semester. This class assumes collaboration and sharing as part of our learning model.\nRead with Care: This course focuses on learning by doing, however, there are important details contained within the documentation on our course website and within reading selections. Details matter in this class - be intentional about reading carefully and completely important course documents (including this syllabus).\nAct with Integrity: We expect that you will act with integrity in all that you do in this class. The class contract grading system places trust in you to not just meet the nominal standards contained within the contract, but to push yourself to produce your best work.\nSeek Balance: We expect that you’ll actively work to find balance between the many demands in your life. This means budgeting adequate time to engage fully in our course but also budgeting time for adequate rest and sleep, exercise, and other actions that support your mental and physical health.\n\n\n\nCourse Prerequisites\nUP 570: Neighborhood Analysis is open to students with graduate-level standing. The course assumes that you have previously taken UP 517: Data Science for Planners or have substantial experience with manipulating data in R. Our first few course sessions will focus on ensuring that we are all familiar with some of the basic workflows and methods which we’ll make use of over the course of the semester.\nPlease talk with me if you have any questions regarding whether this course is right for you."
  },
  {
    "objectID": "syllabus/index.html#assignments-evaluation-and-grading-contract",
    "href": "syllabus/index.html#assignments-evaluation-and-grading-contract",
    "title": "Syllabus",
    "section": "Assignments, Evaluation, and Grading Contract",
    "text": "Assignments, Evaluation, and Grading Contract\nAlongside our learning of techniques and perspectives on working with small area data, a primary goal of this class is to learn to think critically about the ways in which those data and our analysis embody power and exist within power relationships. One implicit goal of our learning this semester is to embody through our work alternative ways to share power and create meaning.\n\nAssignments\nAssignments You will find detailed information on assignments in the Assignments section."
  },
  {
    "objectID": "syllabus/index.html#honor-code",
    "href": "syllabus/index.html#honor-code",
    "title": "Syllabus",
    "section": "Honor Code",
    "text": "Honor Code\nThe Illinois Student Code states: “It is the responsibility of the student to refrain from infractions of academic integrity, from conduct that may lead to suspicion of such infractions, and from conduct that aids others in such infractions.” Note that you are subject to the Honor Code, as well as procedures for addressing violations to the Code, regardless of whether you have read it and understand it. According to the Code, “ignorance is no excuse.”\nTo meet this standard in this course, note the following: in written work, all ideas (as well as data or other information) that are not your own must be cited. Note that ideas that require citation may not have been published or written down anywhere. While you are free—and indeed encouraged—to discuss assignments with your peers, all of your analysis, and writing should be your own. The consequence for violating these expectations may include receiving no credit for the assignment in question, and may include automatic failure of the course.\n\n\n\n\n\n\nA Simple Standard\n\n\n\nPut simply, don’t cheat and give credit where credit is due."
  },
  {
    "objectID": "syllabus/index.html#learning-environment-and-support",
    "href": "syllabus/index.html#learning-environment-and-support",
    "title": "Syllabus",
    "section": "Learning Environment and Support",
    "text": "Learning Environment and Support\n\nOur Learning Environment\nThe Department of Urban and Regional Planning is committed to maintaining a learning environment that is rooted in the goals and responsibilities of professional planners. By enrolling in a class offered by the Department of Urban and Regional Planning, students agree to be responsible for maintaining an atmosphere of mutual respect in all activities, including lectures, discussions, labs, projects, and extracurricular opportunities. The University of Illinois Student Code should be considered part of this syllabus. See in particular Student Code Article 1-Student Rights and Responsibilities, Part 1. Student Rights: §1-102.\n\n\nOur Class Environment\nAs part of our classroom and university community, you have an obligation to do the following:\n\nAttend all class sessions if you are feeling well.\nIf you feel ill, do not come to class.\nIf you test positive for covid or have an exposure that requires testing or quarantine, do not come to class.\nPlease be respectful of all members of our learning community and their decisions regarding health and safety precautions.\n\n\n\nAccomodations for Students with Disabilities\nIf you need accommodations for any sort of disability, please make an office hours appointment so we can discuss your needs and ways I can support your learning. To ensure that disability-related concerns are properly addressed, students who require assistance to participate in this class should contact Disability Resources and Educational Services (DRES). DRES provides students with academic accommodations, access, and support services. To contact DRES you may visit 1207 S. Oak St., Champaign, call 333-4603 (V/TDD), or e-mail disability@illinois.edu.\n\n\nSexual Misconduct Reporting Obligation\nThe University of Illinois is committed to combating sexual misconduct. Faculty and staff members are required to report any instances of sexual misconduct to the University’s Title IX and Disability Office. In turn, an individual with the Title IX and Disability Office will provide information about rights and options, including accommodations, support services, the campus disciplinary process, and law enforcement options.\nA list of the designated University employees who, as counselors, confidential advisors, and medical professionals, do not have this reporting responsibility and can maintain confidentiality, can be found here. Other information about resources and reporting is available at wecare.illinois.edu.\n\n\nMental Health\nThe University of Illinois offers a variety of confidential services including individual and group counseling, crisis intervention, psychiatric services, and specialized screenings which are covered through the Student Health Fee. If you or someone you know experiences mental health concerns, please contact or visit any of the University’s resources provided below. Getting help is a smart and courageous thing to do for yourself and for those who care about you.\n\nCounseling Center: (217) 333-3704\nMcKinley Health Center: (217) 333-2700\n988 Suicide and Crisis Lifeline: (800) 273-8255\nRosecrance Crisis Line (217) 359-4141\n\nIf you are in immediate danger, call 911."
  },
  {
    "objectID": "syllabus/index.html#your-wellness",
    "href": "syllabus/index.html#your-wellness",
    "title": "Syllabus",
    "section": "Your Wellness",
    "text": "Your Wellness\nWellness at Illinois: Throughout the semester, you may need assistance coping with emotional, interpersonal, or academic concerns. wellness.illinois.edu is a good resource to identify help for yourself or others who may need assistance. Please do not hesitate to reach out or request assistance.\n\nLearning R and Your Wellness\nProblem solving is a major part of being a coder - you will face challenges related to working with the software this semester. That’s a given, and is an expected part of learning in this class. Part of the goal is to teach you how to understand the intentionality and logic behind the software so that you can anticipate where errors are likely to occur.\n\n\n\n\n\n\nExperiential Learning\n\n\n\nThe only way to do this is to encounter errors - and there will be many!\n\n\nYou are entering an intermediate to advanced stage of learning a new language, it’s grammar, and its application. While this will be frustrating at times, there is a major payoff in the capacity you will gain in analytic skills and problem-solving. This payoff will come slowly over time - do not expect it to come easily.\n\nYou are not alone in this struggle. In addition to your classmates and others who are going through the same thing, there is a large R user community, and lots of existing documentation and troubleshooting. Any problem you will encounter has likely been encountered and addressed before.\n\n\n\n\n\n\nTip\n\n\n\nWhen I run into an error, after an initial check for simple issues like closing parentheses and spelling errors, I copy and paste error codes directly into a web search to see how others have dealt with similar problems. I encourage you to do the same.\n\n\nYou got this, and there will be a payoff, so long as you use the tools consistently - I promise!\n\n\nWe’re Here For You\nWe’ve been living in particularly abnormal times for the last few years - while for many, things are improving, it would be irresponsible to expect that teaching and learning would occur “normally” right now. We continue to teach and learn under emergency circumstances.\nAs you face challenges this semester (and beyond) I need you to communicate with me, either during our course sessions or individually. You can schedule an appointment with me at your convenience via my Calendly page. I promise to listen, to be a resource, and to help in any way that I can - if I can’t help you, I will find someone who can."
  },
  {
    "objectID": "howto/getstarted.html",
    "href": "howto/getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "Here you will find a range of “how to” resources and code base to help you advance tasks in Neighborhood Analysis."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": "Schedule",
    "section": "",
    "text": "Please find descriptions for our course sessions here - you will also find resources that will help you prepare for each session."
  },
  {
    "objectID": "schedule/index.html#course-introduction",
    "href": "schedule/index.html#course-introduction",
    "title": "Schedule",
    "section": "Course Introduction",
    "text": "Course Introduction\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 1\nSession 1\nJanuary 18\nCourse Introduction\n\n\nWeek 2\nSession 2\nJanuary 23\nWhat is a Neighborhood?\n\n\nWeek 2\nSession 3\nJanuary 25\nBuilding a Data Pipeline\n\n\nWeek 3\nSession 4\nJanuary 30\nSharing Your Work\n\n\nWeek 3\nSession 5\nFebruary 1\nEarning your Learner’s Permit\n\n\nWeek 3\n\nFebruary 3\nPlace Selection Memorandum\n\n\nWeek 4\nSession 6\nFebruary 6\nDescribing Places\n\n\nWeek 4\nSession 7\nFebruary 8\nDescribing Places"
  },
  {
    "objectID": "schedule/index.html#strategies-for-analysis",
    "href": "schedule/index.html#strategies-for-analysis",
    "title": "Schedule",
    "section": "Strategies for Analysis",
    "text": "Strategies for Analysis\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 5\nSession 8\nFebruary 13\nPopulation and the Census\n\n\nWeek 5\nSession 9\nFebruary 15\nPopulation and the Census\n\n\nWeek 5\n\nFebruary 17\nPlace Background Memorandum\n\n\nWeek 6\nSession 10\nFebruary 20\nPopulation Projections\n\n\nWeek 6\nSession 11\nFebruary 22\nPopulation Projections\n\n\nWeek 7\nSession 12\nFebruary 27\nSegregation\n\n\nWeek 7\nSession 13\nMarch 1\nSegregation\n\n\nWeek 7\n\nMarch 3\nPopulation Memorandum\n\n\nWeek 8\nSession 14\nMarch 6\nNeighborhood Change\n\n\nWeek 8\nSession 15\nMarch 8\nNeighborhood Change\n\n\nWeek 9\n\nMarch 13\nSpring Break\n\n\nWeek 9\n\nMarch 15\nSpring Break\n\n\nWeek 10\nSession 16\nMarch 20\nPlace Opportunity\n\n\nWeek 10\nSession 17\nMarch 22\nPlace Opportunity\n\n\nWeek 10\n\nMarch 24\nPolicy Memorandum\n\n\nWeek 11\nSession 18\nMarch 27\nTransportation Equity\n\n\nWeek 11\nSession 19\nMarch 29\nTransportation Equity\n\n\nWeek 12\nSession 20\nApril 3\nNeighborhood Health Equity\n\n\nWeek 12\nSession 21\nApril 5\nNeighborhood Health Equity\n\n\nWeek 13\nSession 22\nApril 10\nField Observation\n\n\nWeek 13\nSession 23\nApril 12\nField Observation\n\n\nWeek 13\n\nApril 14\nFinal Assignment First Draft"
  },
  {
    "objectID": "schedule/index.html#course-wrap-up",
    "href": "schedule/index.html#course-wrap-up",
    "title": "Schedule",
    "section": "Course Wrap-Up",
    "text": "Course Wrap-Up\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 14\nSession 24\nApril 17\nFinal Project Peer Review\n\n\nWeek 14\nSession 25\nApril 19\nFinal Presentations\n\n\nWeek 15\nSession 26\nApril 24\nIndependent Work and Advising\n\n\nWeek 15\nSession 27\nApril 26\nIndependent Work and Advising\n\n\nWeek 16\nSession 28\nMay 1\nFinal Presentations\n\n\nWeek 16\nSession 29\nMay 3\nFinal Presentations\n\n\nWeek 17\n\nMay 10\nFinal Assignment"
  },
  {
    "objectID": "schedule/01_introduction.html",
    "href": "schedule/01_introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to UP 570: Neighborhood Analysis! This session serves as our introduction to the class and an opportunity to start exploring how we will work together over the course of the semester. We will introduce ourselves and discuss how we will learn together. We’ll also think through two foundational questions:\n\nWhat is a neighborhood?\nWhy do neighborhoods matter?"
  },
  {
    "objectID": "schedule/01_introduction.html#before-class",
    "href": "schedule/01_introduction.html#before-class",
    "title": "Course Introduction",
    "section": "Before Class",
    "text": "Before Class\n\nBookmark this website https://up570s23.netlify.app so you can access it easily.\nJoin our course Slack Channel\nRead the course syllabus and bring questions with you.\nComplete the course introductory survey.\nJoin the Data Science in Planning (DSIP) Listserv (dsip-durp-l@lists.illinois.edu) where members of our community can share opportunities things of interest.\n\nChoose the address with which you want to subscribe to the list - you should choose an address you can check frequently.\nSend a message to sympa@lists.illinois.edu from the address you want to subscribe to the list.\nIn the subject line of your message, type in subscribe dsip-durp-l Firstname Name (replace Firstname and Name with your preferred first name and last name).\nLeave the message body blank.\nAfter this you will recieve a message telling you whether your request was accepted or not."
  },
  {
    "objectID": "schedule/01_introduction.html#reflect",
    "href": "schedule/01_introduction.html#reflect",
    "title": "Course Introduction",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are your goals for taking this class? What would you like to learn about neighborhoods?\nWhat matters about neighborhoods? How have neighborhoods shaped your life?\nWhat types of stories do we tend to tell about neighborhoods? How do these stories contextualize how neighborhoods “fit” within cities and their regions?"
  },
  {
    "objectID": "schedule/01_introduction.html#slides",
    "href": "schedule/01_introduction.html#slides",
    "title": "Course Introduction",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/01_introduction.html#resources-for-further-exploration",
    "href": "schedule/01_introduction.html#resources-for-further-exploration",
    "title": "Course Introduction",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nComprehensive Development Plan for Champaign - Urbana (1950)"
  },
  {
    "objectID": "slides/01_CourseIntroduction.html#welcome-to-up-570-neighborhood-analysis",
    "href": "slides/01_CourseIntroduction.html#welcome-to-up-570-neighborhood-analysis",
    "title": "Course Introduction",
    "section": "Welcome to UP 570: Neighborhood Analysis",
    "text": "Welcome to UP 570: Neighborhood Analysis"
  },
  {
    "objectID": "schedule/02_neighborhood.html",
    "href": "schedule/02_neighborhood.html",
    "title": "What is a Neighborhood?",
    "section": "",
    "text": "In this session, we’ll explore the many ways in which the concept of neighborhoods are used in various areas of urban planning and governance. We will continue to build upon our initial discussion regarding the significance of neighborhoods as a unit of analysis, planning, and policymaking, and will explore frameworks for neighborhood analysis."
  },
  {
    "objectID": "schedule/02_neighborhood.html#before-class",
    "href": "schedule/02_neighborhood.html#before-class",
    "title": "What is a Neighborhood?",
    "section": "Before Class",
    "text": "Before Class\nYour readings for today provide insight into some of the working definitions for value judgments regarding the qualities of neighborhoods and why planners have found these qualities to be of importance to measure and understand.\n\n\n\n\n\n\nImportant\n\n\n\nYou must be logged in to your UIUC Box.com account in order to access these readings. This is to respect the copyright of the authors on a publicly accessible website.\n\n\nTalen, Emily, Sunny Menozzi, and Chloe Schaefer. (2015). What is a “Great Neighborhood”? An Analysis of APA’s Top-Rated Places. Journal of the American Planning Association, 81(2): 121-141. \nRohe, William. (2009). From Local to Global: One Hundred Years of Neighborhood Planning. Journal of the American Planning Association 75(2): 209-230."
  },
  {
    "objectID": "schedule/02_neighborhood.html#reflect",
    "href": "schedule/02_neighborhood.html#reflect",
    "title": "What is a Neighborhood?",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are your goals for taking this class? What would you like to learn about neighborhoods?\nWhat matters about neighborhoods? How have neighborhoods shaped your life?\nWhat types of stories do we tend to tell about neighborhoods? How do these stories contextualize how neighborhoods “fit” within cities and their regions?"
  },
  {
    "objectID": "schedule/02_neighborhood.html#slides",
    "href": "schedule/02_neighborhood.html#slides",
    "title": "What is a Neighborhood?",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/02_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/02_neighborhood.html#resources-for-further-exploration",
    "title": "What is a Neighborhood?",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nDahir, James. (1947). The Neighborhood Unit Plan: Its Spread and Acceptance. New York: The Russell Sage Foundation.\nMumford, Lewis. (1954). The Neighborhood and Neighborhood Unit. The Town Planning Review, 24(4): 256-270.\nSteuteville, Robert. (2019). The Once and Future Neighborhood. CNU Public Square."
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments Overview",
    "section": "",
    "text": "flowchart LR\n\nsubgraph Learn\nAA[Data Pipeline]\nAB[Sharing Your Work]\nAC[Learner's Permit]\nAD[Describing Places]\nAE[Population]\nAF[Projections]\nAG[Segregation]\nAH[Neighborhood Change]\nAI[Place Opportunity]\nAJ[Transportation Equity]\nAK[Neighborhood Health Equity]\nend\n\nsubgraph Apply\n\nAA --> B[Place Selection]\nAB --> B\nAC --> C\nAD --> C[Place Background]\nAE --> D[Population]\nAF --> D\nAG --> E[Policy]\nAH --> E\nAI --> E\nAJ --> E\nAK --> E\nend\n\nsubgraph Communicate\n  direction LR\nB --> F{Final Report}\nC --> F\nD --> F\nE --> F\nF --> G[Document or Website]\nF --> H[Presentation]\nend"
  },
  {
    "objectID": "assignments/index.html#apply",
    "href": "assignments/index.html#apply",
    "title": "Assignments Overview",
    "section": "Apply",
    "text": "Apply\nYou’ll apply what you’ve learned over the course of the semester to the analysis of a place of your choosing. At the beginning of the semester, you will identify a place, and start to examine the relevant policy issues that motivate your analysis. You will then apply course tools and concepts to your place and will communicate your findings in a series of memos."
  },
  {
    "objectID": "assignments/index.html#communicate",
    "href": "assignments/index.html#communicate",
    "title": "Assignments Overview",
    "section": "Communicate",
    "text": "Communicate\nOur ultimate goal is to develop the capacity to communicate our analysis to diverse audiences. You will draw from your place analysis and memos to develop a final policy report in which you communicate your to a public audience in a form that is polished and professional."
  },
  {
    "objectID": "assignments/index.html#learn",
    "href": "assignments/index.html#learn",
    "title": "Assignments Overview",
    "section": "Learn",
    "text": "Learn\nOn a weekly basis, we’ll incorporate new tools and analytic strategies into our repertoire. We’ll devote some time most weeks in class to working through lab examples in class that help to illustrate how tools work and how to apply them. Labs are designed to be a low-stakes opportunity to learn."
  },
  {
    "objectID": "assignments/index.html#evaluation",
    "href": "assignments/index.html#evaluation",
    "title": "Assignments Overview",
    "section": "Evaluation",
    "text": "Evaluation\n\nEvaluation Components\n\n\n\n\n\n\n\n\nStrategy\nMaster’s Students\nPh.D. Students\n\n\n\n\nLearn\nLabs\nLabs\n\n\nApply\nPlace Selection Memo\nPlace Background Memo\nPopulation Memo\nPolicy Memo\nPaper Proposal\nPaper Outline\nPaper Draft\n\n\nCommunicate\nFinal Report\nTerm Paper\n\n\nEngage\nParticipation and Engagement\nParticipation and Engagement\n\n\n\n\n\nPh.D. Students\nDoctoral students may elect to either satisfy course requirements laid out for Master’s students or may instead complete a research-oriented term project that applies neighborhood analysis techniques to a specific research problem.\nDoctoral students should speak with me during weeks 1 or 2 if you wish to pursue this option so we can develop an appropriate research proposal, timeline, and outputs."
  },
  {
    "objectID": "assignments/index.html#engage",
    "href": "assignments/index.html#engage",
    "title": "Assignments Overview",
    "section": "Engage",
    "text": "Engage\nConsistent contributions in class, thoughtful engagement with course material, and other achievements may lead to overall adjustments in your course grade."
  },
  {
    "objectID": "assignments/index.html#grading-policy",
    "href": "assignments/index.html#grading-policy",
    "title": "Assignments Overview",
    "section": "Grading Policy",
    "text": "Grading Policy\nThis course requires your engagement both within and outside of the classroom. Grading criteria include contribution to the course environment, quality of writing, and depth of analysis. Your work will be graded on a 100-point scale:\n\n\n\n\n\n\n\n\nGrade\nPoints\nDescription\n\n\n\n\nA\n90-100\nOutstanding work, ready for dissemination\n\n\nB\n80-89\nGood work, needs minor revision to be ready for dissemination\n\n\nC\n70-79\nWork needs major revision, not ready for dissemination\n\n\nD\n60-69\nWork does not meet course expectations\n\n\nF\n0-59\nWork does not meet minimum standards"
  },
  {
    "objectID": "syllabus/index.html#assignments-evaluation-and-grading",
    "href": "syllabus/index.html#assignments-evaluation-and-grading",
    "title": "Syllabus",
    "section": "Assignments, Evaluation, and Grading",
    "text": "Assignments, Evaluation, and Grading\nYou will find detailed information on assignments, evaluation, and grading in the Assignments section."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "This page contains resources that will help you over the course of the class. As you encounter more resources that you find useful, please share them so they can be added to this page."
  },
  {
    "objectID": "resources/analysis.html",
    "href": "resources/analysis.html",
    "title": "Neighborhood Analysis",
    "section": "",
    "text": "City of Champaign Neighborhood Wellness Vision Report and Action Plan\nChicago's Million Dollar Blocks\nDePaul Institute for Housing Studies: Mapping Displacement Pressure in Chicago, 2020\nNathalie P. Voorhees Center's Socioeconomic Change of Chicago's Community Areas (1970-2010)\nNational Neighborhood Indicators Partnership\nFind My Landlord, a tool that maps the properties in Chicago owned by a single landlord or management company.\nMetropolitan Planning Council and Urban Institute's The Cost of Segregationreport for Chicago.\nAmerican segregation, mapped at day and night"
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data Analysis",
    "section": "",
    "text": "covdata R Package\nData Science Central Big data sets available for free\nEconomic Innovation Group Distressed Communities Index\nEviction Lab data on residential evictions summarized to various administrative geographies\nGlobal Human Settlements Dataset\nHUD Location Affordability Index\nHUD Urbanization Perceptions Small Area Index\nHUD USPS Vacancy Data\nLongitudinal Tract Database (LTDB): 1970 - 2010 Census data normalized to 2010 census tracts\nMIT Senseable Cities Lab Treepedia\nNational Housing Preservation Database (NHPD)\nPolicy Surveillance Program Eviction Laws database\nUSDA Rural-Urban Commuting Area Codes\nStanford Open Policing Project\nSURGO Foundation U.S. COVID Community Vulnerability Index (CCVI)"
  },
  {
    "objectID": "resources/inspiration.html",
    "href": "resources/inspiration.html",
    "title": "Inspiration",
    "section": "",
    "text": "BBC: How the BBC Visual and Data Journalism Team Works with Graphics in R\nBloomberg: Visualizing U.S. Immigration History as Tree Rings\nCraig Dalton and Timm Stallmann: Counter-mapping data science\nGapminder (check out in particular their temporal animations)\nGeoDa Center U.S. COVID Atlas\nR at the ACLU: Joining Tables to Reunite Families\nLondon: The Information Capital: Stunning book featuring 100 visualizations about the city of London. Many visualizations were created using R.\nMIT: Atlas of Inequality\nNew York City Neighborhood Planning Playbook\nR Graph Gallery\nWashington Post: America is more diverse than ever - but still segregated\nW.E.B. Du Bois’s Data Portraits: Visualizing Black America and related background from Smithsonian Magazine"
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How To",
    "section": "",
    "text": "R is an open source programming language that is a common tool used for data analysis across a range of disciplines. This means that in addition to being free and available for a range of operating systems and environments, R is also directly supported by a diverse user community who continually develop approaches for specialized applications or data. Need to download U.S. Census Data? There’s an R package for that. Need to perform common data cleaning tasks? There’s an R package for that too. We’ll be exploring a range of these specialized applications over the course of the semester.\nOf course there are alternate languages which we could employ in service of neighborhood analysis. Python, for example, is an even more ubiquitous programming language with its own set of tools for data science. R was originally built as a statistical computing language, and that brings some important benefits for the types of data science we’ll be learning this semester. R is also fairly prevalent among the user community working in public policy analysis and urban data science - this is the user community which you will be joining. Finally, R has a high learning curve, but also a very active user community, meaning that abundant documentation of problems and their solutions is available.\nAs we get started, let’s be clear - you are going to experience some frustration and challenges as you learn the R programming language. This class assumes no prior background in R or any other programming language for that matter, and we’ll work to quickly build your “vocabulary” and the ability to get results. We will spend some time picking up basics, and will then use our exploration of specific analysis approaches to reinforce our use of the grammar and structure of the language and to build more complex scripts over time.\nIt’s fair to equate learning R with learning to drive a manual car. Increasingly, people learn how to drive in automatic cars - essentially allowing the car to handle the function of switching gears - you put the car into drive, press the accelerator pedal and the car moves forward. Your past exposure to computer-based analytic tools has probably followed a similar strategy - you likely learned using software that had graphical user interfaces that allow them to call up and run programs and then spit out results. Most of us learn to point and click in order to accomplish a particular set of analytic tasks, meaning that if we want to generate the same results in the future, we would have to repeat all of those same steps over again.\nSo why learn on a manual? For many car enthusiasts, manuals are both more efficient and more engaging to drive - they offer additional control, and come with a heightened awareness of what the car is doing. Of course, they also come with a steep learning curve.\nSome of the same attributes apply to our use of R as a tool for analysis:\n\nFirst, a manual approach forces you to explicitly understand more of the requirements and assumptions that go into the analysis that you’re doing.\nSecond, you have to know your data (and its strengths and limitations) well in order to get it set up for analysis and to produce useful output.\nThird, this approach emphasizes reproducible analysis, meaning that you will develop workflows that can be repeated over and over again producing the same results - important for sharing your work with others and for accountability, especially within contexts related to public deliberation of the strengths and weaknesses of policy arguments.\n\nKeeping these three benefits in mind, we’ll be using the R software coupled with the RStudio Integrated Development Environment (IDE) to counter some of the downsides of a purely manual approach. RStudio will help us implement R code more effectively and efficiently. Hopefully at this point, this prospect leaves you excited rather than daunted!"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Setting up R and RStudio",
    "section": "",
    "text": "R can be downloaded from the Comprehensive R Archive Network (CRAN), a network of mirrored servers throughout the world that host the R software as well as user-developed packages. Visit https://cloud.r-project.org which will automatically direct you to the best download server.\nOnce here, you can then download and install R using the version that matches your operating system:\n\nGo ahead and download R and install following the installer defaults. After R is downloaded and installed, run R to confirm. You should see a single window that looks something like this:\nThat’s it for R. This should be the first and last time you’ll need to open the R console directly. We typically will not access R directly, but instead will use RStudio as our primary interface. Go ahead and close the R console and proceed by downloading and installing RStudio, which is the IDE we will use to interface and write code with R."
  },
  {
    "objectID": "howto/setupr.html#downlad-rstudio",
    "href": "howto/setupr.html#downlad-rstudio",
    "title": "Setting up R and RStudio",
    "section": "Downlad RStudio",
    "text": "Downlad RStudio\nNext up, let’s download RStudio. Go to https://rstudio.com/products/rstudio/download. Click on “Download” under RStudio Desktop. The website will suggest the most appropriate current version of the software based upon the computer you are loading RStudio on. You may also choose from an alternate installer if you believe it is more appropriate for your operating system.\nDownload and install RStudio, again using the default installation settings.\nOnce you have RStudio installed, open RStudio. You should see something that looks like this (the information in your console window will likely describe a newer version of R than what is displayed here).\n\nYou’ll note that what appears in the portion of the console to the left looks very similar to the R console window which you opened before you started your install of RStudio. This console functions in exactly the same way, however, you’ll also note that there are other areas which you have access to as well.\n## Rtools error (PC users)\nSome PC users may encounter an error message regarding Rtools not being installed. This would come up with you first try to install a package such as tidyverse either using R’s package manager or using the install.packages() command. To remedy this error if it occurs, PC users need to close R, manually download a patch, install it, and restart your computer. This error should then be remedied.\nIf you are a PC user who encounters this error, please do the following: 1. Save any work and close your RStudio session 2. Download Rtools (download link) + the link above contains instructions for putting Rtools on the PATH - you should not need to do this. 3. Run the downloaded executable file to install Rtools 4. Restart your computer 5. Re-open RStudio - the problem should be resolved"
  },
  {
    "objectID": "howto/setupr.html#run-r-in-the-cloud",
    "href": "howto/setupr.html#run-r-in-the-cloud",
    "title": "Setting up R and RStudio",
    "section": "Run R in the Cloud",
    "text": "Run R in the Cloud\nUIUC AnyWare provides cloud-based options for running software including RStudio."
  },
  {
    "objectID": "howto/setupr.html#the-rstudio-interface",
    "href": "howto/setupr.html#the-rstudio-interface",
    "title": "Setting up R and RStudio",
    "section": "The RStudio Interface",
    "text": "The RStudio Interface\n\nThe R Console is the place where code you write is executed. Typically we’ll write code in a script or R Notebook (more on those later) and active pieces of code will them be executed in the R console. You can also type code directly into the console and execute it by hitting the return or enter key.\nThe Environment Window provides information on variables, data tables, and other objects you create and define as you work.\nThe Auxiliary Window provides a range of information, and includes a file browser, plot visualization window, and access to help documentation.\n\nWith those basics in mind, you can start to explore the basic functionality of R."
  },
  {
    "objectID": "assignments/05_term.html#design-considerations",
    "href": "assignments/05_term.html#design-considerations",
    "title": "Term Assignment",
    "section": "Design Considerations",
    "text": "Design Considerations\n\nYou have nearly all of the raw material you need from your prior memorandums, however, you probably received comments from your peers and your instructors about how you might improve your analyses. Now is the time to implement that advice.\nAs usual, you are not required to include all of the tables and graphics you produced for your drafts and exercises. Narrow the focus to the most significant trends.\nYou are encouraged to bring in other material (e.g., existing reports or studies) that will help you provide context for your discussion or help you interpret trends. Cite accordingly.\nYou should strive to write this report in fewer than 4,000 words, not including references, tables, figures, or appendices. The aim is to produce a narrative that is concise and focused. Highlight what matters in the context of your narrative. Follow the strategies for effective professional writing—including use of active headings and concise reporting of trends—to convey the information succinctly. Title your report appropriately."
  },
  {
    "objectID": "assignments/05_term.html#outputs",
    "href": "assignments/05_term.html#outputs",
    "title": "Term Assignment",
    "section": "Outputs",
    "text": "Outputs\n\nReport: Prepare a professional report of around 4,000 words. In April, you will share a draft report and will receive feedback during a peer review session. Your final report will be due after the conclusion of classes. Your report may be formatted as a stand alone document or may be shared as a website.\nPresentation: Prepare and deliver a 12-minute professional presentation that summarizes your policy issue, analysis, and policy recommendations. Your presentation should around 10 minutes long and the remainder of your time will be devoted to immediate questions. We will also complete written feedback and reactions that will be shared after the conclusion of your presentation."
  },
  {
    "objectID": "assignments/05_term.html#submission-instructions",
    "href": "assignments/05_term.html#submission-instructions",
    "title": "Term Assignment",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/02_place_background.html#goals",
    "href": "assignments/02_place_background.html#goals",
    "title": "Place Background Memorandum",
    "section": "Goals",
    "text": "Goals\n\nAdd more thought and detail to your previous place selection memorandum\nDraw upon a range of sources to critically analyze the place you selected\nUse research to develop priorities for exploratory analysis"
  },
  {
    "objectID": "assignments/02_place_background.html#your-place-background-memorandum",
    "href": "assignments/02_place_background.html#your-place-background-memorandum",
    "title": "Place Background Memorandum",
    "section": "Your Place Background Memorandum",
    "text": "Your Place Background Memorandum\nDrawing from external sources and your own research, prepare a memorandum of around 2,000 words that provides background on your chosen place. Your memorandum should provide some general background on your place, and should then identify one or several key issues that will guide your subsequent analysis and work over the course of the semester. Key issues should be guided by a combination of your interests and what you can justify as relevant given your background research.\nBelow, please find some guiding questions to help you structure your memorandum:\n\nWhat is the overall history of your place? What are the types of issues that have mattered in the past, and what types of issues matter at present?\nWhich neighborhoods or subareas of your place are frequently mentioned in local plans, reports, or journalistic accounts? What differentiates those neighborhoods? At what geographic scale can these subareas be classified or analyzed?\nBased upon your background research, are there any population groups that are likely to warrant special attention. Why?\nBased upon your background research, are there policy issues or debates that warrant special attention or that could benefit from detailed analysis. What are these, and what factors are important for understanding them?\nBased upon your exploration, identify key themes and indicators which you believe will be important to focus on in your subsequent analyses. What types of strategies do you intend to use in order to examine these themes and indicators?"
  },
  {
    "objectID": "assignments/02_place_background.html#submission-instructions",
    "href": "assignments/02_place_background.html#submission-instructions",
    "title": "Place Background Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/03_population.html#goals",
    "href": "assignments/03_population.html#goals",
    "title": "Population Memorandum",
    "section": "Goals",
    "text": "Goals\n\nIntegrate analytical techniques learned from labs into targeted demographic analysis of your place\nCritically assess the story of your region and use it as a framework for writing and description\nDevelop baseline analysis that supports future policy analysis"
  },
  {
    "objectID": "assignments/03_population.html#your-population-memorandum",
    "href": "assignments/03_population.html#your-population-memorandum",
    "title": "Population Memorandum",
    "section": "Your Population Memorandum",
    "text": "Your Population Memorandum\nPrepare a memorandum of around 2,000 words (not including tables and graphics) that provides an overview of the population of your place, and that identifies important policy-relevant population subgroups. Your memorandum should summarize for your place major characteristics and trends in:\n\nPopulation and population change;\nRacial and ethnic composition;\nAge structure;\nIncome, wages, and poverty;\nEducational attainment and employment by industry\n\nThe objective is to provide a clear portrait of the demographic characteristics of your place and how they are changing over time and space. Support your discussion with tables and figures as appropriate, and include at least one map displaying the geography of your place."
  },
  {
    "objectID": "assignments/03_population.html#memorandum-preparation-tips",
    "href": "assignments/03_population.html#memorandum-preparation-tips",
    "title": "Population Memorandum",
    "section": "Memorandum Preparation Tips",
    "text": "Memorandum Preparation Tips\n\nProvide some context, namely information on the area’s physical features and a very brief history. This does not need to be done in a separate section and it should not adopt the “booster” language of your typical Chamber of Commerce brochure. Sometimes it can be accomplished in a sentence or two, or in a few well-placed sentences in the report.\nLook for a story line in the data. Often there are some important local trends (sustained economic distress, rapid economic growth, high rates of international migration, etc.) that provide a good organizing framework for more specific social and demographic trends.\nYou already have most of the data and code you need from class lab sessions. You should plan on drawing from these, but also expanding upon them. Expansion may mean looking at data sources or indicators we have not explored before. We are looking for evidence that you are using the most appropriate information to follow through on your lines of argumentation. What is important is dictated by… your place!\nLikewise, you are not required to include all of the types of tables, graphics, and analyses you produced for the prior exercises. Now is the time to narrow your focus to the most significant local characteristics and trends.\nFeel free to incorporate other material (e.g., existing studies or reports) that will help you provide context for your discussion or help you interpret trends. Cite accordingly.\nYou should strive to write this memorandum in fewer than 2,000 words, not including references, tables, and figures. Two thousand words is a maximum, not a minimum. Follow the strategies for effective professional writing—including use of active headings and concise reporting of trends—to convey the information concisely."
  },
  {
    "objectID": "assignments/03_population.html#submission-instructions",
    "href": "assignments/03_population.html#submission-instructions",
    "title": "Population Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/04_policy.html#goals",
    "href": "assignments/04_policy.html#goals",
    "title": "Policy Memorandum",
    "section": "Goals",
    "text": "Goals\n\nDraw from your prior population profiles and analyses to identify a policy issue or question for analysis.\nBuild off of the analysis strategies we’ve learned about in class to develop an appropriate analytic strategy for the identified policy issue of your choice."
  },
  {
    "objectID": "assignments/04_policy.html#memorandum-preparation-tips",
    "href": "assignments/04_policy.html#memorandum-preparation-tips",
    "title": "Policy Memorandum",
    "section": "Memorandum Preparation Tips",
    "text": "Memorandum Preparation Tips\n\nThe challenge of this assignment is picking an appropriate policy question to engage with. Please rely upon each other as peer reviewers and on your instructors for help in identifying an appropriate policy or research question to focus your memorandum on.\nPolicy analysis depends upon following a logical chain of thought through existing policies and questions. Be sure to review and understand these before attempting to develop your own approach. Please also think carefully and critically about how your your chosen analysis may iterate over various neighborhoods in your place.\nAt this point in the semester, we expect to see refined maps, data visualizations, and tables in your memos. Please allocate appropriate time to working on these elements of your memo.\nThink through how the story of your policy analysis relates back to the other stories you’ve told in subsequent memos over the course of the semester. Your final assignment depends upon you integrating these items into a final policy report, so having a good understanding of how these stories are related will make it easier to develop a coherent narrative in your final report."
  },
  {
    "objectID": "assignments/04_policy.html#submission-instructions",
    "href": "assignments/04_policy.html#submission-instructions",
    "title": "Policy Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "schedule/03_datapipeline.html",
    "href": "schedule/03_datapipeline.html",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this session, we’ll explore some of the basic workflow which we’ll use over the course of the semester to package and share analysis. We’ll develop familiarity with Quarto, and basic operations in Github so that you are able to share code and analysis over the course of the semester.\nLab 1 Link"
  },
  {
    "objectID": "schedule/03_datapipeline.html#before-class",
    "href": "schedule/03_datapipeline.html#before-class",
    "title": "Building a Data Pipeline",
    "section": "Before Class",
    "text": "Before Class\nEnsure that your computer has the latest stable versions of R and RStudio installed.\nAccept the GitHub invitation to our Lab 1 repository and download the repository to your local computer (we will set up more advanced tools for interacting with GitHub in our next lab session.\nD’Ignazio, Catherine, and Lauren F. Klein. (2020). Data Feminism. MIT Press. Chapter 1 , Chapter 2"
  },
  {
    "objectID": "schedule/03_datapipeline.html#reflect",
    "href": "schedule/03_datapipeline.html#reflect",
    "title": "Building a Data Pipeline",
    "section": "Reflect",
    "text": "Reflect\n\nWorkflows\n\nWhat are the types of common tasks in your workflows that you think would benefit from a data pipeline?\nHow do we hold ourselves accountable for our analysis?\n\n\n\nReadings\n\nWhose interests and goals do you seek to represent through your work?\nHow does Collins’ matrix of domination (structural, disciplinary, hegemonic, interpersonal) interact with acts of data-driven storytelling?\nWhat missing datasets (akin to the Library of Missing Datasets) have you observed?1\nWhat’s an analysis for which you’d like to reconstruct in ways that challenge the power manifested?"
  },
  {
    "objectID": "schedule/03_datapipeline.html#slides",
    "href": "schedule/03_datapipeline.html#slides",
    "title": "Building a Data Pipeline",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/03_datapipeline.html#resources-for-further-exploration",
    "href": "schedule/03_datapipeline.html#resources-for-further-exploration",
    "title": "Building a Data Pipeline",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/Session_template.html#before-class",
    "href": "schedule/Session_template.html#before-class",
    "title": "NAME",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/Session_template.html#reflect",
    "href": "schedule/Session_template.html#reflect",
    "title": "NAME",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/Session_template.html#slides",
    "href": "schedule/Session_template.html#slides",
    "title": "NAME",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/Session_template.html#resources-for-further-exploration",
    "href": "schedule/Session_template.html#resources-for-further-exploration",
    "title": "NAME",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "slides/01_CourseIntroduction.html",
    "href": "slides/01_CourseIntroduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "What is a Neighborhood?\n\nWe’re going to find out…\n\nIf you know, you know…"
  },
  {
    "objectID": "assignments/index.html#grading",
    "href": "assignments/index.html#grading",
    "title": "Assignments Overview",
    "section": "Grading",
    "text": "Grading\n\nGrading Policy\nThis course requires your engagement both within and outside of the classroom. Grading criteria include contribution to the course environment, quality of writing, and depth of analysis. Your work will be graded on a 100-point scale:\n\n\n\n\n\n\n\n\nGrade\nPoints\nDescription\n\n\n\n\nA+\nA\nA-\n97-100\n93-96\n90-92\nOutstanding work, ready for dissemination. Demonstrates mastery of course concepts and skills, clear communication, and appropriate use of analytical methods.\n\n\nB+\nB\nB-\n87-89\n83-86\n80-82\nGood work, that meets expectations. Work needs minor revision address minor communication or technical errors in order to be ready for dissemination\n\n\nC+\nC\nC-\n77-79\n73-77\n70-72\nWork needs major revision to writing or analysis due to technical errors, not ready for dissemination.\n\n\nD+\nD\n67-69\n60-66\nWork does not meet course expectations for graduate-level work. Reflects lack of understanding or conceptualization of work, major technical errors, and requires significant rethinking.\n\n\nF\n0-59\nWork does not meet minimum expectations, effort, or understanding.\n\n\n\n\n\nAttendance Policy\nThe learning environment in this class depends upon your engagement and participation. Full participation is expected for course sessions. For each course session that you are absent from (excluding excused absences), 2 percent will be deducted from your final course grade. Excused absences will be granted on a case-by-case basis for extenuating circumstances such as illness or work-related travel, but must be approved by me prior to the course session which you are absent from."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html",
    "href": "assignments/labs/01_datapipeline.html",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this lab, we’ll explore some of the basic workflows which we’ll use over the course of the semester to package and share analysis. A primary principle of our class is that our analysis be reproducible by others - this is an important prnciple both for supporting the validity of our arguments, and also accountability to those neighborhoods and communities we are analyzing and to the broader audience who may use our work in their deliberation or who might adapt our workflows to address similar questions in their areas of focus.\nOur class will focus on developing data analysis and sharing pipelines that make use of R and RStudio’s core functions. We’ll gain familiarity with RStudio’s Quarto format which allows for the integration of plain text, code, and code output in the same documents and which is focused on easily translating text and code into multiple output formats. We’ll also become more familiar with Github, which we’ll use as a tool to share code and analysis."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#goals",
    "href": "assignments/labs/01_datapipeline.html#goals",
    "title": "Building a Data Pipeline",
    "section": "Goals",
    "text": "Goals\n\nBecome familiar with core programmatic ways to manipulate files in R and RStudio.\nBecome familiar with the core features and outputs related to Quarto.\nSet up your computer so that RStudio can communicate with your Github account."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#core-concepts",
    "href": "assignments/labs/01_datapipeline.html#core-concepts",
    "title": "Building a Data Pipeline",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nProject\nWorking Directory\nWorkspace\nQuarto Markdown\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#the-lab",
    "href": "assignments/labs/01_datapipeline.html#the-lab",
    "title": "Building a Data Pipeline",
    "section": "The Lab",
    "text": "The Lab\n\n\nFile System and Structure\nWhen you started learning R and RStudio, you likely struggled a bit with some of the core concepts related to how R views your computer’s file system and where it looks for files and stores work. There’s nothing to complex about what R is doing, but as computers have become better at making sensible decisions for us, it can be easy to lose track of some of the basics of how we need to set up file structures that help us to actively manage our workflows and outputs.\nThere are a few basics which we need to have down. The first is a gentle reminder about how computer file systems work. Back when I was a kid in the early 1980s, the primary way we interacted with computers was at the command line. When you started a computer, you might be greeted with something that looks like the terminal tab in your RStudio session:\n From here, you’d type commands to either navigate between directories, list files in a directory, or execute files. For instance, we can use ls to list all the files in my home directory: \nAnd then take a look at the results: \nWhich is the equivalent of looking at what files are in my computer’s home directory in a graphical user interface: \nWhile for the most part we wont be bothered with accessing our file system this way, it’s useful to understand what’s going on with the pathnames that are shown. Whether you’re used to working with Mac, PC, or Linux operating systems, the approach to file systems is sufficiently similar to talk about them all together.\nMost modern operating systems operate using relative pathnames which allow us to be more efficient in referencing where files are located in our computer. The path that you saw above on my computer points to the “home directory” which is the base directory associated with my user account on the computer. My home director is situated however within other directories that are part of the operating system and that are ultimately on a hard disk drive.\n So why is all of this important? This underscores the rationale for and utility of relative paths in our filesystem. My mac computer’s home directory is Machinsosh HD/Users/andrewgreenlee/ which means that rather than having to type out all of that content, I can just start instructions to paths beloww that directory. For instance, let’s navigate to my desktop folder: \nWhich is the same as navigating from the root folder of our hard drive to the desktop folder: \nThis certainly makes things more convenient for navigating files in Terminal and in our file system. It has a much deeper utulity when we are thinking about reproducible data analysis. Let’s explore in more detail.\n\nDirectory Structure and R\nOFtentimes when people begin learning in R and RStudio, they simply open the program and start typing commands. This works well up until the point when one needs to get data into or out of R. In general, R is agnostic about where to look for files - if you open RStudio without specifying where to look for files, it assumes you want to work out of your home directory (the getwd() command returns the current working directory R is using): \nWe typically do not want to work out of our home directory, but would like to set another directory to work out of. When new R users start to work in R, many are taught to use setwd() to explicitly set a directory to work out of. This will work, and sets a “home” directory for the particular work you are doing.\n\nsetwd(\"Desktop/andrew-home/neighborhood_types/\")\n\nFor the sake of illustration, let’s say we wnat to open a dataset containing a list of community area definitions for Chicago:\n\ndataset <- read.csv(\"CommAreas.csv\")\ndataset\n\nWe create a script in our working directory that sets the directory, reads the file, and then shows the file to us.\nWhat happens when you move your working folder to another computer or share it? If you explicitly set your working directory, it won’t be able to access that directory which is at a path that doesn’t exist on your other computer:\n R Projects help us to address this problem. Projects add a file to the folder we wish to set as our home directory for our project which establishes where R will look for files.\nIn the right top corner of your RStudio window, you’ll note a blue cube which leads to a project management menu:\n If you click “New Project”, a dialog box will open up which allows you to specify project options:\n\nYou have options here - you can either create a new project in a new folder, use an existing folder which you’ve already created, or check a project out from Github or another vesion control system. Since we already have a directory we’ve been working out of, we can use the existing directory dialog, and then set the directory where our script and data are located.\n\nFrom here, so long as we open our work by clicking on the project file, we’ll be taken into the project which will remember what the “home” directory is for our work. \nThe benefit of this approach is that if we move our project directory to another computer and open our project, R understands that the folder with the project file reflects the relative path to where we will find the other files referenced in our project. This means that if we share our working folder with others, they can reproduce what we did so long as all files are contained within that project directory.\nProject files represent mileposts that set the relative path at which RStudio will look for files within our particular project. This is useful for creating a project file structure within which we can create reproducible and easily shareable analysis. You should be in the habit of starting each new analysis you do by setting up a new work folder with a new project file.\n\n\n\nProjects vs. Workspaces\nEach time you close your RStudio session, you will be faced with a temptation - RStudio will ask whether you want to save your workspace.\n\nWhat’s the difference between a script, a project, and a workspace? A script contains the code which tells R what you want it to do. Script commands are run in the console in the order they are written. You want to get in the habit of saving your script files often. Your project file tells R the path to look for files and folders at. This sets a relative path for your work in that project. A workspace file saves the exact image of what’s going on when you close out of RStudio - that may include loaded datasets, functions, and objects.\nYou have likely been tempted to save your workspace when you are closing your R session. While tempting, I encourage you not to get in the habit of saving your workspace. This runs counter to most other interactions you’ve had with computers - saving documents, video games, or survey responses. Saving your workspace can lead to bad habits with regards to reproducibility. You want to allow your scripts to serve as a repository for all steps that go from your raw data to your analysis and outputs. Unless your script has time or computationally intensive steps, your scripts and computer can re-run your workflow to return to the point that you left off when you last worked on the project.\n\n\nLater in the class, we’ll introduce more powerful strategies like the renv and Targets packages which can help to manage your workflows and their dependencies.\nMore Here"
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#file-system-and-structure",
    "href": "assignments/labs/01_datapipeline.html#file-system-and-structure",
    "title": "Building a Data Pipeline",
    "section": "File System and Structure",
    "text": "File System and Structure\n\nWhen you started learning R and RStudio, you likely struggled a bit with some of the core concepts related to how R views your computer’s file system and where it looks for files and stores work. There’s nothing to complex about what R is doing, but as computers have become better at making sensible decisions for us, it can be easy to lose track of some of the basics of how we need to set up file structures that help us to actively manage our workflows and outputs.\nThere are a few basics which we need to have down. The first is a gentle reminder about how computer file systems work. Back when I was a kid in the early 1980s, the primary way we interacted with computers was at the command line. When you started a computer, you might be greeted with something that looks like the terminal tab in your RStudio session:\n From here, you’d type commands to either navigate between directories, list files in a directory, or execute files. For instance, we can use ls to list all the files in my home directory: \nAnd then take a look at the results: \nWhich is the equivalent of looking at what files are in my computer’s home directory in a graphical user interface: \nWhile for the most part we wont be bothered with accessing our file system this way, it’s useful to understand what’s going on with the pathnames that are shown. Whether you’re used to working with Mac, PC, or Linux operating systems, the approach to file systems is sufficiently similar to talk about them all together.\nMost modern operating systems operate using relative pathnames which allow us to be more efficient in referencing where files are located in our computer. The path that you saw above on my computer points to the “home directory” which is the base directory associated with my user account on the computer. My home director is situated however within other directories that are part of the operating system and that are ultimately on a hard disk drive.\n So why is all of this important? This underscores the rationale for and utility of relative paths in our filesystem. My mac computer’s home directory is Machinsosh HD/Users/andrewgreenlee/ which means that rather than having to type out all of that content, I can just start instructions to paths beloww that directory. For instance, let’s navigate to my desktop folder: \nWhich is the same as navigating from the root folder of our hard drive to the desktop folder: \nThis certainly makes things more convenient for navigating files in Terminal and in our file system. It has a much deeper utulity when we are thinking about reproducible data analysis. Let’s explore in more detail.\n\nDirectory Structure and R\nOftentimes when people begin learning in R and RStudio, they simply open the program and start typing commands. This works well up until the point when one needs to get data into or out of R. In general, R is agnostic about where to look for files - if you open RStudio without specifying where to look for files, it assumes you want to work out of your home directory (the getwd() command returns the current working directory R is using): \nWe typically do not want to work out of our home directory, but would like to set another directory to work out of. When new R users start to work in R, many are taught to use setwd() to explicitly set a directory to work out of. This will work, and sets a “home” directory for the particular work you are doing.\n\n\nCode\nsetwd(\"Desktop/andrew-home/neighborhood_types/\")\n\n\nFor the sake of illustration, let’s say we wnat to open a dataset containing a list of community area definitions for Chicago:\n\n\nCode\ndataset <- read.csv(\"CommAreas.csv\")\ndataset\n\n\nWe create a script in our working directory that sets the directory, reads the file, and then shows the file to us.\nWhat happens when you move your working folder to another computer or share it? If you explicitly set your working directory, it won’t be able to access that directory which is at a path that doesn’t exist on your other computer:\n R Projects help us to address this problem. Projects add a file to the folder we wish to set as our home directory for our project which establishes where R will look for files.\nIn the right top corner of your RStudio window, you’ll note a blue cube which leads to a project management menu:\n If you click “New Project”, a dialog box will open up which allows you to specify project options:\n\nYou have options here - you can either create a new project in a new folder, use an existing folder which you’ve already created, or check a project out from Github or another vesion control system. Since we already have a directory we’ve been working out of, we can use the existing directory dialog, and then set the directory where our script and data are located.\n\nFrom here, so long as we open our work by clicking on the project file, we’ll be taken into the project which will remember what the “home” directory is for our work. \nThe benefit of this approach is that if we move our project directory to another computer and open our project, R understands that the folder with the project file reflects the relative path to where we will find the other files referenced in our project. This means that if we share our working folder with others, they can reproduce what we did so long as all files are contained within that project directory.\nProject files represent mileposts that set the relative path at which RStudio will look for files within our particular project. This is useful for creating a project file structure within which we can create reproducible and easily shareable analysis. You should be in the habit of starting each new analysis you do by setting up a new work folder with a new project file.\n\n\nProjects vs. Workspaces\nEach time you close your RStudio session, you will be faced with a temptation - RStudio will ask whether you want to save your workspace.\n\nWhat’s the difference between a script, a project, and a workspace? A script contains the code which tells R what you want it to do. Script commands are run in the console in the order they are written. You want to get in the habit of saving your script files often. Your project file tells R the path to look for files and folders at. This sets a relative path for your work in that project. A workspace file saves the exact image of what’s going on when you close out of RStudio - that may include loaded datasets, functions, and objects.\nYou have likely been tempted to save your workspace when you are closing your R session. While tempting, I encourage you not to get in the habit of saving your workspace. This runs counter to most other interactions you’ve had with computers - saving documents, video games, or survey responses. Saving your workspace can lead to bad habits with regards to reproducibility. You want to allow your scripts to serve as a repository for all steps that go from your raw data to your analysis and outputs. Unless your script has time or computationally intensive steps, your scripts and computer can re-run your workflow to return to the point that you left off when you last worked on the project.\n\n\nLater in the class, we’ll introduce more powerful strategies like the renv and Targets packages which can help to manage your workflows and their dependencies."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#working-in-quarto",
    "href": "assignments/labs/01_datapipeline.html#working-in-quarto",
    "title": "Building a Data Pipeline",
    "section": "Working in Quarto",
    "text": "Working in Quarto\nQuarto is a tool built into RStudio which allows you to create a variety of documents that can integrate plain text and code. It’s utility comes both in it’s ability to translate plain text into a variety of well-formatted outputs, but also due to the ease with which we can use this format to share code, analysis, and writing with others. Most of your written assignments in class will be produced in Quarto, therefore, you’ll want to become familiar with the basic syntax and logic.\nQuarto’s plain text formatting is handled using Markdown syntax which is designed to be simple, easily interpreted as text with formatting in place, and readable on most computers. An increasing number of note systems and word processors have adopted Markdown as their language.\n\n\nSome examples include Bear, Obsidian, Notion, Ulysses. Mentions are not endorsements.\n\nFormatting Your Documents\nQuarto makes use of a modified version of Markdown syntax. The Quarto website maintains excellent documentation regarding basic markdown formatting options as well as Quarto-specific formatting options:\n\nYou should familiarize yourself with how to format text, how to use section headings, and how to link files and insert references to other files.\n\n\nAdding References\nQuarto can help you either manage references in very sinple ways or more complex ways. You can either add references directly to your text as either footnotes or endnotes.\n\nFootnotes\nWe oftentimes use footnotes as an easy way to convey references or links to details or important information that does not need to be in our main body of text. Quarto allows you to create both footnotes and long notes. Footnotes are typically short and take up only a line or two. Long notes are longer and may contain multiple paragraphs.\n  \nHere is some text with a footnote.[^1]\n\n[^1]: And here is the footnote that accompanies that text in the footnotes section.\n\nHere is also some text with an inline footnote.^[Here is the text associated with the inline footnote - this may be easier to track because it is in line with the text.]\n\nHere is some text with a longnote.[^longnote]\n\n[^longnote]: And here is a longnote that takes up multiple lines. You can experiment in your text to see whether a footnote or longnote will serve you better for a given situation.\n  \n  This text is also part of the long note - the indent signals that it remains part of the note.\n  \nAnd this text would be part of the next paragraph that's outside of the note because it is not indented.\nHere is some text with a footnote1\nHere is also some text with an inline footnote.2\nHere is some text with a longnote3\nAnd this text would be part of the next paragraph that’s outside of the note because it is not indented.\n\n\nAsides\nYou can also add asides to documents which places a note in line with text in the margin.\n\n\nFor a very effective example of asides, see Kieran Healy’s book Data Visualization: A Practical Introduction.\n:::{.aside}\nHere is an aside placed in line with text in the margin.\n:::\n\n\nHere is an aside placed in line with text in the margin.\n\n\nCitations and Bibliographies\nFor longer or more advanced documents, you may wish to rely upon Quarto’s suppot for using and managing references with BibTex, a standard reference format. This allows you to add references to your document and then create in line citations and a formatted bibliography to accompany your writing.\nIn order to create a bibliography that is associated with a document, you’ll need three files:\n\nA main file written in Quarto Markdown (.qmd). This file will be formatted to include citation references.\nA bibliographic data source, typically a Bibtex (.bibtex) file.\nA csl file - this specifies the format for your bibliography. You can find and include different csl files to produce a bibliography using different reference styles.\n\n\nStep 1: Create a bibliography file.\nUsing a text editor, create a plain text file that will contain your bibliographic references. In this case, we’ll create a text file called “references.bib”. Note that this file should end in .bib to signify that it is a bibliographic reference file.\n\n\nStep 2: Refer to the bibliography file in your YAML Header\nNext, in your Quarto Markdown document, you’ll add a line to your YAML header that refers to the location and name of your bibliography file.\n\n\nCode\n---\ntitle: \"Neighborhood Analysis: Place Selection Memorandum\"\nbibliography: references.bib\n---\n\n\n\n\nStep 3: Define a citation style\nLike most other reference management systems, you can define what reference syle your references will be rendered as, and change this as needed. The Citation Style Language (CSL) project defines a common language and structure for styles. These are stored in .csl files which you will associate with your document. You can find and download .csl files in a range of formats from the CSL Project’s repository.\n\n\nYou can also find and download csl files in Zotero’s style repository. If you hover over a style format, it provides examples of what a citation will look like in your redered document.\nFor the sake of example, we’ll format our document using American Psychological Association (APA) format. We’ll search the CSL Project’s repository to find an appropriate APA style .csl file. Once you find the appropriate file, then download the file and place it in the same directory as our .bib file.\n Click on the .csl style format you want to use. You’ll then be taken to a page that includes the specific file’s contents.\n\nFrom here, you can right click on the button that says “raw” and download the .csl file.\n Note that before you move this file from your download folder to the folder where your .bib file is located, you may need to alter the file extension to indicate that this is a .csl file. Once you have done this, move it to the folder where your .bib file is located and then add a csl reference to your YAML header.\n\n\nCode\n---\ntitle: \"Neighborhood Analysis: Place Selection Memorandum\"\nbibliography: references.bib\ncsl: apa-6th-edition.csl\n---\n\n\n\n\nStep 4: Add references to your document\nYou’re now set up to add references to your document. You can add references in lots of different ways. If you’re searching for references using a service like Google Scholar, under the “cite” options for a given reference, you’ll see a URL to download the BibTeX citation associated with that reference.\n Click on BibTeX and the citation contents will show:\n\nCopy this text, and paste it into your bibliographic reference file (in our example, references.bib).\n\nYou can then cite this reference while you write by referring to it as follows:\n\n\nCode\n[@healy2018data] provides useful examples for leveraging GGPlot to create data visualization.\n\n\n(Healy, 2018) provides useful examples for leveraging GGPlot to create data visualization.\nNote that the actual reference corresponds to the first clause in the bibtex reference.\n\n\nYou can change the reference name if you’d like to something you’ll remember as you cite.\nYou can paste more references into the same references.bib file and cite them as well.\n\n\nYou can find more details on formatting citations on the Quarto website.\nOnline tools like Citation Machine can help you to build references for a range of online sources. (Feder, n.d.) takes a little while to get used to, but these tools can help you to build out an effective citation workflow.\n\n\nStep 5: Place your bibliography\nQuarto will automatically place your references at the end of your document (or wherever your style definition file calls for references to be placed). You can also explicitly tell Quarto where to place your references by creating a div as follows:\n\n\nCode\n\n## References\n\n::: {#refs}\n:::\n\n\nYou should end up with a formatted reference section at the end of your document.\n ### Reference Considerations\nIt’s ultimately up to you how you manage and generate references within your documents. If you have a document with very few references, footnotes and/or endnotes may be an appropriate strategy. A more complex document with many references may be better managed using BibTeX. You can also use another reference management system and then export citations in BibTeX format which can then be added to your Quarto Markdown document."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#header-2",
    "href": "assignments/labs/01_datapipeline.html#header-2",
    "title": "Building a Data Pipeline",
    "section": "Header 2",
    "text": "Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n# Header 1\n# Header 1\n\n\n## Header 2\n## Header 2\n\n\n### Header 3\n### Header 3\n\n\n#### Header 4\n#### Header 4\n\n\n##### Header 5\n##### Header 5\n\n\n###### Header 6\n###### Header 6\n\n\n\n\n\n\n\n\nLinks\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n<https://quarto.org>\nhttps://quarto.org\n\n\n[Quarto]\n[Quarto]\n\n\n[Quarto](https://quarto.org)\nQuarto\n\n\n![Burnham Plan](burnham.jpg)\n\n\n\n\n\n\nLists\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n* Unordered list</br>    + sub-item 1</br>    + sub-item 2       - sub-sub-item 1\n* Unordered list + sub-item 1 + sub-item 2 - sub-sub-item 1\n\n\n* item 2      Continued (indent 4 spaces)\n* item 2 Continued (indent 4 spaces)\n\n\n\n1. Ordered List 2. Item 2 i) sub-item 1 A. sub-sub-item1"
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#sharing",
    "href": "assignments/labs/01_datapipeline.html#sharing",
    "title": "Building a Data Pipeline",
    "section": "Sharing",
    "text": "Sharing\nOne of the major goals of our class is to develop workflows that allow us to easily share and disseminate our work - we’ll rely upon elements of the Quarto Markdown ecosystem to help us accomplish this.\n\nSharing via Quarto Pub\nhttps://quarto.org/docs/publishing/quarto-pub.html\n\n\nSharing via Github"
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#references",
    "href": "assignments/labs/01_datapipeline.html#references",
    "title": "Building a Data Pipeline",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#workflow-1-set-up-a-new-project",
    "href": "assignments/labs/01_datapipeline.html#workflow-1-set-up-a-new-project",
    "title": "Building a Data Pipeline",
    "section": "Workflow 1: Set up a New Project",
    "text": "Workflow 1: Set up a New Project\nThe first workflow we need to develop is proper project setup in R. In our next lab, we’ll add some more complexity to this project setup by incorporating version control and sharing, but we’ll start by getting a handle on projects that reside on our local computer. Let’s start with a quick review of file system and structure, and then transition into the nuts and bolds of project setup.\n\nFile System and Structure\n\nWhen you started learning R and RStudio, you likely struggled a bit with some of the core concepts related to how R views your computer’s file system and where it looks for files and stores work. There’s nothing to complex about what R is doing, but as computers have become better at making sensible decisions for us, it can be easy to lose track of some of the basics of how we need to set up file structures that help us to actively manage our workflows and outputs.\nThere are a few basics which we need to have down. The first is a gentle reminder about how computer file systems work. Back when I was a kid in the early 1980s, the primary way we interacted with computers was at the command line. When you started a computer, you might be greeted with something that looks like the terminal tab in your RStudio session:\n From here, you’d type commands to either navigate between directories, list files in a directory, or execute files. For instance, we can use ls to list all the files in my home directory: \nAnd then take a look at the results: \nWhich is the equivalent of looking at what files are in my computer’s home directory in a graphical user interface: \nWhile for the most part we wont be bothered with accessing our file system this way, it’s useful to understand what’s going on with the pathnames that are shown. Whether you’re used to working with Mac, PC, or Linux operating systems, the approach to file systems is sufficiently similar to talk about them all together.\nMost modern operating systems operate using relative pathnames which allow us to be more efficient in referencing where files are located in our computer. The path that you saw above on my computer points to the “home directory” which is the base directory associated with my user account on the computer. My home director is situated however within other directories that are part of the operating system and that are ultimately on a hard disk drive.\n So why is all of this important? This underscores the rationale for and utility of relative paths in our filesystem. My mac computer’s home directory is Machinsosh HD/Users/andrewgreenlee/ which means that rather than having to type out all of that content, I can just start instructions to paths beloww that directory. For instance, let’s navigate to my desktop folder: \nWhich is the same as navigating from the root folder of our hard drive to the desktop folder: \nThis certainly makes things more convenient for navigating files in Terminal and in our file system. It has a much deeper utulity when we are thinking about reproducible data analysis. Let’s explore in more detail.\n\nDirectory Structure and R\nOftentimes when people begin learning in R and RStudio, they simply open the program and start typing commands. This works well up until the point when one needs to get data into or out of R. In general, R is agnostic about where to look for files - if you open RStudio without specifying where to look for files, it assumes you want to work out of your home directory (the getwd() command returns the current working directory R is using): \nWe typically do not want to work out of our home directory, but would like to set another directory to work out of. When new R users start to work in R, many are taught to use setwd() to explicitly set a directory to work out of. This will work, and sets a “home” directory for the particular work you are doing.\n\n\nCode\nsetwd(\"Desktop/andrew-home/neighborhood_types/\")\n\n\nFor the sake of illustration, let’s say we wnat to open a dataset containing a list of community area definitions for Chicago:\n\n\nCode\ndataset <- read.csv(\"CommAreas.csv\")\ndataset\n\n\nWe create a script in our working directory that sets the directory, reads the file, and then shows the file to us.\nWhat happens when you move your working folder to another computer or share it? If you explicitly set your working directory, it won’t be able to access that directory which is at a path that doesn’t exist on your other computer:\n R Projects help us to address this problem. Projects add a file to the folder we wish to set as our home directory for our project which establishes where R will look for files.\nIn the right top corner of your RStudio window, you’ll note a blue cube which leads to a project management menu:\n If you click “New Project”, a dialog box will open up which allows you to specify project options:\n\nYou have options here - you can either create a new project in a new folder, use an existing folder which you’ve already created, or check a project out from Github or another vesion control system. Since we already have a directory we’ve been working out of, we can use the existing directory dialog, and then set the directory where our script and data are located.\n\nFrom here, so long as we open our work by clicking on the project file, we’ll be taken into the project which will remember what the “home” directory is for our work. \nThe benefit of this approach is that if we move our project directory to another computer and open our project, R understands that the folder with the project file reflects the relative path to where we will find the other files referenced in our project. This means that if we share our working folder with others, they can reproduce what we did so long as all files are contained within that project directory.\nProject files represent mileposts that set the relative path at which RStudio will look for files within our particular project. This is useful for creating a project file structure within which we can create reproducible and easily shareable analysis. You should be in the habit of starting each new analysis you do by setting up a new work folder with a new project file.\n\n\nProjects vs. Workspaces\nEach time you close your RStudio session, you will be faced with a temptation - RStudio will ask whether you want to save your workspace.\n\nWhat’s the difference between a script, a project, and a workspace? A script contains the code which tells R what you want it to do. Script commands are run in the console in the order they are written. You want to get in the habit of saving your script files often. Your project file tells R the path to look for files and folders at. This sets a relative path for your work in that project. A workspace file saves the exact image of what’s going on when you close out of RStudio - that may include loaded datasets, functions, and objects.\nYou have likely been tempted to save your workspace when you are closing your R session. While tempting, I encourage you not to get in the habit of saving your workspace. This runs counter to most other interactions you’ve had with computers - saving documents, video games, or survey responses. Saving your workspace can lead to bad habits with regards to reproducibility. You want to allow your scripts to serve as a repository for all steps that go from your raw data to your analysis and outputs. Unless your script has time or computationally intensive steps, your scripts and computer can re-run your workflow to return to the point that you left off when you last worked on the project.\n\n\nLater in the class, we’ll introduce more powerful strategies like the renv and targets packages which can help to manage your workflows and their dependencies.\n\n\n\n\nProject Setup\nEach project you create should have it’s own directory which will be a folder with a descriptive file name. This root folder will contain a directory structure which allows you to organize your data and workflows. There’s no standardized format which this directory structure has to take, however there are some useful principles that will help you stay organized.\n\nProject Organization Principles\n\nUse subdirectories to stay organized. While R doesn’t care whether your project directory contains subdirectories, subdirectories are useful for keeping track of your files. As you build out increasingly complicated projects, you will likelly generate lots of data, metadata, scripts, and outputs. Subdirectories can help you to keep all of this information organized, and will make it easier for others to navigate your project.\nKeep raw and processed data separate. We want to set up workflows that allow us to consistently be able to transition from raw data to processed data, to outputs (that will end up in our outputs folder). If our project relied upon locally stored datasets, raw data should be placed in a separate folder for raw data. After it exists, raw data should only be read - it shouldn’t be written to. Depending upon the considerations of your workflow, you may want to create a separate folder for processed data - unlike your raw data folder, you may both read and write data to this folder.\nMaintain a local copy of data documentation. Whenever possible, download or include data documentation to go along with your raw data. This will be useful to refer to as you’re working, and will also help others who make work on the project in the future. As you modify data and create your own datasets, you should also consider creating documentation for your working data. Again, this will help you or others in the future.\nOutputs get their own subdirectory. Outputs may include tables, graphics, or any other output created by your analysis. Depending upon the quantity of outputs, you may choose to add additional subdirectories to your output folder to help organize your outputs.\nOrder your scripts. In addition to creating a dedicated subdirectory for your scripts, consider naming them in such a way that they are ordered and follow your data workflow. You may also want to develop a master script that sequentially runs your other scripts. You can use source() in this master script to run other scripts.\nQuarto Markdown gets its own subdirectory. Depending upon the complexity of your project, it may be useful to creata a directory for any Quarto Markdown documents you create.\n\nYour base project directory and structure will end up looking something like this:\nRoot\n |\n |_ data\n |   |_ data_raw\n |   |_ data_processed\n |\n |_ documentation\n |_ output\n |_ scripts\n |   |_ 01_master.r\n |   |_ 02_get_data.r\n |   |_ 03_plots.r\n |_ quarto\nAgain, there’s no standardized format which your directory structure has to take. Your structure may also change depending upon the specific considerations and workflows involved for a given project. Over time, you’ll develop a default folder structure that makes sense for how you tend to work.\n\nWhen we assess your work in UP 570, we will be looking for evidence that you are making effective use of project directory structures. Unless we state so, there’s no required directory structure format, however, we want to see you applying principles of good directory management in your work.\n\n\n\nProject Organization Workflows\nAfter we create our project directory and create a r project file, we can programmatically create our directory structure using the dir.create() command. For example:\n\n\nCode\n# Create project subdirectories\n\ndir.create(\"data\")\ndir.create(\"data/data_raw\")\ndir.create(\"data/data_processed\")\ndir.create(\"documentation\")\ndir.create(\"output\")\ndir.create(\"scripts\")\ndir.create(\"quarto\")\n\n\nWe could also save ourselves repetition in our code using the base R lapply() syntax. We create a list with our project directory names and assign that into an object. We then feel lapply() the list along with the function we wish to apply to each element of that list - in this case, dir.create():\n\n\nCode\n# Create project subdirectories\n\nproject_dirs <- c(\"data\", \"data/data_raw\", \"data/data_processed\", \"documentation\", \"output\", \"scripts\", \"quarto\")\n\nlapply(project_dirs, dir.create)\n\n\n\n\nScript Headers and Structure\nWhile a lot of our coding can be done directly in Quarto Markdown documents, you’ll also want to use scripts to store code and develop workflows that may eventually be represented as output in your Quarto document. Script headers can help to organize the work we do in scripts, and can help when we get ready to share our work with others or pick up work on files we have not looked at in a while.\nTaking a few extra minutes to develop a nice script header will be beneficial in the future - your future self, your collaborators, and others who use your code will appreciate it!\nLike so many other R things, there’s no standardized example of what a script header needs to include, howevver, here’s a basic header template to start with:\n\n\nCode\n# Title: [Insert script title here]\n# Author: [Your name]\n# Email: [Your email address]\n# Date: [Insert date here]\n\n# Script Name: [Script Name]\n\n# Description: [Insert a brief description of the script and its purpose]\n\n# Notes: [Insert any process notes or potential revision plans here]\n\n# Set up the working environment\n\n# Load required libraries\nlibrary(library1)\nlibrary(library2)\n\n# Set global options\noptions(option1 = value1, option2 = value2)\n\n# Define functions\nfunction1 <- function(input1, input2) {\n  # function code goes here\n}\n\nfunction2 <- function(input1, input2) {\n  # function code goes here\n}\n\n# Main script\n\n# End of script\n\n\nThe different elements here help record who coded the script, what it is designed to do, and provides a common place to set options, load packages, and define functions. Although these components could occur anywhere in the script before they are called upon, includig them in the header helps track what’s going on in the script.\nWhile you could easily type out each element of your header every time that you develop a new script, you can also save your script as a code snippet within RStudio, which will allow you to easily insert the header template into new scripts you create.\n\n\nThis code snippet trick is paraphrased from the description provided by Dr. Timothy S. Farewell.\n\nIn RStudio, go to Tools -> Global Options -> Code -> Snippets -> Edit Snippets. This will bring up a block of code - scroll to the bottom.\nModify the below code header template to include your information and then copy / paste this into the snippets code block.\n\n\n\nCode\nsnippet header\n    # Title: \n    # Author:\n    # Email:\n    # Date:\n\n    # Script Name:\n\n    # Description:\n\n    # Notes:\n\n    # Working Environment Setup\n\n    # Load required libraries\n    load_packages <- c(\"tidyverse\", \"sf\") # Add more packages here as needed\n    lapply(load_packages, library, character.only = TRUE)\n\n    # Set global options\n\n    # Define functions\n\n\n 3. Save and close the window.\nYou have now added this snippet to your library. When you start a new script, simply type “header” and press tab and RStudio will fill in your header template.\n\n\nSnippets are handy - you may soon identify other common bits of code that you use frequently that you want to make snippets out of.\n\n\nNaming Files\nAs mentioned in the Project Organization Principles section, it’s important to spend a little time thinking about conventions for your filenames. Some useful principles:\n\nAvoid using special characters (for instance, \\ / : * ? ” < > |) in file names.\nDo not use spaces in filenames. You may use an underscore (e.g. map_census_tracts.png) to help with readability.\nIf you choose to use dates as part of your filename, order them yyy-mm-dd (e.g. 20230102 would be January 2, 2023). If you have multiple versions of the file from different dates, this ensures that your files will be properly ordered by the date.\n\n\n\nThis date format (yyy-mm-dd) conforms to ISO 8601 format.\n\nWhen creating ordered lists for file names (ordered by number) use at least two digits (e.g. “01_Main”, “02_Download”, “03_Map”). This will keep your files in order once you get past 10."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#github-lab-repository",
    "href": "assignments/labs/01_datapipeline.html#github-lab-repository",
    "title": "Building a Data Pipeline",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#workflow-2-working-in-quarto",
    "href": "assignments/labs/01_datapipeline.html#workflow-2-working-in-quarto",
    "title": "Building a Data Pipeline",
    "section": "Workflow 2: Working in Quarto",
    "text": "Workflow 2: Working in Quarto\nQuarto is a tool built into RStudio which allows you to create a variety of documents that can integrate plain text and code. It’s utility comes both in it’s ability to translate plain text into a variety of well-formatted outputs, but also due to the ease with which we can use this format to share code, analysis, and writing with others. Most of your written assignments in class will be produced in Quarto, therefore, you’ll want to become familiar with the basic syntax and logic.\nQuarto’s plain text formatting is handled using Markdown syntax which is designed to be simple, easily interpreted as text with formatting in place, and readable on most computers. An increasing number of note systems and word processors have adopted Markdown as their language.\n\n\nSome examples include Bear, Obsidian, Notion, Ulysses. Mentions are not endorsements.\n\nFormatting Your Documents\nQuarto makes use of a modified version of Markdown syntax. The Quarto website maintains excellent documentation regarding basic markdown formatting options as well as Quarto-specific formatting options:\n\nYou should familiarize yourself with how to format text, how to use section headings, and how to link files and insert references to other files.\n\n\nAdding References\nQuarto can help you either manage references in very sinple ways or more complex ways. You can either add references directly to your text as either footnotes or endnotes.\n\nFootnotes\nWe oftentimes use footnotes as an easy way to convey references or links to details or important information that does not need to be in our main body of text. Quarto allows you to create both footnotes and long notes. Footnotes are typically short and take up only a line or two. Long notes are longer and may contain multiple paragraphs.\n  \nHere is some text with a footnote.[^1]\n\n[^1]: And here is the footnote that accompanies that text in the footnotes section.\n\nHere is also some text with an inline footnote.^[Here is the text associated with the inline footnote - this may be easier to track because it is in line with the text.]\n\nHere is some text with a longnote.[^longnote]\n\n[^longnote]: And here is a longnote that takes up multiple lines. You can experiment in your text to see whether a footnote or longnote will serve you better for a given situation.\n  \n  This text is also part of the long note - the indent signals that it remains part of the note.\n  \nAnd this text would be part of the next paragraph that's outside of the note because it is not indented.\nHere is some text with a footnote1\nHere is also some text with an inline footnote.2\nHere is some text with a longnote3\nAnd this text would be part of the next paragraph that’s outside of the note because it is not indented.\n\n\nAsides\nYou can also add asides to documents which places a note in line with text in the margin.\n\n\nFor a very effective example of asides, see Kieran Healy’s book Data Visualization: A Practical Introduction.\n:::{.aside}\nHere is an aside placed in line with text in the margin.\n:::\n\n\nHere is an aside placed in line with text in the margin.\n\n\nCitations and Bibliographies\nFor longer or more advanced documents, you may wish to rely upon Quarto’s suppot for using and managing references with BibTex, a standard reference format. This allows you to add references to your document and then create in line citations and a formatted bibliography to accompany your writing.\nIn order to create a bibliography that is associated with a document, you’ll need three files:\n\nA main file written in Quarto Markdown (.qmd). This file will be formatted to include citation references.\nA bibliographic data source, typically a Bibtex (.bibtex) file.\nA csl file - this specifies the format for your bibliography. You can find and include different csl files to produce a bibliography using different reference styles.\n\n\nStep 1: Create a bibliography file.\nUsing a text editor, create a plain text file that will contain your bibliographic references. In this case, we’ll create a text file called “references.bib”. Note that this file should end in .bib to signify that it is a bibliographic reference file.\n\n\nStep 2: Refer to the bibliography file in your YAML Header\nNext, in your Quarto Markdown document, you’ll add a line to your YAML header that refers to the location and name of your bibliography file.\n\n\nCode\n---\ntitle: \"Neighborhood Analysis: Place Selection Memorandum\"\nbibliography: references.bib\n---\n\n\n\n\nStep 3: Define a citation style\nLike most other reference management systems, you can define what reference syle your references will be rendered as, and change this as needed. The Citation Style Language (CSL) project defines a common language and structure for styles. These are stored in .csl files which you will associate with your document. You can find and download .csl files in a range of formats from the CSL Project’s repository.\n\n\nYou can also find and download csl files in Zotero’s style repository. If you hover over a style format, it provides examples of what a citation will look like in your redered document.\nFor the sake of example, we’ll format our document using American Psychological Association (APA) format. We’ll search the CSL Project’s repository to find an appropriate APA style .csl file. Once you find the appropriate file, then download the file and place it in the same directory as our .bib file.\n Click on the .csl style format you want to use. You’ll then be taken to a page that includes the specific file’s contents.\n\nFrom here, you can right click on the button that says “raw” and download the .csl file.\n Note that before you move this file from your download folder to the folder where your .bib file is located, you may need to alter the file extension to indicate that this is a .csl file. Once you have done this, move it to the folder where your .bib file is located and then add a csl reference to your YAML header.\n\n\nCode\n---\ntitle: \"Neighborhood Analysis: Place Selection Memorandum\"\nbibliography: references.bib\ncsl: apa-6th-edition.csl\n---\n\n\n\n\nStep 4: Add references to your document\nYou’re now set up to add references to your document. You can add references in lots of different ways. If you’re searching for references using a service like Google Scholar, under the “cite” options for a given reference, you’ll see a URL to download the BibTeX citation associated with that reference.\n Click on BibTeX and the citation contents will show:\n\nCopy this text, and paste it into your bibliographic reference file (in our example, references.bib).\n\nYou can then cite this reference while you write by referring to it as follows:\n\n\nCode\n[@healy2018data] provides useful examples for leveraging GGPlot to create data visualization.\n\n\n(Healy, 2018) provides useful examples for leveraging GGPlot to create data visualization.\nNote that the actual reference corresponds to the first clause in the bibtex reference.\n\n\nYou can change the reference name if you’d like to something you’ll remember as you cite.\nYou can paste more references into the same references.bib file and cite them as well.\n\n\nYou can find more details on formatting citations on the Quarto website.\nOnline tools like Citation Machine can help you to build references for a range of online sources. (Feder, n.d.) takes a little while to get used to, but these tools can help you to build out an effective citation workflow.\n\n\nStep 5: Place your bibliography\nQuarto will automatically place your references at the end of your document (or wherever your style definition file calls for references to be placed). You can also explicitly tell Quarto where to place your references by creating a div as follows:\n\n\nCode\n\n## References\n\n::: {#refs}\n:::\n\n\nYou should end up with a formatted reference section at the end of your document.\n\n\n\n\n\nReference Considerations\nIt’s ultimately up to you how you manage and generate references within your documents. If you have a document with very few references, footnotes and/or endnotes may be an appropriate strategy. A more complex document with many references may be better managed using BibTeX. You can also use another reference management system and then export citations in BibTeX format which can then be added to your Quarto Markdown document."
  },
  {
    "objectID": "assignments/labs/01_datapipeline.html#lab-evaluation",
    "href": "assignments/labs/01_datapipeline.html#lab-evaluation",
    "title": "Building a Data Pipeline",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nImplementing best practices in root directory structure, file naming, and organization.\nFormatting of a script file including the implementation of a header.\nFormatting of the Quarto Markdown document to refer to analytic output and which uses formatting features to structure the document.\nA brief but appropriate description of population change in Chicago Community Areas.\nImplementation of a bibliography using the MLA format in your Quarto Markdown file.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "syllabus/index.html#evaluation-and-course-expectations",
    "href": "syllabus/index.html#evaluation-and-course-expectations",
    "title": "Syllabus",
    "section": "Evaluation and Course Expectations",
    "text": "Evaluation and Course Expectations\nYou will find detailed information on assignments, evaluation, and grading in the Assignments section.\n\nClass Attendance\nYou are expected to attend all of our class sessions in order to meet my standards for adequate performance in this course. Please notify me in advance of any course sessions which you will miss. Your final grades will be reduced by 1% per unexcused absence.\nFor those students who need to miss class due to a religious observance, please complete the Request for Accommodation for Religious Observances form should any instructors require an absence letter in order to manage the absence. In order to best facilitate planning and communication between students and faculty, we request that students make requests for absence letters as early as possible in the semester in which the request applies.\nFor more information on attendance policy as described in the University of Illinois Student Code, please see Sections 1-501 and 1-502.\n\n\nA Note on AI Tools\nArtificial intelligence tools like Chat GPT have quickly made waves with their ability to produce text, code, and explanations from natural language prompts. I encourage (and at times may even expect) you to integrate such tools into your problem solving strategies and workflow, especially with regards to coding in our class. These tools must, however, be used with care and with understanding around how they synthesize and produce information. A few words of guidance:\n\nAssume that any code produced will require additional tweaking or troubleshooting to be implemented effectively in your workflow.\nAssume any facts or figures produced are incorrect.\nUse these tools to help you break through coding or analysis challenges, not for writing up your narrative or findings.\nWhile you do not need to reference Chat GPT or other similar tools in your narrative or references, please indicate in any methods statements that these tools were employed. Please also reflect upon your application of these tools in your assignment submission reflections.\n\nThese tools are still emerging and are likely to evolve further, even over the course of the class. Let’s keep a running dialog about how you’re employing these tools, what some of the challenges are, and how you may want to integrate such technology into your workflows."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#goals",
    "href": "assignments/labs/02_sharing.html#goals",
    "title": "Sharing Your Work",
    "section": "Goals",
    "text": "Goals\n\nSet up your computer so that RStudio can communicate with Github.\nLearn several workflows for translating Quarto documents into simple websites.\nSubmit your previous lab via Github."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#core-concepts",
    "href": "assignments/labs/02_sharing.html#core-concepts",
    "title": "Sharing Your Work",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nTerminal\n\n\n\nGithub\n\nPush\nPull\nPull request\nRepository\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#github-lab-repository",
    "href": "assignments/labs/02_sharing.html#github-lab-repository",
    "title": "Sharing Your Work",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nLink to the Github lab repository. Please accept the Github Classroom assignment repository link for Lab 2: Sharing your Work."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#workflow-1-set-up-a-new-project",
    "href": "assignments/labs/02_sharing.html#workflow-1-set-up-a-new-project",
    "title": "Sharing Your Work",
    "section": "Workflow 1: Set up a New Project",
    "text": "Workflow 1: Set up a New Project"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#workflow-2-working-in-quarto",
    "href": "assignments/labs/02_sharing.html#workflow-2-working-in-quarto",
    "title": "Sharing Your Work",
    "section": "Workflow 2: Working in Quarto",
    "text": "Workflow 2: Working in Quarto"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#lab-evaluation",
    "href": "assignments/labs/02_sharing.html#lab-evaluation",
    "title": "Sharing Your Work",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nSuccessfully create and publish a quarto page via Quarto Pub.\nSuccessfully create a publish a quarto page based upon your first lab assignment via GitHub and Netlify.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#references",
    "href": "assignments/labs/02_sharing.html#references",
    "title": "Sharing Your Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/02_sharing.html",
    "href": "assignments/labs/02_sharing.html",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This lab focuses on getting your computer set up to communicate with Github and to introduce several workflows for publicly sharing your work. We will use these workflows throughout the course to share work and record feedback. Github represents a standard industry tool for code version control, collaborative development, debugging, and documentation.\nWhile code version control isn’t directly a focal point within practices of neighborhood analysis, we leverage it as a strategy to achieve one of our course goals. As we have discussed in our introductory framing to the course, transparency is an important principle of accountability within data analysis. Version control systems allow us to track changes in our code over time. These systems can help to facilitate accountability by making code accessible and easy to share. Version control systems also opens up the opportunities for collaboration, especially around the debugging of code.\n\n\n\n\n\nFor those people who have not used these systems before, there’s some core concepts to master. Once you do, integrating these systems into your workflow will become much easier. By the end of the class, you will have a lot of experience leveraging these tools as part of your workflows.\nVersion control systems like Github are also well integrated with several other tools that make it really easy to quickly and effectively share analysis via the internet. Again, this form of sharing is not integral to neighborhood analysis, but it does help us achieve a course goal focused on accessibility of analysis. While print and paper documents still rightfully have their place in our workflows, increasingly, our work is shared and consumed digitally. Being able to produce documents that are publicly accessible via the internet, therefore, becomes an important workflow for us to master.\n\n\nPlease read this lab background in its entirety before you proceed to engage the lab prompts."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#configure-your-computer-to-talk-to-github",
    "href": "assignments/labs/02_sharing.html#configure-your-computer-to-talk-to-github",
    "title": "Sharing Your Work",
    "section": "Configure Your Computer to Talk to Github",
    "text": "Configure Your Computer to Talk to Github\nOur first task is to configure your computer to talk to Github. This assumes that you already have a Github account set up, but that you have not linked this account to your local computer. A great resource that takes you step by step through Github workflows is Jenny Bryan’s Happy Git and GitHub for the useR. The recommended configuration below paraphrases the options found on that site.\nLet’s get started!\n\nRegister a Github Account\nGithub is an implementation of Git, which is a version control system designed to help you keep track of files (especially code) which are likely to be updated often. GitHub has some features similar to Dropbox or Box, but with far more emphasis on versioning your work and tracking changes. Within our class, we’ll use GitHub as a location where you’ll store work outputs which you will receive feedback.\n\n\nLet’s get this set up:\n\nIf you do not already have a GitHub account, go to GitHub.com and then click on Sign Up to create a new account. You will make a username (see some sage username advice here), enter your email address, and password, and then hit create account. I recommend using your UIUC email address for this step if you’re creating a new account (for my rationale, see the next step).\nGitHub offers some services for free which you can access with your basic account, however, as a student at an educational institution, you can register with GitHub for Education to receive enhanced benefits and services. You will be asked to verify your association with an academic institution, which you can do by entering your [netid](illinois.edu?) email address and taking a picture of your I-Card.\n\n\n\n\n\n\n\n\nConfigure Your Local Machine to Talk to GitHub\nYou can upload files directly to a GitHub repository in the GitHub web interface, but as you create more complex files and file structures, that’s not going to be a feasible way to manage your work. That strategy also overlooks most of the features of what GitHub is at its core - a version control system.\n\n\n\n\n\nWe might want to version our work for several reasons:\n\nTo keep track of changes we’ve made, so that we can track when code works and when we’ve broken it\nTo be able to share code with collaborators and work on the same files at the same time and then reconcile and combine work all together\nTo be able to share code and data publicly with others\n\nAll three of these rationales are important motivations within the context of our class. There are two strategies which you may want to try to integrate GitHub into your R workflow.\nRStudio has GitHub functionality built right in, which is very handy for integrating version control into your R workflow. Assuming you’ve already got R and RStudio installed on your system and have already set up a GitHub account, start here:\n\nCheck to see if Git is installed on your computer, and if it isnt, Install Git.\nAssociate git on your computer with your GitHub account.\nInstall a Git client like GitHub Desktop.\nConnect to GitHub\nConnect RStudio to GitHub\nGet Started using GitHub integrated with R. This video shows you how to communicate with your Github repository using RStudio:\n\n\n\n\n\n\n\nWe will ease our way into using Github, and will also devote some time to troubleshooting in class. I promise, it’s going to make sense soon!"
  },
  {
    "objectID": "assignments/labs/99_template.html#goals",
    "href": "assignments/labs/99_template.html#goals",
    "title": "Sharing Your Work",
    "section": "Goals",
    "text": "Goals\n\nSet up your computer so that RStudio can communicate with your Github account."
  },
  {
    "objectID": "assignments/labs/99_template.html#core-concepts",
    "href": "assignments/labs/99_template.html#core-concepts",
    "title": "Sharing Your Work",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\n[concepts here]\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/99_template.html#github-lab-repository",
    "href": "assignments/labs/99_template.html#github-lab-repository",
    "title": "Sharing Your Work",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/99_template.html#workflow-1-set-up-a-new-project",
    "href": "assignments/labs/99_template.html#workflow-1-set-up-a-new-project",
    "title": "Sharing Your Work",
    "section": "Workflow 1: Set up a New Project",
    "text": "Workflow 1: Set up a New Project"
  },
  {
    "objectID": "assignments/labs/99_template.html#workflow-2-working-in-quarto",
    "href": "assignments/labs/99_template.html#workflow-2-working-in-quarto",
    "title": "Sharing Your Work",
    "section": "Workflow 2: Working in Quarto",
    "text": "Workflow 2: Working in Quarto"
  },
  {
    "objectID": "assignments/labs/99_template.html#lab-evaluation",
    "href": "assignments/labs/99_template.html#lab-evaluation",
    "title": "Sharing Your Work",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/99_template.html#references",
    "href": "assignments/labs/99_template.html#references",
    "title": "Sharing Your Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#publish-using-posit-connect",
    "href": "assignments/labs/02_sharing.html#publish-using-posit-connect",
    "title": "Sharing Your Work",
    "section": "Publish using Posit Connect",
    "text": "Publish using Posit Connect"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#publish-using-quarto-pub",
    "href": "assignments/labs/02_sharing.html#publish-using-quarto-pub",
    "title": "Sharing Your Work",
    "section": "Publish using Quarto Pub",
    "text": "Publish using Quarto Pub\nQuarto Pub is a free service that allows you to quickly publish Quarto documents. Quarto Pub represents one of the easiest ways to publish documents.\n\n1. Create Account2. Preview3. Publish4. Options5. Finished!\n\n\nIn order to use Quarto Pub, you’ll need to create an account. Visit https://quartopub.com and create an account. I chose to use my GitHub username, but you can create an account with any name you wish (please keep in mind that the account name will become part of the URL which others will see).\n\n\n\nIt’s a good idea to preview your document before publishing it to see how it will look in a web browser. Render your document so that you can see what the final document will look like online.\n\n\nWhile we are using Quarto within RStudio, Quarto can also be run at the command line to render a range of document types. We will briefly use the command line in order to publish out document to Quarto Pub.\nIn your RStudio session, click on the terminal tab and type:\n\n\n\n\n\n\nTerminal\n\n\n\nquarto publish quarto-pub\n\n\nPut in plain language we are asking Quarto quarto to publish our document publish via Quarto Pub quarto-pub.\n\n\n\nYou may be prompted to log in to your Quarto Pub account on the internet before proceeding. Depeding upon the name of your document, you may also need to provide a shorter name (you will be prompted in the terminal window if that is the case). Quarto will then publish the document and will open a window that takes you to your published document.\n\n\n\nYou can then examine your published document at the URL that’s generated for you. For example: https://agreen4.quarto.pub/cca/\n\n\nIn the future if you need to update the document, you can run the publish step again."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#publish-using-github-and-netlify",
    "href": "assignments/labs/02_sharing.html#publish-using-github-and-netlify",
    "title": "Sharing Your Work",
    "section": "Publish using GitHub and Netlify",
    "text": "Publish using GitHub and Netlify\nQuarto Pub represents a really fast and easy option for creating and sharing Quarto documents. We’ll also learn another more involved strategy which you may choose to employ. This involves pushing published code and HTML content that represents a publishable document or website to Github and then linking this repository to a web hosting service like Netlify which will then publish the site for you.\nThis strategy can be useful in that you end up with two deliverables - your code base and associated HTML files on GitHub and a website hosted by Netlify. Our course website - the very website you are reading right now - is published using this pipeline. The webpages are each a Quarto Markdown file. The files are pushed to a Github repository. Netlify links to this repository and when new updates are pushed to Github, Netlify “refreshes” the website, typically within a minute or less of the push being complete.\nIf you simply want a quick and easy way to share your work, Quarto Pub may be completely sufficient! If you want more advanced web hosting options and configurability (like domain management), Netlify may be a more appropriate solution. We will learn both strategies, and you should take the time to become comfortable using either strategy.\n\nWhat is Netlify?\nNetlify is a web development platform that automates the building a serving of websites. While certain aspects of the Netlify platform are monetized, the types of basic elements which we can use to create and serve a website are available for free.\nWhen we grant Netlify access to a GitHub repository, it looks for files that can be rendered as html and then serves those at a unique URL. When we commit and push updates to that repository, Netlify updates the files that are being served to the internet.\nQuarto Markdown can produce outputs in a variety of formats including html. Netlify is built around looking for html files in our GitHub repository. Typically, when we render Quarto Markdown as HTML locally, it will create a _site file in our root folder. That folder has all of the content that we want Netlify to serve on the internet.\nWith these things in mind, let’s try getting our lab files on Netlify!"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#terminal",
    "href": "assignments/labs/02_sharing.html#terminal",
    "title": "Sharing Your Work",
    "section": "terminal",
    "text": "terminal\nquarto publish quarto-pub"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#finished",
    "href": "assignments/labs/02_sharing.html#finished",
    "title": "Sharing Your Work",
    "section": "4. Finished!",
    "text": "4. Finished!\nYou can then examine your published document at the URL that’s generated for you.\nhttps://agreen4.quarto.pub/cca/"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#create-account",
    "href": "assignments/labs/02_sharing.html#create-account",
    "title": "Sharing Your Work",
    "section": "1. Create Account",
    "text": "1. Create Account\nIn order to use Quarto Pub, you’ll need to create an account. Visit https://quartopub.com and create an account. I chose to use my GitHub username, but you can create an account with any name you wish (please keep in mind that the account name will become part of the URL which others will see)."
  },
  {
    "objectID": "assignments/labs/02_sharing.html#preview-document",
    "href": "assignments/labs/02_sharing.html#preview-document",
    "title": "Sharing Your Work",
    "section": "2. Preview Document",
    "text": "2. Preview Document"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#publish",
    "href": "assignments/labs/02_sharing.html#publish",
    "title": "Sharing Your Work",
    "section": "3. Publish",
    "text": "3. Publish\nWhile we are using Quarto within RStudio, Quarto can also be run at the command line to render a range of document types. We will briefly use the command line in order to publish out document to Quarto Pub.\nIn your RStudio session, click on the terminal tab and type:\n\n\n\n\n\n\nTerminal\n\n\n\nquarto publish quarto-pub\nPut in plain language we are asking Quarto quarto to publish our document publish via Quarto Pub quarto-pub.\nYou may be prompted to log in to your Quarto Pub account on the internet before proceeding. Depeding upon the name of your document, you may also need to provide a shorter name (you will be prompted in the terminal window if that is the case). Quarto will then publish the document and will open a window that takes you to your published document.\n\n4. Finished!\nYou can then examine your published document at the URL that’s generated for you.\nhttps://agreen4.quarto.pub/cca/"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#communicating-with-github-from-rstudio.",
    "href": "assignments/labs/02_sharing.html#communicating-with-github-from-rstudio.",
    "title": "Sharing Your Work",
    "section": "Communicating with GitHub from Rstudio.",
    "text": "Communicating with GitHub from Rstudio.\nAlthough this lab is formally introducing GitHub’s functionality as part of our class, you have already gotten some experience interacting manually with GitHub as you needed to download a repository in order to complete your first lab. From here on out, you can use RStudio to communicate directly with GitHub so that pushing and pulling repositories can all happen within your RStudio session. By the end of this lab, you will have gained some experience pushing and pulling files (specifically your lab files). From here on out in class, we will assume that you will use GitHub to submit your code for review. Instructions will be clearly posted with both lab and homework assignments.\nWith those things in mind, let’s think about how we communicate with Github from RStudio.\n\nBasic Workflows\nAs you have read on the Happy with GitR site, GitHub’s utility comes from allowing us to store remote versions of our code which others can then access, comment on, and modify. GitHub allows us to track versions of our code, and if we need to, we can revert to earlier versions, or fork branches - make special versions of our code which we may want to experiment with or modify. We can also reconcile multiple versions of code back into a single main version, which is particularly useful when we are collaboratively writing or editing code.\nFor the most part, our course focuses on building a workflow involving a single code author, and not on building collaborative workflows, although you should try collaborating with each other on modifying code. Our end goal in this class is to develop workflows for transparency, accountability, and accessibility.\n\n\nReviewing Common Operations\n\n\n\nImage by Allison Horst from https://allisonhorst.com\n\n\n\nLocal vs. Remote\nLocal vs Remote: Let’s start with the basic distinction between local and remote versions of our work. Local versions are contained on the hard drive of the computer we are working on. Remote versions are in the cloud, in this case, on GitHub’s servers. We will typically work on editing data locally, and will then push our modifications to be integrated into the remote version.\n\n\nPush vs. Pull\nPush vs Pull: To understand this, it’s useful to think about what’s happening from the perspective of your local computer. We pull a version of the repository from the remote repository to our local computer. We push a version of the repository from our local computer to the remote repository.\n\n\nCommit\nCommit: As we make changes to our local repository, GitHub is comparing these changes to the prior version of our repository. When we commit changes, we are essentially asking GitHub to create a markerpost that we could return to in the future. We even label our commits with a brief descriptive note so we can remember what that marker represents. Note that when you commit changes locally, they are not automatically pushed to your remote repository - you have to do that as an extra step. You can, for instance, make multiple local commits to a local repository and then push them to your remote repository.\nCommits help to keep us organized. While your commits do not have to be linear or follow any particular order within your data and analysis workflow, they can help you track contributions to what has been done.\n\n\n\nImage by Allison Horst from https://allisonhorst.com\n\n\nCommits serve as failsafe points - we can always revert or rewind our code to where we made commits if we need to debug or if something goes wrong. \n\n\n\nCommon Operations\nSo how do we put these pieces together?\n\nIf we already have a remote repository with code or a file structure in it, we create a connection to that remote repository and create a local repository by pulling it from GitHub.\nWe then make changes to the repository - this might be in RStudio, or we might add, move, or delete things manually.\nWhen we get to a point where we want to create a failsafe point in our code, we commit the changes we made (and add a plain text description of what our commit contains).\nWe push our commit to our remote version where it will become to version of record (GitHub is tracking the changes we made, so if we need to revert to our previous commit, we could).\nJust to be safe, after we push our commits, we may want to pull from the remote server again just to confirm that the version on our local machine is up to date.\n\nFrom there, we can continue making changes until we are ready to make another commit.\n\n\n\nImage by Allison Horst from https://allisonhorst.com\n\n\n\n\nWorkflow - Setting up a Local Repository from an Existing GitHub Repository\nFor interaction with our labs (which are already generated repositories on GitHub), you’ll want to follow the instructions for “Existing Project, GitHub First” on the Happy with Git and GitHub for the useR site. The site presents multiple ways to do this. I suggest starting with 16.2.2 where the instructions walk through using the RStudioIDE to connect to an existing remote GitHub repository.\nFor all UP 570 labs and assignments, existing repositories are provided for you (when you click the lab URL to accept the repository, a user-specific version is created for you from a master copy stored on GitHub). You can then connect to this repository in RStudio, create a local copy on your computer, modify that local repository by making changes, adding data and writing, as well as proejct documentation. You can then stage the changes you’ve made, commit your changes locally, and push your changes to the remote repository on GitHub."
  },
  {
    "objectID": "howto/PWBTN 2.0/PWBTN.html",
    "href": "howto/PWBTN 2.0/PWBTN.html",
    "title": "Communicating Quantitative Information",
    "section": "",
    "text": "This document draws heavily from Dr. Ed. Feser’s Professional Writing by the Numbers: For Planners and Policy Analysts, Version 4.0 (2006). Updated content focuses on digital-first communication strategies."
  },
  {
    "objectID": "howto/PWBTN 2.0/PWBTN.html#basic-communication-strategies",
    "href": "howto/PWBTN 2.0/PWBTN.html#basic-communication-strategies",
    "title": "Communicating Quantitative Information",
    "section": "Basic Communication Strategies",
    "text": "Basic Communication Strategies\n\nMemo Format\nMemos are often used by urban planners to share information within their agency or with other planners and local government officials. Memos tend to be concise documents that may contain either requested information for staff or recommendations on how to debate or take action around a particular policy issue. Memos are built around clear and concise writing with a logical structure, clear illustrations and visualizations, proper formatting and spelling.\nMemos start with a heading that includes the name of the memo author, name of the intended recipient, date, and a descriptive subject heading:\nMemorandum\n\nTo: Janet Jacobs, Planning Director\nFrom: Mae Q. Plannington, Planner 1\nDate: October 7, 2022\nSubject: Recommendation on Zoning Change for Holloway Parcel\nA memo is typically written in response to something. The first paragraph of the memo should describe what the memo is responding to, and should summarize what the writer of the memo has done to prepare the response. The first paragraph should also summarize any key findings or recommendations.\nInformation within a memo should be organized in a logical fashion. Section headings with descriptive titles help your audience find particular information more easily.\nGeneral information should be presented first followed by more detailed information. As you support information with evidence, you should follow the same progression of general to specific.\nMemos should conclude with a summary statement that encapsulates the key findings which you have come up with. If you are providing recommendations in your memo, this is also an appropriate place to re-state your recommendations, and provide instructions for how your memo recipient should follow up.\nBecause a memo is designed to be a concise document, you may have important additional information which would be useful to share, but which does not fit within the scope of the memorandum. Such items can be included in an appendix or attachment to the memo, and should be referenced as appropriate within the memo (e.g. “See the attached document for the property surveyor’s description of the Holloway parcel”).\n\n\nShort Report Format\nPresent your analysis in the format of a short report or “data brief.”\n\n\nDo Not Include Title Page\nTitle pages have their place in academic papers and larger, formal technical reports. However, they are inappropriate for memos and short data or policy briefs. Keep it simple.\n\n\nAvoid Headers and Footers in Memos\nRunning footers and headers also have their place for certain types of documents, but not for memos.\n\n\nNo Conclusion Needed\nIn other contexts, you have likely been taught to include both introductions and conclusions as ways to help transition in and out of different sections. The memo format is more direct, and does not require the use of opening and closing paragraphs. State you case and let it go at that.\n\n\nNotes and Citations\n\nUse Notes for Technical Explanations\nFootnotes or endnotes are best for stating brief technical explanations for methods and data. Reports may require a longer methodological appendix, but this is typically not appropriate for a memorandum.\n\n\nUse Notes for Citations\nUse an endnote or footnote to cite sources for memos and report briefs. Bibliographies or “Works Cited” lists are appropriate for longer reports or academic papers.\n\n\nCombine Data Source Citations\nIt makes good sense to use endnotes or footnotes to discuss data used in an analysis in a memo or report brief. But you do not necessarily need to use a separate note for each data source or series. Instead, combine them by introducing one note early in the document. Something like: “The data used in this analysis are from the U.S. Census Bureau’s 2020 5-year American Community Survey and 2020 Census Household Pulse Survey”.\n\n\n\nFormatting Considerations for Printing\nIncreasingly, memos and other routine planning communications are communicated digitally, however, it is important to prepare documents that will be legible when printed. In most circumstances, formatting considerations for printing will also apply to documents intended to be communicated digitally.\n\nLeft Justify Text\nUse left justification for text, and not full justification.\n\n\nInclude Page Numbers\nInclude page numbers in the bottom center or bottom right of your page.\n\n\nOne Inch Margins\nUse one inch margins on the left, right, top and bottom, including on all pages with tables and figures.\n\n\nTwelve point font\nUse a twelve point font, preferably something like Times for your text. You can use a ten point font for footnotes and endnotes. No text should be smaller than ten points."
  },
  {
    "objectID": "howto/PWBTN 2.0/PWBTN.html#style",
    "href": "howto/PWBTN 2.0/PWBTN.html#style",
    "title": "Communicating Quantitative Information",
    "section": "Style",
    "text": "Style\n\nAwkward Sentence Construction\nA catch-all category for a painful-to-read but not necessarily grammatically incorrect sentence. Sentence may be an affront to good writing style. Admittedly, professors often put AWK in places where they know something is amiss, but not being writing instructors, they’re not sure technically how to explain what is wrong. Bottom line: the sentence doesn’t work. Proofread yourself and then have someone else read to ensure clarity.\n\n\nWrite Directly\nDon’t mince words: come out and say it. Instead of “The BLS and BEA data show that employment in the microchip industry is very small,” write “Employment in the microchip industry is very small.” Or, instead of “It is very important to note that data disclosure rules preclude reporting employment for all sectors,” write “Data disclosure rules preclude. . .”\n\n\nAvoid Excessive Use of Jargon\nAvoid the planner’s pitfall: too much use of all those fancy planning terms and acronyms. It’s not impressive or erudite, just boring.\n\n\nAvoid Passive Voice\nThe extensive use of passive voice may be the single most common reason why a majority of sane individuals would rather walk on hot coals than read a technical document. Doesn’t “The region’s population growth dramatically outpaced the nation’s over the study period” sound better than “Using population data, it was found that the region grew much faster than the US over the study period”?\nAnother example: replace “A three-part analysis will be conducted in this paper” with “This paper will present a three-part analysis” or “In this paper, I will conduct a three-part analysis.” Reject passivity. Be active.\n\n\nUse Active Headings and Subheadings\nUse headings and subheadings to help organize your findings and discussion. In addition, keep ’em active and efficient. Instead of “Location Quotient Analysis” as a sub-heading, try a short title that conveys findings, e.g., “Region Specialized in Manufacturing and Services.” Headings and subheadings should never extend beyond one line of text.\n\n\nAvoid Unnecessary Equivocation\nSometimes it makes sense to offer caveats or otherwise “hedge your bets” when discussing a finding. However, don’t overdo it. Population growth doesn’t “seem fast.” It either is or is not fast, relative to something else (which you should be comparing it to). A location quotient indicates whether a region is or is not specialized in a given industry; it does not indicate that the region “seems specialized” or “may be specialized.” On the other hand, location quotients do not say much beyond specialization (you would not damn the torpedoes and argue, on the basis of a manufacturing LQ above 1.2, that the region is “highly competitive” in manufacturing).\nIf you find yourself making equivocations, it’s worthwhile to revisit the evidence you’ve provided to make a point to ensure you’re making the strongest argument possible. Communicate that argument confidently, and provide appropriate caveats when necessary.\n\n\nIn Memos, Don’t Stay What You’re Going to Say\nIn academic papers or longer reports it often makes sense to provide a roadmap to the document (e.g., “This report begins by summarizing major trends in population. It then. . .”). The adage that you should “say what you are going to say, say it, then say what you said” doesn’t apply for short policy and analysis pieces. Just get on with the analysis and findings. Strong organization and active headings will help your reader infer how you’ll make your points.\n\n\nPlaces Don’t Have Agency\nIn discussing social, economic, and demographic trends for neighborhoods, cities and regions, avoid implying that places have “agency.” Example:\nEl Paso shifted its population mix in response to major changes in Federal immigration policy.\nIn this case, El Paso as a collective, is being treated as an actor when it is actually simply a place with a collection of actors (people, households, businesses, organizations) who are reacting to the federal policy change in various ways.\nIt would be more accurate to say something like this:\nEl Paso’s population mix shifted in response to major changes in Federal immigration policy.\nThere may be cases when implying the place is an actor makes sense, for instance when you are discussing a community-wide strategy or policy.\n\n\nDiscuss Findings, Not Exhibits\nPeople don’t want to read about figures and tables. They want to read about trends that matter from your analysis. Avoid discussing exhibits. Instead, discuss findings, referring to figures and tables as supporting evidence.\nNo: “Figure 1 shows that poverty in center city neighborhoods in Cleveland is increasing.”\nYes: “Poverty in center city Cleveland is increasing (see Table 1).”"
  },
  {
    "objectID": "howto/PWBTN 2.0/PWBTN.html#usage",
    "href": "howto/PWBTN 2.0/PWBTN.html#usage",
    "title": "Communicating Quantitative Information",
    "section": "Usage",
    "text": "Usage\n\nPrint and Proof\nWe increasingly have an imperative to produce, edit, and disseminate our work using a completely electronic workflow. This has its benefits, but you need to develop a workflow that allows you to edit your draft with diligence. Printing and proofing a document by hand can often catch errors that would otherwise become lost in digital copy - such mistakes often reveal themselves in embarrassingly stark relief once disseminated.\n\n\nDefine Acronyms on First Use\nThe first time you use an acronym (e.g., HUD), spell it out, followed by the acronym in parentheses. “Data are from the U.S. Department of Housing and Urban Development (HUD).” Then use the acronym to your heart’s content.\n\n\nWrite out Numbers Less than 11\nAlthough sometimes the convention is that numbers one through ninety-nine should be spelled out. Numerals should be used for rates, percentages and other “data” indicators. Thus we would write “there are eight counties in the MSA,” but “the region grew by 8 percent.”\n\n\nSpell out Numbers Starting Sentences\nAny number that begins a sentence (or a bullet point) should be spelled out.\n\n\nWatch Your Capitalization\nNo need to capitalize industries, occupations, or other sectoral-type categories. Do not over-capitalize. Note that when referring to a single county, write “Tehama County.” But writing about multiple counties, it is “Tehama and Shasta counties.” Also: the “City of New York” but the “cities of Palo Alto and San Jose.”\n\n\nNo Apostrophe on Dates\nWhen referring to decades (e.g., 1990s), do not use an apostrophe.\n\n\nData are Plural\nNo: “The data is hard to find.”\nYes: “The data are hard to find.”\n\n\nUse Proper Note Punctuation\nNote numbers are best placed at the end of a sentence outside the punctuation.\nYes: This is a sentence that requires a citation.1\nNo: This is a sentence that requires a citation2.\n\n\nComplete Sentences in Notes\nFootnotes and endnotes should be complete sentences. Complete sentences have punctuation at the end.\n\n\nPunctuation Inside Quotes\nPunctuation generally goes inside quotes.\nYes: “Run, Tom, run,” said Jane.\nNo: “Down, Spot, down”, said Percival, Dick’s little-known cousin from Topeka.\n\n\nWrite out Percent\nOne of the few times the “less ink is better” rule is violated. Write “8 percent,” not “8%.” It’s less distracting to the eye.\n\n\nUse Arabic Numbers for Notes\nMicrosoft Word often defaults to the use of Roman numerals for notes and endnotes (i, ii, iii. . .). Change this option and use Arabic numerals. More efficient.\n\n\nCiting the URL is Not Enough\nIn the Internet age it has become distressingly common practice to cite only the URL for online documents. But consider this: if you quoted Tolstoy’s War and Peace using a copy you checked out from the New York Public Library, you would not cite the library as the source. The same principle applies to the Internet. For web sites, which are inherently Internet-based, you should list the name of the cite and then the URL. For documents accessed online, you should cite in the usual way (author, date, title, etc.) and then include the URL. Note that you are not obligated to list the URL for freestanding documents if you include the complete citation otherwise.\n\n\nAvoid Ampersand (&) in Text\nThe symbol “&”, known as the ampersand, should not be used in your text write-up. It is ok to use it for labels in tables and figures."
  },
  {
    "objectID": "howto/PWBTN 2.0/PWBTN.html#numbers",
    "href": "howto/PWBTN 2.0/PWBTN.html#numbers",
    "title": "Communicating Quantitative Information",
    "section": "Numbers",
    "text": "Numbers\n\nInterpret Quantities by Comparison\nDo not just report growth rates, quantities and other indicators for individual places. They are hard to interpret by themselves. For example, to back up a claim that your region has faced substantial population growth in the last decade, contrast its growth rate with the national average growth rate.\n\n\nExplain Regional Geography\nThe first time you mention your region, explain–either in the text or in a note–what its geographic composition is (e.g., its counties).\n\n\nSignificant Digits\nUse numbers with levels of precision that match the realistic precision in the underlying data. Should a location quotient be expressed as 1.709? No. Round to the nearest tenth (1.7). Percentage growth rates for subnational areas are also usually best expressed with one decimal place. Shares can be converted to percentages and expressed to one decimal place to make them more readable.\n\n\nOnly Include Exhibits You Reference in Text\nNo table, figure, chart or line drawing should be included in the report or report appendix unless it is referenced somewhere in the text discussion. That reference may be very brief (“see Table 4”), but it has to be there (it could also be in an endnote or footnote). Think of it this way: if it wasn’t important enough for you to mention it, why did you include it?\n\n\nReport the Quantity\nSometimes it is easy to forget to report the variable levels when we are analyzing variable trends. For example, it is common in analyses of wage trends (“wages are on the up and up”) to find nary a mention of wage levels. As a reader, you are left wondering “wages are going up, but are they high or low?” It is better to ground an analysis of changes in a given variable with a mention of the levels of the variable. So “The current annual average wage in River City for production workers is $27,500. That is up 12 percent in real terms since 1997. By contrast, the average production worker wage nationwide increased by 16.5 percent. . .”\n\n\nEmphasize Significant Findings\nWhen analyzing data we are looking for the most significant findings and often a “story” that helps explain those findings. In descriptive analysis, significance is often first assessed not in a statistical sense, but by looking for high and low values or major changes. But not all high/low values or large changes are necessarily important from a policy or planning point of view. Be careful to think through the potential implications of a finding before discussing it. Ask yourself: “So what?” If you can’t think of an answer, leave it out. (Example: “Since 1990, River City’s unemployment rate has registered below the U.S. rate in every quarter except IIQ 1996 and IVQ 1998” might be better stated as “River City’s unemployment rate has registered below the U.S. rate in 54 of the last 56 quarters.” A subtle change but the reader is not left asking: “Hmm, I wonder if I’m supposed to know why it was higher in those two periods?”\n\n\nPercent versus Percentage Points\nLet’s say you’re comparing the U.S. unemployment rate of 5.0 percent to the Peoria unemployment rate of 4.0 percent. Is the Peoria rate 1 percent lower than the national rate? No. It is 1 percentage point lower. A Peoria rate 1 percent lower than the national average would be 4.95 percent.\n\n\nRefer to Exhibits in Text\nRefer to your figures and tables directly in your text. Example: “Table 2 reports employment growth figures for the 1995 to 2003 period.” Or: “Employment growth was particularly strong in the retail and construction sectors (see Table 4).” If you are going to send readers to a table in a parenthetical phrase (like the last example), be sure to include the word “see.” So, you would not write: “Employment growth was particularly strong in the retail and construction sectors (Table 4).”"
  },
  {
    "objectID": "howto/PWBTN 2.0/PWBTN.html#tables-figures-and-graphics",
    "href": "howto/PWBTN 2.0/PWBTN.html#tables-figures-and-graphics",
    "title": "Communicating Quantitative Information",
    "section": "Tables, Figures and Graphics",
    "text": "Tables, Figures and Graphics\n\nMake ’Em so they Stand Alone\nFigures, graphs and tables should be constructed so that they can stand alone (as much as feasible). That is, someone could pick up and read the table without the accompanying text and get the gist of what it is trying to say. It goes without saying that sources of all data and calculations should be clearly indicated.\n\n\nAvoid Use of Grids in Tables\nThere is almost never any reason to include gridlines on a data table. Putting a line below the column headings and one below the last row of data, followed by the data source, is usually the best approach.\n\n\nDecimal Justify Data Columns\nLine up columns of numbers in tables on their explicit or implicit decimal points. Do not center justify numeric data.\n\n\nUse Descriptive Column Labels in Tables\nRemember the golden rule in table and figure construction: make it stand alone. That means column headings that someone can understand without reading the report body. Sometimes this is hard to do efficiently (headings can get too long). In such cases an alternative is to use short-hand headings but explain what they mean in a footnote to the table.\n\n\nUse Label Hierarchies in Tables\nA good way to make tables more efficient is to use hierarchical labeling of columns. In the table below, employment and payroll data for Illinois were available in 2003. But national and regional data were available only for 2001. The table efficiently reports the 2003 Illinois numbers along with national and Midwest growth rates for 1990-2001 and location quotients for 2001. A note at the bottom of the table should clarify how the Midwest is defined as well as what the reference area is for the location quotient.\n\n\n\nIndicate Source on Figures and Tables\nData sources should be indicated clearly on all figures and tables.\n\n\nUse Detailed Sources\nFor some series simply listing the data provider agency is not enough. For example, BLS reports several employment series. Therefore, list the series within BLS (or other) that you are using. For example, Regional Economic Information System, US Bureau of Economic Analysis, or Covered Wages and Employment, US Bureau of Labor Statistics.\n\n\nPie Chart? No Thanks\nWhile seductive to the eye, pie charts are far less interpretable than a simple bar chart. Go for conveying your findings clearly, not spicing up your document with spurious graphical devices.\n\n\nShun 3-D\nThree dimensional bar charts and other graphics generally should be avoided. They usually compromise proper interpretation of the findings. Keep it simple: no 3-D.\n\n\nCall Tables “Tables” and Figures “Figures”\nBy convention, tables are called “tables” and graphics such as data charts or line drawings are called “figures.” Sometimes people use the word “exhibit” to label a line drawing. Avoid use of the terms “chart” and “graphic” for labels (e.g., Chart 1, Graphic 1). Also, never call a table a figure or a figure a table. Note that exhibit labels are generally capitalized: “Population trends for River City are summarized in Table 4.”\n\n\nSequence Tables and Figures in Order in the Text\nIf you have three tables, do not discuss Table 1, then Table 3, then Table 2. If the results in Table 3 are mentioned prior to those in Table 2, renumber the tables. Same for figures or any other exhibit.\n\n\nUse Commas for Numbers\nIn data tables, include commas in the number formatting to denote significant digits. So: $47,500 instead of $47500.\n\n\nAvoid Grids on Figures\nMany graphical packages default settings include gridlines on charts. Most charts are simple enough not to need them to be clearly interpreted. Get rid of the clutter and save some ink!\n\n\nDon’t Rely on Color\nRemember that your best intents for people to view your document in color may be foiled - some individuals may print documents in grayscale, and others may be colorblind and unable to perceive all colors. That means your document should not depend on color to convey its findings. An example of depending on color would be to use red text in your tables to indicate negative values or any color to highlight interesting trends.\n\n\nReport Numbers in 000s\nIt is ok to report very large numbers in thousands or even millions in tables if the precision in the original units is unnecessary. It is probably not critical in most cases for people to know that the US population changed by 1,456,789 over a given period (1.46 million will probably suffice). For small areas, such as counties and regions, reporting the original units is usually preferred except for large number variables (e.g., total dollar income).\n\n\nNo Superfluous Material\nAvoid the practice of tacking on government documents or tables to your reports as “general points of information.” The idea in most professional writing is to convey maximum information with minimum material; tossing in nonessential material from secondary sources defeats that aim."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html",
    "href": "howto/PWBTN/PWBTN.html",
    "title": "Communicating Quantitative Information",
    "section": "",
    "text": "This document draws heavily from Dr. Ed. Feser’s Professional Writing by the Numbers: For Planners and Policy Analysts, Version 4.0 (2006). Updated content focuses on digital-first communication strategies."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#basic-communication-strategies",
    "href": "howto/PWBTN/PWBTN.html#basic-communication-strategies",
    "title": "Communicating Quantitative Information",
    "section": "Basic Communication Strategies",
    "text": "Basic Communication Strategies\n\nMemo Format\nMemos are often used by urban planners to share information within their agency or with other planners and local government officials. Memos tend to be concise documents that may contain either requested information for staff or recommendations on how to debate or take action around a particular policy issue. Memos are built around clear and concise writing with a logical structure, clear illustrations and visualizations, proper formatting and spelling.\nMemos start with a heading that includes the name of the memo author, name of the intended recipient, date, and a descriptive subject heading:\nMemorandum\n\nTo: Janet Jacobs, Planning Director\nFrom: Mae Q. Plannington, Planner 1\nDate: January 7, 2023\nSubject: Recommendation on Zoning Change for Holloway Parcel\nA memo is typically written in response to something. The first paragraph of the memo should describe what the memo is responding to, and should summarize what the writer of the memo has done to prepare the response. The first paragraph should also summarize any key findings or recommendations.\nInformation within a memo should be organized in a logical fashion. Section headings with descriptive titles help your audience find particular information more easily.\nGeneral information should be presented first followed by more detailed information. As you support information with evidence, you should follow the same progression of general to specific.\nMemos should conclude with a summary statement that encapsulates the key findings which you have come up with. If you are providing recommendations in your memo, this is also an appropriate place to re-state your recommendations, and provide instructions for how your memo recipient should follow up.\nBecause a memo is designed to be a concise document, you may have important additional information which would be useful to share, but which does not fit within the scope of the memorandum. Such items can be included in an appendix or attachment to the memo, and should be referenced as appropriate within the memo (e.g. “See the attached document for the property surveyor’s description of the Holloway parcel”).\n\n\nShort Report Format\nPresent your analysis in the format of a short report or “data brief.”\n\n\nDo Not Include Title Page\nTitle pages have their place in academic papers and larger, formal technical reports. However, they are inappropriate for memos and short data or policy briefs. Keep it simple.\n\n\nAvoid Headers and Footers in Memos\nRunning footers and headers also have their place for certain types of documents, but not for memos.\n\n\nNo Conclusion Needed\nIn other contexts, you have likely been taught to include both introductions and conclusions as ways to help transition in and out of different sections. The memo format is more direct, and does not require the use of opening and closing paragraphs. State you case and let it go at that.\n\n\nNotes and Citations\n\nUse Notes for Technical Explanations\nFootnotes or endnotes are best for stating brief technical explanations for methods and data. Reports may require a longer methodological appendix, but this is typically not appropriate for a memorandum.\n\n\nUse Notes for Citations\nUse an endnote or footnote to cite sources for memos and report briefs. Bibliographies or “Works Cited” lists are appropriate for longer reports or academic papers.\n\n\nCombine Data Source Citations\nIt makes good sense to use endnotes or footnotes to discuss data used in an analysis in a memo or report brief. But you do not necessarily need to use a separate note for each data source or series. Instead, combine them by introducing one note early in the document. Something like: “The data used in this analysis are from the U.S. Census Bureau’s 2020 5-year American Community Survey and 2020 Census Household Pulse Survey”.\n\n\n\nFormatting Considerations for Printing\nIncreasingly, memos and other routine planning communications are communicated digitally, however, it is important to prepare documents that will be legible when printed. In most circumstances, formatting considerations for printing will also apply to documents intended to be communicated digitally.\n\nLeft Justify Text\nUse left justification for text, and not full justification.\n\n\nInclude Page Numbers\nInclude page numbers in the bottom center or bottom right of your page.\n\n\nOne Inch Margins\nUse one inch margins on the left, right, top and bottom, including on all pages with tables and figures.\n\n\nTwelve point font\nUse a twelve point font, preferably something like Times for your text. You can use a ten point font for footnotes and endnotes. No text should be smaller than ten points."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#style",
    "href": "howto/PWBTN/PWBTN.html#style",
    "title": "Communicating Quantitative Information",
    "section": "Style",
    "text": "Style\n\nAwkward Sentence Construction\nA catch-all category for a painful-to-read but not necessarily grammatically incorrect sentence. Sentence may be an affront to good writing style. Admittedly, professors often put AWK in places where they know something is amiss, but not being writing instructors, they’re not sure technically how to explain what is wrong. Bottom line: the sentence doesn’t work. Proofread yourself and then have someone else read to ensure clarity.\n\n\nWrite Directly\nDon’t mince words: come out and say it. Instead of “The BLS and BEA data show that employment in the microchip industry is very small,” write “Employment in the microchip industry is very small.” Or, instead of “It is very important to note that data disclosure rules preclude reporting employment for all sectors,” write “Data disclosure rules preclude. . .”\n\n\nAvoid Excessive Use of Jargon\nAvoid the planner’s pitfall: too much use of all those fancy planning terms and acronyms. It’s not impressive or erudite, just boring.\n\n\nAvoid Passive Voice\nThe extensive use of passive voice may be the single most common reason why a majority of sane individuals would rather walk on hot coals than read a technical document. Doesn’t “The region’s population growth dramatically outpaced the nation’s over the study period” sound better than “Using population data, it was found that the region grew much faster than the US over the study period”?\nAnother example: replace “A three-part analysis will be conducted in this paper” with “This paper will present a three-part analysis” or “In this paper, I will conduct a three-part analysis.” Reject passivity. Be active.\n\n\nUse Active Headings and Subheadings\nUse headings and subheadings to help organize your findings and discussion. In addition, keep ’em active and efficient. Instead of “Location Quotient Analysis” as a sub-heading, try a short title that conveys findings, e.g., “Region Specialized in Manufacturing and Services.” Headings and subheadings should never extend beyond one line of text.\n\n\nAvoid Unnecessary Equivocation\nSometimes it makes sense to offer caveats or otherwise “hedge your bets” when discussing a finding. However, don’t overdo it. Population growth doesn’t “seem fast.” It either is or is not fast, relative to something else (which you should be comparing it to). A location quotient indicates whether a region is or is not specialized in a given industry; it does not indicate that the region “seems specialized” or “may be specialized.” On the other hand, location quotients do not say much beyond specialization (you would not damn the torpedoes and argue, on the basis of a manufacturing LQ above 1.2, that the region is “highly competitive” in manufacturing).\nIf you find yourself making equivocations, it’s worthwhile to revisit the evidence you’ve provided to make a point to ensure you’re making the strongest argument possible. Communicate that argument confidently, and provide appropriate caveats when necessary.\n\n\nIn Memos, Don’t Stay What You’re Going to Say\nIn academic papers or longer reports it often makes sense to provide a roadmap to the document (e.g., “This report begins by summarizing major trends in population. It then. . .”). The adage that you should “say what you are going to say, say it, then say what you said” doesn’t apply for short policy and analysis pieces. Just get on with the analysis and findings. Strong organization and active headings will help your reader infer how you’ll make your points.\n\n\nPlaces Don’t Have Agency\nIn discussing social, economic, and demographic trends for neighborhoods, cities and regions, avoid implying that places have “agency.” Example:\nEl Paso shifted its population mix in response to major changes in Federal immigration policy.\nIn this case, El Paso as a collective, is being treated as an actor when it is actually simply a place with a collection of actors (people, households, businesses, organizations) who are reacting to the federal policy change in various ways.\nIt would be more accurate to say something like this:\nEl Paso’s population mix shifted in response to major changes in Federal immigration policy.\nThere may be cases when implying the place is an actor makes sense, for instance when you are discussing a community-wide strategy or policy.\n\n\nDiscuss Findings, Not Exhibits\nPeople don’t want to read about figures and tables. They want to read about trends that matter from your analysis. Avoid discussing exhibits. Instead, discuss findings, referring to figures and tables as supporting evidence.\nNo: “Figure 1 shows that poverty in center city neighborhoods in Cleveland is increasing.”\nYes: “Poverty in center city Cleveland is increasing (see Table 1).”"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#usage",
    "href": "howto/PWBTN/PWBTN.html#usage",
    "title": "Communicating Quantitative Information",
    "section": "Usage",
    "text": "Usage\n\nPrint and Proof\nWe increasingly have an imperative to produce, edit, and disseminate our work using a completely electronic workflow. This has its benefits, but you need to develop a workflow that allows you to edit your draft with diligence. Printing and proofing a document by hand can often catch errors that would otherwise become lost in digital copy - such mistakes often reveal themselves in embarrassingly stark relief once disseminated.\n\n\nDefine Acronyms on First Use\nThe first time you use an acronym (e.g., HUD), spell it out, followed by the acronym in parentheses. “Data are from the U.S. Department of Housing and Urban Development (HUD).” Then use the acronym to your heart’s content.\n\n\nWrite out Numbers Less than 11\nAlthough sometimes the convention is that numbers one through ninety-nine should be spelled out. Numerals should be used for rates, percentages and other “data” indicators. Thus we would write “there are eight counties in the MSA,” but “the region grew by 8 percent.”\n\n\nSpell out Numbers Starting Sentences\nAny number that begins a sentence (or a bullet point) should be spelled out.\n\n\nWatch Your Capitalization\nNo need to capitalize industries, occupations, or other sectoral-type categories. Do not over-capitalize. Note that when referring to a single county, write “Tehama County.” But writing about multiple counties, it is “Tehama and Shasta counties.” Also: the “City of New York” but the “cities of Palo Alto and San Jose.”\n\n\nNo Apostrophe on Dates\nWhen referring to decades (e.g., 1990s), do not use an apostrophe.\n\n\nData are Plural\nNo: “The data is hard to find.”\nYes: “The data are hard to find.”\n\n\nUse Proper Note Punctuation\nNote numbers are best placed at the end of a sentence outside the punctuation.\nYes: This is a sentence that requires a citation.1\nNo: This is a sentence that requires a citation2.\n\n\nComplete Sentences in Notes\nFootnotes and endnotes should be complete sentences. Complete sentences have punctuation at the end.\n\n\nPunctuation Inside Quotes\nPunctuation generally goes inside quotes.\nYes: “Run, Tom, run,” said Jane.\nNo: “Down, Spot, down”, said Percival, Dick’s little-known cousin from Topeka.\n\n\nWrite out Percent\nOne of the few times the “less ink is better” rule is violated. Write “8 percent,” not “8%.” It’s less distracting to the eye.\n\n\nUse Arabic Numbers for Notes\nMicrosoft Word often defaults to the use of Roman numerals for notes and endnotes (i, ii, iii. . .). Change this option and use Arabic numerals. More efficient.\n\n\nCiting the URL is Not Enough\nIn the Internet age it has become distressingly common practice to cite only the URL for online documents. But consider this: if you quoted Tolstoy’s War and Peace using a copy you checked out from the New York Public Library, you would not cite the library as the source. The same principle applies to the Internet. For web sites, which are inherently Internet-based, you should list the name of the cite and then the URL. For documents accessed online, you should cite in the usual way (author, date, title, etc.) and then include the URL. Note that you are not obligated to list the URL for freestanding documents if you include the complete citation otherwise.\n\n\nAvoid Ampersand (&) in Text\nThe symbol “&”, known as the ampersand, should not be used in your text write-up. It is ok to use it for labels in tables and figures."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#numbers",
    "href": "howto/PWBTN/PWBTN.html#numbers",
    "title": "Communicating Quantitative Information",
    "section": "Numbers",
    "text": "Numbers\n\nInterpret Quantities by Comparison\nDo not just report growth rates, quantities and other indicators for individual places. They are hard to interpret by themselves. For example, to back up a claim that your region has faced substantial population growth in the last decade, contrast its growth rate with the national average growth rate.\n\n\nExplain Regional Geography\nThe first time you mention your region, explain–either in the text or in a note–what its geographic composition is (e.g., its counties).\n\n\nSignificant Digits\nUse numbers with levels of precision that match the realistic precision in the underlying data. Should a location quotient be expressed as 1.709? No. Round to the nearest tenth (1.7). Percentage growth rates for subnational areas are also usually best expressed with one decimal place. Shares can be converted to percentages and expressed to one decimal place to make them more readable.\n\n\nOnly Include Exhibits You Reference in Text\nNo table, figure, chart or line drawing should be included in the report or report appendix unless it is referenced somewhere in the text discussion. That reference may be very brief (“see Table 4”), but it has to be there (it could also be in an endnote or footnote). Think of it this way: if it wasn’t important enough for you to mention it, why did you include it?\n\n\nReport the Quantity\nSometimes it is easy to forget to report the variable levels when we are analyzing variable trends. For example, it is common in analyses of wage trends (“wages are on the up and up”) to find nary a mention of wage levels. As a reader, you are left wondering “wages are going up, but are they high or low?” It is better to ground an analysis of changes in a given variable with a mention of the levels of the variable. So “The current annual average wage in River City for production workers is $27,500. That is up 12 percent in real terms since 1997. By contrast, the average production worker wage nationwide increased by 16.5 percent. . .”\n\n\nEmphasize Significant Findings\nWhen analyzing data we are looking for the most significant findings and often a “story” that helps explain those findings. In descriptive analysis, significance is often first assessed not in a statistical sense, but by looking for high and low values or major changes. But not all high/low values or large changes are necessarily important from a policy or planning point of view. Be careful to think through the potential implications of a finding before discussing it. Ask yourself: “So what?” If you can’t think of an answer, leave it out. (Example: “Since 1990, River City’s unemployment rate has registered below the U.S. rate in every quarter except IIQ 1996 and IVQ 1998” might be better stated as “River City’s unemployment rate has registered below the U.S. rate in 54 of the last 56 quarters.” A subtle change but the reader is not left asking: “Hmm, I wonder if I’m supposed to know why it was higher in those two periods?”\n\n\nPercent versus Percentage Points\nLet’s say you’re comparing the U.S. unemployment rate of 5.0 percent to the Peoria unemployment rate of 4.0 percent. Is the Peoria rate 1 percent lower than the national rate? No. It is 1 percentage point lower. A Peoria rate 1 percent lower than the national average would be 4.95 percent.\n\n\nRefer to Exhibits in Text\nRefer to your figures and tables directly in your text. Example: “Table 2 reports employment growth figures for the 1995 to 2003 period.” Or: “Employment growth was particularly strong in the retail and construction sectors (see Table 4).” If you are going to send readers to a table in a parenthetical phrase (like the last example), be sure to include the word “see.” So, you would not write: “Employment growth was particularly strong in the retail and construction sectors (Table 4).”"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#tables-figures-and-graphics",
    "href": "howto/PWBTN/PWBTN.html#tables-figures-and-graphics",
    "title": "Communicating Quantitative Information",
    "section": "Tables, Figures and Graphics",
    "text": "Tables, Figures and Graphics\n\nMake ’Em so they Stand Alone\nFigures, graphs and tables should be constructed so that they can stand alone (as much as feasible). That is, someone could pick up and read the table without the accompanying text and get the gist of what it is trying to say. It goes without saying that sources of all data and calculations should be clearly indicated.\n\n\nAvoid Use of Grids in Tables\nThere is almost never any reason to include gridlines on a data table. Putting a line below the column headings and one below the last row of data, followed by the data source, is usually the best approach.\n\n\nDecimal Justify Data Columns\nLine up columns of numbers in tables on their explicit or implicit decimal points. Do not center justify numeric data.\n\n\nUse Descriptive Column Labels in Tables\nRemember the golden rule in table and figure construction: make it stand alone. That means column headings that someone can understand without reading the report body. Sometimes this is hard to do efficiently (headings can get too long). In such cases an alternative is to use short-hand headings but explain what they mean in a footnote to the table.\n\n\nUse Label Hierarchies in Tables\nA good way to make tables more efficient is to use hierarchical labeling of columns. In the table below, employment and payroll data for Illinois were available in 2003. But national and regional data were available only for 2001. The table efficiently reports the 2003 Illinois numbers along with national and Midwest growth rates for 1990-2001 and location quotients for 2001. A note at the bottom of the table should clarify how the Midwest is defined as well as what the reference area is for the location quotient.\n\n\n\nIndicate Source on Figures and Tables\nData sources should be indicated clearly on all figures and tables.\n\n\nUse Detailed Sources\nFor some series simply listing the data provider agency is not enough. For example, BLS reports several employment series. Therefore, list the series within BLS (or other) that you are using. For example, Regional Economic Information System, US Bureau of Economic Analysis, or Covered Wages and Employment, US Bureau of Labor Statistics.\n\n\nPie Chart? No Thanks\nWhile seductive to the eye, pie charts are far less interpretable than a simple bar chart. Go for conveying your findings clearly, not spicing up your document with spurious graphical devices.\n\n\nShun 3-D\nThree dimensional bar charts and other graphics generally should be avoided. They usually compromise proper interpretation of the findings. Keep it simple: no 3-D.\n\n\nCall Tables “Tables” and Figures “Figures”\nBy convention, tables are called “tables” and graphics such as data charts or line drawings are called “figures.” Sometimes people use the word “exhibit” to label a line drawing. Avoid use of the terms “chart” and “graphic” for labels (e.g., Chart 1, Graphic 1). Also, never call a table a figure or a figure a table. Note that exhibit labels are generally capitalized: “Population trends for River City are summarized in Table 4.”\n\n\nSequence Tables and Figures in Order in the Text\nIf you have three tables, do not discuss Table 1, then Table 3, then Table 2. If the results in Table 3 are mentioned prior to those in Table 2, renumber the tables. Same for figures or any other exhibit.\n\n\nUse Commas for Numbers\nIn data tables, include commas in the number formatting to denote significant digits. So: $47,500 instead of $47500.\n\n\nAvoid Grids on Figures\nMany graphical packages default settings include gridlines on charts. Most charts are simple enough not to need them to be clearly interpreted. Get rid of the clutter and save some ink!\n\n\nDon’t Rely on Color\nRemember that your best intents for people to view your document in color may be foiled - some individuals may print documents in grayscale, and others may be colorblind and unable to perceive all colors. That means your document should not depend on color to convey its findings. An example of depending on color would be to use red text in your tables to indicate negative values or any color to highlight interesting trends.\n\n\nReport Numbers in 000s\nIt is ok to report very large numbers in thousands or even millions in tables if the precision in the original units is unnecessary. It is probably not critical in most cases for people to know that the US population changed by 1,456,789 over a given period (1.46 million will probably suffice). For small areas, such as counties and regions, reporting the original units is usually preferred except for large number variables (e.g., total dollar income).\n\n\nNo Superfluous Material\nAvoid the practice of tacking on government documents or tables to your reports as “general points of information.” The idea in most professional writing is to convey maximum information with minimum material; tossing in nonessential material from secondary sources defeats that aim."
  },
  {
    "objectID": "schedule/09_census.html",
    "href": "schedule/09_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will work on our lab focused on exploring population and the census."
  },
  {
    "objectID": "schedule/09_census.html#before-class",
    "href": "schedule/09_census.html#before-class",
    "title": "Population and the Census",
    "section": "Before Class",
    "text": "Before Class\nAccept the GitHub Classroom invitation to our lab repository and use RStudio to pull the repository to your local computer.\nRead our lab background so you are prepared to start working through the lab repository notebook.\nIf you do not already have one, please register for a U.S. Census Bureau API Key which we will need for the tidycensus package."
  },
  {
    "objectID": "schedule/09_census.html#reflect",
    "href": "schedule/09_census.html#reflect",
    "title": "Population and the Census",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/09_census.html#slides",
    "href": "schedule/09_census.html#slides",
    "title": "Population and the Census",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/09_census.html#resources-for-further-exploration",
    "href": "schedule/09_census.html#resources-for-further-exploration",
    "title": "Population and the Census",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/04_sharing.html#before-class",
    "href": "schedule/04_sharing.html#before-class",
    "title": "Sharing Your Work",
    "section": "Before Class",
    "text": "Before Class\n\nRead through the entire lab background description before approaching lab tasks.\nBe prepared to access the formatted Quarto notebook you worked on in the last lab that contains your analysis of Chicago community areas.\nBe prepared to access your Lab 1 reflection.\n\nD’Ignazio, Catherine, and Lauren F. Klein. (2020). Data Feminism. MIT Press. Chapter 3 , Chapter 4"
  },
  {
    "objectID": "schedule/04_sharing.html#reflect",
    "href": "schedule/04_sharing.html#reflect",
    "title": "Sharing Your Work",
    "section": "Reflect",
    "text": "Reflect\n\nHow can planners and others engaging directly in public policy discourse and debate leverage emotion in their analysis in ways that generate meaning and connection without manipulating or leading towards particular conclusions?\nWhat does the rhetorics of design look like today? How do we reclaim the rhetorical roots of data analysis amidst the proliferation of technocal approaches and (overly) abundant data?\nIs there such thing a “neutral” data analysis?\nCan you think of classification systems that may have unintended consequences or biases in data that you’ve used for urban analysis in the past?"
  },
  {
    "objectID": "schedule/04_sharing.html#slides",
    "href": "schedule/04_sharing.html#slides",
    "title": "Sharing Your Work",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/04_sharing.html#resources-for-further-exploration",
    "href": "schedule/04_sharing.html#resources-for-further-exploration",
    "title": "Sharing Your Work",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/05_permit.html#before-class",
    "href": "schedule/05_permit.html#before-class",
    "title": "Earning your Learner’s Permit",
    "section": "Before Class",
    "text": "Before Class\n\nTake a look at these instructions.\nAccept the lab repository, link to Github, and create a local version of the repository.\nCome to class with any initial questions you have about the Learner’s Permit of what you’re being asked."
  },
  {
    "objectID": "schedule/05_permit.html#reflect",
    "href": "schedule/05_permit.html#reflect",
    "title": "Earning your Learner’s Permit",
    "section": "Reflect",
    "text": "Reflect\n\nHow do you typically approach exploring unfamiliar data? What types of questions help you find a direction?\nWhat kinds of stories might data on code violations help us tell?\nWhat types of information are missing from these datasets? What questions come up as you complete your labs?"
  },
  {
    "objectID": "schedule/05_permit.html#slides",
    "href": "schedule/05_permit.html#slides",
    "title": "NAME",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/05_permit.html#resources-for-further-exploration",
    "href": "schedule/05_permit.html#resources-for-further-exploration",
    "title": "Earning your Learner’s Permit",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nNew York City Code Violations \nNew York City PLUTO Data"
  },
  {
    "objectID": "schedule/06_places.html#before-class",
    "href": "schedule/06_places.html#before-class",
    "title": "Describing Places",
    "section": "Before Class",
    "text": "Before Class\nD’Ignazio, Catherine, and Lauren F. Klein. (2020). Data Feminism. MIT Press. Chapter 5 , Chapter 6 , Chapter 7 \nLynch, Kevin. (1960). The Image of the City. Chapter 3 ."
  },
  {
    "objectID": "schedule/06_places.html#reflect",
    "href": "schedule/06_places.html#reflect",
    "title": "Describing Places",
    "section": "Reflect",
    "text": "Reflect\n\nWe intuitively tell lots of stories about places. What are some of the most evocative tropes you can think of that could apply to places? What makes for a good story about place?\nWhat are some of the benefits and challenges of making place comparisons? How does this feel from the perspective of an analyst? What dangers or challenges does the analyst face when telling comparative stories to others?\nYour readings from Data Feminism discuss embracing pluralism in data storytelling. What might this look like in the context of describing systems of neighborhoods?"
  },
  {
    "objectID": "schedule/06_places.html#slides",
    "href": "schedule/06_places.html#slides",
    "title": "Describing Places",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/06_places.html#resources-for-further-exploration",
    "href": "schedule/06_places.html#resources-for-further-exploration",
    "title": "Describing Places",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nAnti-Eviction Mapping Project \nEviction Lab Methodology \nBloomberg: What’s Really Warming the World \nDesign Justice Project \nGlobal Atlas of Environmental Justice \nFive Thirty Eight: Kidnapping of Girls in Nigeria Is Part of a Worsening Problem \nMemorandum on Transparency and Open Government"
  },
  {
    "objectID": "schedule/07_places.html#before-class",
    "href": "schedule/07_places.html#before-class",
    "title": "Describing Places",
    "section": "Before Class",
    "text": "Before Class\nAccept the GitHub Classroom invitation to our lab repository and use RStudio to pull the repository to your local computer.\nRead our lab background so you are prepared to start working through the lab repository notebook."
  },
  {
    "objectID": "schedule/07_places.html#reflect",
    "href": "schedule/07_places.html#reflect",
    "title": "Describing Places",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/07_places.html#slides",
    "href": "schedule/07_places.html#slides",
    "title": "Describing Places",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/07_places.html#resources-for-further-exploration",
    "href": "schedule/07_places.html#resources-for-further-exploration",
    "title": "Describing Places",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/08_census.html",
    "href": "schedule/08_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will talk about how planners measure basic dimensions of population and change in population at the neighborhood level. We’ll also discuss some of the basic principles and features of the main Census products which planners use to describe places and the people who live and work in them. We’ll introduce this week’s lab, which focuses on working with basic data on population characteristics."
  },
  {
    "objectID": "schedule/08_census.html#before-class",
    "href": "schedule/08_census.html#before-class",
    "title": "Population and the Census",
    "section": "Before Class",
    "text": "Before Class\nKlosterman 2 ."
  },
  {
    "objectID": "schedule/08_census.html#reflect",
    "href": "schedule/08_census.html#reflect",
    "title": "Population and the Census",
    "section": "Reflect",
    "text": "Reflect\n\nWhat types of characteristics might be well-represented within Census data? What characteristics are harder to measure or represent?\nAs we’ve discussed in class, the census is a dynamic and evolving survey, and the questions we ask are a valuable window into the social questions and issues at a given time. What questions do you think we should be asking in this current moment? How well are they reflected in the census (as you know it)?"
  },
  {
    "objectID": "schedule/08_census.html#slides",
    "href": "schedule/08_census.html#slides",
    "title": "Population and the Census",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/08_census.html#resources-for-further-exploration",
    "href": "schedule/08_census.html#resources-for-further-exploration",
    "title": "Population and the Census",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\n2020 Census Enumeration Form \n2020 Census Post Enumeration Survey Documentation \nDifferential Privacy and the 2020 US Census \nAmerican Community Survey Sample Size"
  },
  {
    "objectID": "schedule/10_projections.html#before-class",
    "href": "schedule/10_projections.html#before-class",
    "title": "Population Projections",
    "section": "Before Class",
    "text": "Before Class\nKlosterman 3 \nKlosterman 4 \nKlosterman 5"
  },
  {
    "objectID": "schedule/10_projections.html#reflect",
    "href": "schedule/10_projections.html#reflect",
    "title": "Population Projections",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/10_projections.html#slides",
    "href": "schedule/10_projections.html#slides",
    "title": "Population Projections",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/10_projections.html#resources-for-further-exploration",
    "href": "schedule/10_projections.html#resources-for-further-exploration",
    "title": "Population Projections",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/11_projections.html#before-class",
    "href": "schedule/11_projections.html#before-class",
    "title": "Population Projections",
    "section": "Before Class",
    "text": "Before Class\nPendall, Rolf. (2018). The Cost of Segregation: Population and Household Projections in the Chicago Commuting Zone and Implications for Economic and Racial Segregation. Urban Institute."
  },
  {
    "objectID": "schedule/11_projections.html#reflect",
    "href": "schedule/11_projections.html#reflect",
    "title": "Population Projections",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/11_projections.html#slides",
    "href": "schedule/11_projections.html#slides",
    "title": "Population Projections",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/11_projections.html#resources-for-further-exploration",
    "href": "schedule/11_projections.html#resources-for-further-exploration",
    "title": "Population Projections",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/12_segregation.html",
    "href": "schedule/12_segregation.html",
    "title": "Segregation",
    "section": "",
    "text": "This week, we will begin a conversation about the nature of residential segregation, and the common ways in which it is measured."
  },
  {
    "objectID": "schedule/12_segregation.html#before-class",
    "href": "schedule/12_segregation.html#before-class",
    "title": "Segregation",
    "section": "Before Class",
    "text": "Before Class\nCunningham, Mary K., and Augrey Droesch. Neighborhood Quality and Racial Segregation. The Urban Institute. \nU.S. Bureau of the Census: Measures of Residential Segregation"
  },
  {
    "objectID": "schedule/12_segregation.html#reflect",
    "href": "schedule/12_segregation.html#reflect",
    "title": "Segregation",
    "section": "Reflect",
    "text": "Reflect\n\nWhy, in your opinion, does segregation remain an enduring characteristic for most American cities, despite efforts to address it?\nHow can visualization of segregation (and its consequences) make a difference?\nWhat can segregation measures capture well? What aspects of segregation are more challenging to measure?"
  },
  {
    "objectID": "schedule/12_segregation.html#slides",
    "href": "schedule/12_segregation.html#slides",
    "title": "Segregation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/12_segregation.html#resources-for-further-exploration",
    "href": "schedule/12_segregation.html#resources-for-further-exploration",
    "title": "Segregation",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/13_segregation.html",
    "href": "schedule/13_segregation.html",
    "title": "Segregation",
    "section": "",
    "text": "In this session, we will begin to work on a lab focused on measures of residential segregation."
  },
  {
    "objectID": "schedule/13_segregation.html#before-class",
    "href": "schedule/13_segregation.html#before-class",
    "title": "Segregation",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/13_segregation.html#reflect",
    "href": "schedule/13_segregation.html#reflect",
    "title": "Segregation",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/13_segregation.html#slides",
    "href": "schedule/13_segregation.html#slides",
    "title": "Segregation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/13_segregation.html#resources-for-further-exploration",
    "href": "schedule/13_segregation.html#resources-for-further-exploration",
    "title": "Segregation",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/14_neighborhood.html",
    "href": "schedule/14_neighborhood.html",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this session, we’ll begin exploring the measurement of neighborhood change as well as underlying theories of transition."
  },
  {
    "objectID": "schedule/14_neighborhood.html#before-class",
    "href": "schedule/14_neighborhood.html#before-class",
    "title": "Neighborhood Change",
    "section": "Before Class",
    "text": "Before Class\nWBEZ: There Goes the Neighborhood\nWho Can Live in Chicago?: A Tale of Three Cities\nThe Socioeconomic Change of Chicago’s Community Areas (1970-2010)"
  },
  {
    "objectID": "schedule/14_neighborhood.html#reflect",
    "href": "schedule/14_neighborhood.html#reflect",
    "title": "Neighborhood Change",
    "section": "Reflect",
    "text": "Reflect\n\nWhat role should planners take in managing neighborhood change?\nHow do planners balance changes across multiple neighborhoods?"
  },
  {
    "objectID": "schedule/14_neighborhood.html#slides",
    "href": "schedule/14_neighborhood.html#slides",
    "title": "Neighborhood Change",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/14_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/14_neighborhood.html#resources-for-further-exploration",
    "title": "Neighborhood Change",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nThe Three Cities Within Toronto: Income Polarization Amongst Toronto’s Neighbourhoods, 1970-2005"
  },
  {
    "objectID": "schedule/15_neighborhood.html#before-class",
    "href": "schedule/15_neighborhood.html#before-class",
    "title": "Neighborhood Change",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/15_neighborhood.html#reflect",
    "href": "schedule/15_neighborhood.html#reflect",
    "title": "Neighborhood Change",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/15_neighborhood.html#slides",
    "href": "schedule/15_neighborhood.html#slides",
    "title": "Neighborhood Change",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/15_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/15_neighborhood.html#resources-for-further-exploration",
    "title": "Neighborhood Change",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/16_opportunity.html",
    "href": "schedule/16_opportunity.html",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this session, we’ll begin to examine opportunity analysis and mapping, which is a strategy used to compare multiple measures of sociodemographic wellbeing to create a single measure of how supportive a place is for economic and social mobility. Opportunity mapping in various forms has been used to craft legal remedies to discrimination and segregation, to design policy interventions, and as a source of information for planning and the allocation of resources.\nYour readings for today provide a) a contemporary overview of opportunity mapping, and b) a conceptual overview of the application of opportunity mapping by one of it’s originators, john powell.\nIn addition to our discussion of opportunity mapping, we’ll also take a few moments to check in on your final assignment progress, your detailed project description, and more generally about how things are going with the class."
  },
  {
    "objectID": "schedule/16_opportunity.html#before-class",
    "href": "schedule/16_opportunity.html#before-class",
    "title": "Place Opportunity",
    "section": "Before Class",
    "text": "Before Class\nStromberg, Brian. (2016). Opportunity Mapping. National Housing Conference. \npowell, john. (2005). Remedial Phase Expert Report of john powell in Thompson v. HUD. \nBalachandran, Sowmya, and Andrew Greenlee. (2022). Examining Spatial Opportunity for Local Action: From Theory to Practice. Journal of Planning Education and Research."
  },
  {
    "objectID": "schedule/16_opportunity.html#reflect",
    "href": "schedule/16_opportunity.html#reflect",
    "title": "Place Opportunity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat does place opportunity mean to you? What makes a place more (or less) opportune?\nAre there some universal dimensions of place opportunity? Some that are more specific to certain population groups?\nWhat types of practices support opportunity mapping? How can we use the outputs from opportunity mapping exercises for deliberation and policy decision-making?"
  },
  {
    "objectID": "schedule/16_opportunity.html#slides",
    "href": "schedule/16_opportunity.html#slides",
    "title": "Place Opportunity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/16_opportunity.html#resources-for-further-exploration",
    "href": "schedule/16_opportunity.html#resources-for-further-exploration",
    "title": "Place Opportunity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/17_opportunity.html#before-class",
    "href": "schedule/17_opportunity.html#before-class",
    "title": "Place Opportunity",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/17_opportunity.html#reflect",
    "href": "schedule/17_opportunity.html#reflect",
    "title": "Place Opportunity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/17_opportunity.html#slides",
    "href": "schedule/17_opportunity.html#slides",
    "title": "Place Opportunity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/17_opportunity.html#resources-for-further-exploration",
    "href": "schedule/17_opportunity.html#resources-for-further-exploration",
    "title": "Place Opportunity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/22_fieldobs.html",
    "href": "schedule/22_fieldobs.html",
    "title": "Field Observations",
    "section": "",
    "text": "So far, we have started to learn how to tell stories about places using existing indicators. With so many available sources of existing information, it can be easy to experience a disconnect between the values of those indicators and the complexity of what they represent. To think more about what grounds our analysis as planners, we will go explore a neighborhood in person and then think through elements of the stories that may help us describe that place through indicators.\nWe are going to spend this class session doing field observation of the West Urbana neighborhood which we read about during the first week of class. This neighborhood is designated as an American Planning Association Great Place in America. We will meet at Carle Park in Urbana at the intersection of Carle and Indiana streets. We will not meet in CIF for this session.\nCome prepared to spend the course session exploring the neighborhood. We will use the insights you gather during this course session to inform Thursday’s lab assignment."
  },
  {
    "objectID": "schedule/22_fieldobs.html#before-class",
    "href": "schedule/22_fieldobs.html#before-class",
    "title": "Field Observations",
    "section": "Before Class",
    "text": "Before Class\n\nWrite a short reflection on your pre-existing impressions of the West Urbana neighborhood. Some of you may be very familiar, and others may not at all. Reflect based upon what you know or have heard.\nWear comfortable clothes, and be prepared to be outside for the duration of our course session.\nYou may wish to bring a notebook with you as well as a phone or camera to document your observations."
  },
  {
    "objectID": "schedule/22_fieldobs.html#during-class",
    "href": "schedule/22_fieldobs.html#during-class",
    "title": "Field Observations",
    "section": "During Class",
    "text": "During Class\nWe will split up into small groups in order to observe elements of the West Urbana Neighborhood (group assignments are below). Working in groups of four or five, you will each be responsible for focusing on one element of observation (you may double up in an area of your choice if you have five in your group):\n\nInfrastructure and Environment\nEconomy and Housing\nHealth and Wellbeing\nSense of Place\n\nObservation 1: In your group, spend 20 minutes walking around the neighborhood, taking in and observing your specific element. Then take 5-10 minutes to write down your overall impressions and any questions you have after this brief exploration.\nObservation 2: As a group, pick a block to systematically analyze. Spend 20 minutes observing your specific element as it is reflected on your block. You may want to take pictures, sketch a map, or use other methods to record what you observe.\nObservation 3: Spend the remaining 30 minutes comparing notes with your other group members. Record a list of shared questions you have about the block and portions of the neighborhood that you explored. Reflect as a group upon what you were able to observe.\nAt the end of our class time you can leave. Please bring your individual and shared reflections and observational materials to Thursday’s lab session."
  },
  {
    "objectID": "schedule/22_fieldobs.html#reflect",
    "href": "schedule/22_fieldobs.html#reflect",
    "title": "Field Observations",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/22_fieldobs.html#slides",
    "href": "schedule/22_fieldobs.html#slides",
    "title": "Field Observations",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/22_fieldobs.html#resources-for-further-exploration",
    "href": "schedule/22_fieldobs.html#resources-for-further-exploration",
    "title": "Field Observations",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nUrban Institute: Observation\nParticipant Observation and the Development of Urban Neighborhood Policy"
  },
  {
    "objectID": "schedule/23_fieldobs.html",
    "href": "schedule/23_fieldobs.html",
    "title": "Field Observations",
    "section": "",
    "text": "On Tuesday, you spent time walking and observing portions of the West Urbana neighborhood. Now that you have a shared frame of reference, you will work collectively with your group members to think about how to tell a story of the neighborhood. The challenge of this exercise is to think about how to systematically describe a neighborhood after your observations of a part of it, and to think about how to leverage and organize information to focus your description."
  },
  {
    "objectID": "schedule/23_fieldobs.html#session-goals",
    "href": "schedule/23_fieldobs.html#session-goals",
    "title": "Field Observations",
    "section": "Session Goals",
    "text": "Session Goals\n\nCollaboratively build a story about the West Urbana neighborhood that leverages both your direct observation and existing indicators.\nReflect upon the tactical and design choices you have made in telling your story."
  },
  {
    "objectID": "schedule/23_fieldobs.html#before-class",
    "href": "schedule/23_fieldobs.html#before-class",
    "title": "Field Observations",
    "section": "Before Class",
    "text": "Before Class\n\nReview your notes and observational materials from your neighborhood field visit."
  },
  {
    "objectID": "schedule/23_fieldobs.html#during-class",
    "href": "schedule/23_fieldobs.html#during-class",
    "title": "Field Observations",
    "section": "During Class",
    "text": "During Class\nWork collaboratively with your group members to develop a short memorandum describing the West Urbana neighborhood. Your memo should draw off of your field observations, but should also include demographic summaries coming from census data or other sources of secondary data (please start with the same selected census indicators we have worked with for the past few weeks).\nYour report should describe the following:\n\nNeighborhood character, identity, and assets - integrating both primary and secondary data, describe the character, identity, and assets of the neighborhood, addressing (at a minimum) your areas of focus.\n\nInfrastructure and Environment\nEconomy and Housing\nHealth and Wellbeing\nSense of Place\n\nInformation Gaps - based upon your description above, what information gaps exist? What types of information would you like to have to better convey what you observed on the ground in the West Urbana neighborhood?\nProposed Strategy for Systematic Examination - based upon your assessment of information gaps, how do you propose collecting that information, and how would you integrate it into your report?\nReflection - as a group, reflect upon some of the challenges you faced in systematically observing elements of the neighborhood, linking those observations with existing data or information, and conveying that story in a memorandum."
  },
  {
    "objectID": "schedule/23_fieldobs.html#after-class",
    "href": "schedule/23_fieldobs.html#after-class",
    "title": "Field Observations",
    "section": "After Class",
    "text": "After Class\n\nPlease submit one memorandum as an R notebook and associated files.\nPlease add to your initial reflection on what you knew about the neighborhood before you visited your thoughts on what you know now and what you experienced in attempting to systematically observe the neighborhood and communicate its qualities. Combine these materials into a single notebook file and push these to the appropriate folder in your group’s GitHub repository."
  },
  {
    "objectID": "schedule/23_fieldobs.html#slides",
    "href": "schedule/23_fieldobs.html#slides",
    "title": "Field Observations",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/23_fieldobs.html#resources-for-further-exploration",
    "href": "schedule/23_fieldobs.html#resources-for-further-exploration",
    "title": "Field Observations",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/24_peerrev.html",
    "href": "schedule/24_peerrev.html",
    "title": "Final Project Peer Review",
    "section": "",
    "text": "As we approach the end of the semester, the remainder of our time together will focus on strengthening your final projects. By this point, you should have most of the constituent pieces for this final project as reflected within the memos you’ve produced over the course of the semester. Your task is to now translate these pieces into a coherent and polished final report.\nOne valuable form of support we’ve used over the course of the semester is peer review. Today’s session and our next session will focus on working in small groups to introduce your final projects, set work goals for this week, and then use peer review as a way to further develop and improve some of those components of your final report.\nIn this session, we’ll set the context for peer review of each others’ work. You will split into small groups and spend time introducing your final projects to each other. At the end of this course session, you will share with your peer review team an analytic component which you’d like their feedback on. During our Thursday session, they will provide direct written feedback, and you will have time to incorporate some of this feedback into your work and make adjustments and refinements during class time."
  },
  {
    "objectID": "schedule/24_peerrev.html#before-class",
    "href": "schedule/24_peerrev.html#before-class",
    "title": "Final Project Peer Review",
    "section": "Before Class",
    "text": "Before Class\n\nThink about the main message you wish to convey in your policy report - can you encapsulate that message in a declarative sentence or two?\nThink about one critical component of your final project - this could be a key figure, key analytic concept, key measure, or key outcome. Come prepared to share what this is with your peer review partners.\nThink about key visual or analytic elements you can share with your peer reviewers to offer them context on your project. Come prepared to share this in class."
  },
  {
    "objectID": "schedule/24_peerrev.html#during-class",
    "href": "schedule/24_peerrev.html#during-class",
    "title": "Final Project Peer Review",
    "section": "During Class",
    "text": "During Class\nYou should develop written feedback to share on your peer review team members’ work, and should be prepared to receive feedback on your own work. Please also come prepared to start incorporating that feedback into your work products.\nYour feedback for each of your teammates should build upon the following prompts:\n\nBased upon what you heard in your sharing time, what’s the main message or point of their final project?\nWho do you see as the target audience for their report?\nWhat insights will this work provide?\n\nThinking about the work component they shared with you…\n\nWhat works well about this component?\nWhat do you think could be improved about this component?\nWhat questions do you have after reviewing this component?"
  },
  {
    "objectID": "schedule/24_peerrev.html#reflect",
    "href": "schedule/24_peerrev.html#reflect",
    "title": "Final Project Peer Review",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/24_peerrev.html#slides",
    "href": "schedule/24_peerrev.html#slides",
    "title": "Final Project Peer Review",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/24_peerrev.html#resources-for-further-exploration",
    "href": "schedule/24_peerrev.html#resources-for-further-exploration",
    "title": "Final Project Peer Review",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/25_finalpresent.html#before-class",
    "href": "schedule/25_finalpresent.html#before-class",
    "title": "NAME",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/25_finalpresent.html#reflect",
    "href": "schedule/25_finalpresent.html#reflect",
    "title": "NAME",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/25_finalpresent.html#slides",
    "href": "schedule/25_finalpresent.html#slides",
    "title": "NAME",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/25_finalpresent.html#resources-for-further-exploration",
    "href": "schedule/25_finalpresent.html#resources-for-further-exploration",
    "title": "NAME",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/26_indwork.html",
    "href": "schedule/26_indwork.html",
    "title": "Independent Work and Advising",
    "section": "",
    "text": "This week is devoted to individual advising and independent work. There are no course sessions scheduled for today or Thursday. You should use this time to continue working, to take stock of your progress towards your course contract, and to solidify plans for your final project."
  },
  {
    "objectID": "schedule/26_indwork.html#before-class",
    "href": "schedule/26_indwork.html#before-class",
    "title": "Independent Work and Advising",
    "section": "Before Class",
    "text": "Before Class\nProfessor Greenlee has availability for office hours appointments. If you have not already scheduled an appointment and wish to, please sign up."
  },
  {
    "objectID": "schedule/26_indwork.html#reflect",
    "href": "schedule/26_indwork.html#reflect",
    "title": "Independent Work and Advising",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/26_indwork.html#slides",
    "href": "schedule/26_indwork.html#slides",
    "title": "Independent Work and Advising",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/26_indwork.html#resources-for-further-exploration",
    "href": "schedule/26_indwork.html#resources-for-further-exploration",
    "title": "Independent Work and Advising",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/27_indwork.html",
    "href": "schedule/27_indwork.html",
    "title": "Independent Work and Advising",
    "section": "",
    "text": "This week is devoted to individual advising and independent work. There are no course sessions scheduled for today. You should use this time to continue working, to take stock of your progress towards your course contract, and to solidify plans for your final project."
  },
  {
    "objectID": "schedule/27_indwork.html#before-class",
    "href": "schedule/27_indwork.html#before-class",
    "title": "Independent Work and Advising",
    "section": "Before Class",
    "text": "Before Class\nProfessor Greenlee has availability for office hours appointments. If you have not already scheduled an appointment and wish to, please sign up."
  },
  {
    "objectID": "schedule/27_indwork.html#reflect",
    "href": "schedule/27_indwork.html#reflect",
    "title": "Independent Work and Advising",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/27_indwork.html#slides",
    "href": "schedule/27_indwork.html#slides",
    "title": "Independent Work and Advising",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/27_indwork.html#resources-for-further-exploration",
    "href": "schedule/27_indwork.html#resources-for-further-exploration",
    "title": "Independent Work and Advising",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/28_finalpresent.html#before-class",
    "href": "schedule/28_finalpresent.html#before-class",
    "title": "Final Presentations",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/28_finalpresent.html#reflect",
    "href": "schedule/28_finalpresent.html#reflect",
    "title": "Final Presentations",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/28_finalpresent.html#slides",
    "href": "schedule/28_finalpresent.html#slides",
    "title": "Final Presentations",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/28_finalpresent.html#resources-for-further-exploration",
    "href": "schedule/28_finalpresent.html#resources-for-further-exploration",
    "title": "Final Presentations",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/29_finalpresent.html",
    "href": "schedule/29_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "You made it! This is the final session for Neighborhood Analysis. We will discuss what we have covered over the course of the semester, as well as some next steps in your journey as neighborhood analysts.\nSome questions to seed our discussion:\n\nAre there things that you wish we covered over the course of the semester that we didn't?\nAre there ways that Professor Greenlee could have better supported learning (individually and collectively) over the course of the semester?\nWas there sufficient balance between technical and non-technical content?\nWere labs and other supporting materials clear? Were there resources or content you wish you had to support your independent learning using this format?"
  },
  {
    "objectID": "schedule/29_finalpresent.html#before-class",
    "href": "schedule/29_finalpresent.html#before-class",
    "title": "Final Presentations",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/29_finalpresent.html#reflect",
    "href": "schedule/29_finalpresent.html#reflect",
    "title": "Final Presentations",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/29_finalpresent.html#slides",
    "href": "schedule/29_finalpresent.html#slides",
    "title": "Final Presentations",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/29_finalpresent.html#resources-for-further-exploration",
    "href": "schedule/29_finalpresent.html#resources-for-further-exploration",
    "title": "Final Presentations",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/06_places.html",
    "href": "schedule/06_places.html",
    "title": "Describing Places",
    "section": "",
    "text": "In this session, we’ll spend time talking about how to describe places, as well as some frameworks for making place comparisons. We will think about how the types of stories we tell about places connect with typical arguments and tropes which guide place description and analysis within urban planning and policy settings.\nIn addition to our discussion of places, we’ll debrief your two labs from last week and introduce this week’s lab."
  },
  {
    "objectID": "schedule/10_projections.html",
    "href": "schedule/10_projections.html",
    "title": "Population Projections",
    "section": "",
    "text": "Much of our analysis is oriented towards the past and present, but what about the future? This week, we will learn the basic techniques involved in population projections. We’ll explore several common techniques for projections and discuss some of the reasons why projecting population at the neighborhood level can be challenging."
  },
  {
    "objectID": "schedule/18_transequity.html#before-class",
    "href": "schedule/18_transequity.html#before-class",
    "title": "Transit Equity",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/18_transequity.html#reflect",
    "href": "schedule/18_transequity.html#reflect",
    "title": "Transit Equity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/18_transequity.html#slides",
    "href": "schedule/18_transequity.html#slides",
    "title": "Transit Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/18_transequity.html#resources-for-further-exploration",
    "href": "schedule/18_transequity.html#resources-for-further-exploration",
    "title": "Transit Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/19_transequity.html#before-class",
    "href": "schedule/19_transequity.html#before-class",
    "title": "Transit Equity",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/19_transequity.html#reflect",
    "href": "schedule/19_transequity.html#reflect",
    "title": "Transit Equity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/19_transequity.html#slides",
    "href": "schedule/19_transequity.html#slides",
    "title": "Transit Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/19_transequity.html#resources-for-further-exploration",
    "href": "schedule/19_transequity.html#resources-for-further-exploration",
    "title": "Transit Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/20_healthequity.html#before-class",
    "href": "schedule/20_healthequity.html#before-class",
    "title": "Health Equity",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/20_healthequity.html#reflect",
    "href": "schedule/20_healthequity.html#reflect",
    "title": "Health Equity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/20_healthequity.html#slides",
    "href": "schedule/20_healthequity.html#slides",
    "title": "Health Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/20_healthequity.html#resources-for-further-exploration",
    "href": "schedule/20_healthequity.html#resources-for-further-exploration",
    "title": "Health Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/21_healthequity.html#before-class",
    "href": "schedule/21_healthequity.html#before-class",
    "title": "Health Equity",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/21_healthequity.html#reflect",
    "href": "schedule/21_healthequity.html#reflect",
    "title": "Health Equity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/21_healthequity.html#slides",
    "href": "schedule/21_healthequity.html#slides",
    "title": "Health Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/21_healthequity.html#resources-for-further-exploration",
    "href": "schedule/21_healthequity.html#resources-for-further-exploration",
    "title": "Health Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "assignments/index.html#assessment",
    "href": "assignments/index.html#assessment",
    "title": "Assignments Overview",
    "section": "Assessment",
    "text": "Assessment\nMost of the methods classes you have probably taken have focused on grading you on your skill at reproducing specific outcomes. Our pedagogy in this class is that while it is possible to measure these outcomes, what is more important to assess is the process of learning as well as the effort you put into that process. Here’s some more detail on our thinking:\n\nA unitary assessment of performance in a 500-level class is not likely to serve students well. Each of you has proven time and time again in other classes that you can reproduce certain expected outcomes. The challenge in this class will focus on assessing your effort and learning process as opposed to the reproduction of outcomes. \nImplementing a contract-based assessment approach mirrors course goals regarding our understanding and practice around the power of data, particularly with regards to public deliberation, decision-making, and governance. Dealing differently with power around evaluation in our classroom mirrors how we might deal differently with power and power relationships in professional practice.\nThere is increasing evidence in higher education settings that unitary grading measures tend to hamper individual learning, and have the potential to be biased towards certain types of students. Put simply, obsessing about grades in advanced coursework is counter-productive to deep learning. This course adheres to the pedagogy that when given more control and ownership over the terms of evaluation, a student is more likely to embrace the challenges presented to them within the classroom, and are more likely to take ownership of their work (see, for instance Elbow and Inoue).\n\n\nContract Expectation Summary\nEach of you begins the semester with an “A” in the class. You may alter your grade based upon the choices you make regarding the following accountability expectations: \n\n\n\nGrade\nAbsences\nLate\nMissed\nIgnored\n\n\n\n\nA\n3\n1\n0\n0\n\n\nB\n3\n2\n1\n0\n\n\nC\n4\n2\n1\n1\n\n\nD\n5\n3\n2\n1\n\n\nF\n6\n4\n3\n1\n\n\n\n\nAbsences indicate the number of times you are absent from class. Additional excused absences may be allowed due to documented extenuating circumstances.\nLate Assignments are assignments submitted after a due date but within 48 hours of the due date.\nMissed Assignments are assignments submitted more than 48 hours after the due date.\nIgnored Assignments are assignments that are not submitted by the last day of class (excluding your final assignment). Ignored assignments are a more serious breach of contract than missed assignments as they reflect contracted work which you have not completed.\n\nPlease note that assessment of these contracted standards are based upon what you do. The contracted grade does assess of the quality of your work. You will reflect upon and will receive direct feedback on the quality of your work as part of the assignment review process. Note that we reserve the right to adjust grades based upon exceptionally strong or weak engagement within the class. We will provide sufficient feedback over the course of the semester so that your final grade in the class should not be a surprise based upon what you do and how you reflect upon your learning over the course of the semester."
  },
  {
    "objectID": "assignments/06_phd.html",
    "href": "assignments/06_phd.html",
    "title": "Ph.D. Term Assignment",
    "section": "",
    "text": "Ph.D. students in UP 570 have the option of either completing the series of assignments which Master’s degree students complete, or may opt to complete a term assignment that is focused on conducting research in an area of your choice that incorporates some of the principles, tools, and strategies from the class. The goal is to produce a complete draft of a scholarly article suitable for publication in a peer reviewed journal by the end of the semester."
  },
  {
    "objectID": "assignments/06_phd.html#outputs",
    "href": "assignments/06_phd.html#outputs",
    "title": "Ph.D. Term Assignment",
    "section": "Outputs",
    "text": "Outputs\n\nPaper Proposal\nPrepare a 3-4 page paper proposal that describes the research question, rationale, and approach which you propose taking over the course of the semester. Your proposal should highlight and review 5-10 relevant key sources (at a minimum), should articulate a clear rationale for the methods you intend to use, should describe any specific skills or methods which you propose developing or applying over the course of the semester, and should describe the overall analytic outputs and prospective audience for your work. Your proposal should also describe several candidate publication venues (journals) for which you might submit your article to.\nDue Date: February 3\nPaper Proposal Repository \n\n\nPaper Outline\nPrepare an annotated paper outline that fleshes out the structure of your paper including a complete written introduction, summary of the argument(s) you intend to make in your literature review, research question(s), a written summary of your methods, and a proposed structure for your results, discussion, and conclusion. The detail you invest in this outline will help us to provide feedback to help you as you develop your paper draft. In your outline, indicate the prospective journal you plan to submit your final paper to and ensure that your outline conforms to the structure of articles submitted to that journal.\nDue Date: March 3\nPaper Outline Repository \n\n\nPaper Draft\nPrepare a complete first draft of your paper that includes an introduction, literature review, research questions, methods section, results, discussion, and conclusion (again, conforming to the accepted structure of articles in the journal you are submitting to).\nDue Date: April 14\nPaper Draft Repository \n\n\nFinal Paper\nPrepare a polished draft of your paper incorporating feedback from your previous draft. Separate from your main document, include 1-2 page summary that is suitable for a non-technical non-academic audience that summarizes key takeaways.\nDue Date: May 10\nFinal Paper Repository"
  },
  {
    "objectID": "assignments/06_phd.html#submission-instructions",
    "href": "assignments/06_phd.html#submission-instructions",
    "title": "Ph.D. Term Assignment",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nWhen you are ready to begin work on each assignment component, please accept the following GitHub Classroom assignments (one for each component). Upload your document, reflection and any associated files. Due to GitHub’s file size limitations, you may need to store large files (datasets, etc.) on Box and reference them separately.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "schedule/04_sharing.html",
    "href": "schedule/04_sharing.html",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This session builds upon the work on your last lab. In that lab, you worked on developing several workflows that will support your work over the course of the semester. Coming into this lab, you should have a formatted Quarto markdown document. In this session, we’ll talk about strategies for sharing that work, will configure your computer to communicate with GitHub, and will create your first publicly facing websites."
  },
  {
    "objectID": "schedule/05_permit.html",
    "href": "schedule/05_permit.html",
    "title": "Earning your Learner’s Permit",
    "section": "",
    "text": "In this lab session, you’ll do some initial work on “real world” data to earn your UP 570 Learner’s Permit. This session marks out transition from setting up some basic workflows to focusing on data analysis techniques and applications. This lab is designed to help us learn more about your familiarity and proficiency with basic data manipulation using common dplyr functions.\nYour goal is to use our lab session plus an additional 3-4 hours to work through lab prompts. Address the prompts if you can. If you can’t, provide written descriptions about what you’re trying to do, pointers about how you’ve tried to address the problems, and insights into where you’re getting stuck. To repeat, the overall goal isn’t to complete the lab, but rather to share your process and insights. This will help us to refine future labs and instructions based upon the collective knowledge and understanding of the class."
  },
  {
    "objectID": "assignments/labs/03_permit.html",
    "href": "assignments/labs/03_permit.html",
    "title": "Earning Your Learner’s Permit",
    "section": "",
    "text": "UP 570 assumes that you have a basic familiarity with core principles of data manipulation in R. As we move forward with our class, we will continue to add knowledge of new packages, tools, and data within R.\nThis R Learner’s Permit is designed to assess your knowledge of the core elements of the R language and software. The goal is to provide you and the instructional team with a better sense of your core knowledge of basic data manipulation that will form the basis for more advanced techniques we’ll learn over the course of the semester.\nDo not worry if some of the tasks remain challenging or if you are unable to complete them- a goal of the class is to continue adding complexity and opportunities to repeat tasks to reinforce your familiarity and comfort with their use."
  },
  {
    "objectID": "assignments/labs/03_permit.html#goals",
    "href": "assignments/labs/03_permit.html#goals",
    "title": "Earning Your Learner’s Permit",
    "section": "Goals",
    "text": "Goals\n\nGet your hands dirty with what is likely an unfamiliar source of “real world” data.\nLearn more about your familiarity with basic dplyr data manipulation strategies.\nLearn more about your familiarity with basic data visualization using ggplot2."
  },
  {
    "objectID": "assignments/labs/03_permit.html#core-concepts",
    "href": "assignments/labs/03_permit.html#core-concepts",
    "title": "Earning Your Learner’s Permit",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\ndim()\nsummary()\ngroup_by()\nsummarise()\nleft_join\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/03_permit.html#github-lab-repository",
    "href": "assignments/labs/03_permit.html#github-lab-repository",
    "title": "Earning Your Learner’s Permit",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/03_permit.html#instructions",
    "href": "assignments/labs/03_permit.html#instructions",
    "title": "Earning Your Learner’s Permit",
    "section": "Instructions",
    "text": "Instructions\nFollow the instructions contained within the GitHub lab repository. Most instructions ask you to add or fill in code chunks. Others ask you to provide a written interpretation in the notebook portion of the document.\nComplete as many items as you can. If you run into trouble completing an item, add comments to your code or in the notebook describing where you are running into problems, and what you think the next step might be to solving the problem you’re having.\nComplete as many items as you can and then push your work to the appropriate repository on Github."
  },
  {
    "objectID": "assignments/labs/03_permit.html#lab-evaluation",
    "href": "assignments/labs/03_permit.html#lab-evaluation",
    "title": "Earning Your Learner’s Permit",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nYour code and the way in which you’re approaching problem solving.\nYour written analysis of how you are approaching problem solving in the lab.\nYour written analysis and interpretation of the lab materials.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class. It is okay to touch base with others as you work through the lab, however, please indicate where you are running into challenges with problem solving so we can factor this into our instruction."
  },
  {
    "objectID": "assignments/labs/03_permit.html#references",
    "href": "assignments/labs/03_permit.html#references",
    "title": "Earning Your Learner’s Permit",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/02_sharing.html#setting-up-our-project",
    "href": "assignments/labs/02_sharing.html#setting-up-our-project",
    "title": "Sharing Your Work",
    "section": "Setting Up our Project",
    "text": "Setting Up our Project\nWe have to make two tweaks to our project in order to get them to work properly with Netlify.\n\nAdd a _quarto.yml file\nAll Quarto websites require a special YAML file to tell them how to render. In a text editor of your choice, create a new file called _quarto.yml and save it in the project root directory.\nInside the _quarto.yml file, include the following:\nproject:\n  type: website\nNow when you render your project, it should include a folder called “_site” which contains a self-contained version of the elements of your document. You can explore the many other options to style your site here.\n\n\nRename our document index.qmd\nEvery website needs a homepage. By default, the name of this page is “index.html”. Netlify will always start serving from this page. Rename your Quarto document “index.qmd” and then render this to produce the associated html files.\n\n\nPush to Github\nPush your entire repository including the _site folder to Github.\n\n\nOpen Netlify\nNavigate to Netlify’s website. If you haven’t already done so, create a new account (you can log in and link this directly to your existing GitHub account).\nOnce you have an account, navigate to Sites and click “New Site”.\nNetlify will ask you if you want to create a site from an existing GitHub repository, which you do! Navigate to the repository that contains the lab files and _site folder you want to serve and select this repository.\n\n\n\n\n\n\nImportant\n\n\n\nThere’s one setting we need to modify in order to get this to work properly! Under Build settings, there’s a blank area called Publish directory. In this box, you need to point Netlify to _site as your specified publish directory - this is where Netlify will look for updated files to publish.\n\n\nAfter you do this, click ok. Netlify will build your site (this may take a few minutes) and share a url with you. When you click on it, you should see a live version of your website.\n\n\nNotes\nThis approach is a little more complex, but it does offer you a lot of flexibility to create sites for production. You will likely want to start off using Quarto pub, but as you progress in the complexity of your analysis, you may want to spend more time experienting with Netlify as a web deployment platform."
  },
  {
    "objectID": "schedule/07_places.html",
    "href": "schedule/07_places.html",
    "title": "Describing Places",
    "section": "",
    "text": "Today’s lab session focuses on the basic description of places. In your prior labs, you have developed basic workflows for communicating using principles of reproducible data analysis. Today’s lab asks you to apply those skills to basic description of place characteristics.\nTo add depth to our example, we will explore not only basic place descriptions, but will also think about how these descriptions might change in relation to an applied policy problem."
  },
  {
    "objectID": "assignments/labs/04_describing.html",
    "href": "assignments/labs/04_describing.html",
    "title": "Describing Places",
    "section": "",
    "text": "In this lab, we’ll learn some techniques for creating publication-quality summary tables while working to tell policy-relevant stories about places.\nIn addition to thinking about the basics of how we describe places, we will perform a basic policy analysis of the location of federal Opportunity Zones. This analysis will help illustrate how we can strategically build layers of stories. We’ll add some basic information about all census tracts so that we can describe the differences between ineligible, eligible but not designated, and eligible and designated census tracts."
  },
  {
    "objectID": "assignments/labs/04_describing.html#goals",
    "href": "assignments/labs/04_describing.html#goals",
    "title": "Describing Places",
    "section": "Goals",
    "text": "Goals\n\nSet up your computer so that RStudio can communicate with your Github account."
  },
  {
    "objectID": "assignments/labs/04_describing.html#core-concepts",
    "href": "assignments/labs/04_describing.html#core-concepts",
    "title": "Describing Places",
    "section": "Core Concepts",
    "text": "Core Concepts\nThis lab asks you to practice some basic data manipulation and management skills using the dplyr package.\n\nIntroduce several commonly used demographic indicators from the census\nIntroduce how to join datasets together based upon a common field\nIntroduce how to recode and classify data based upon one or more characteristics\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/04_describing.html#github-lab-repository",
    "href": "assignments/labs/04_describing.html#github-lab-repository",
    "title": "Describing Places",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/04_describing.html#principles-of-tidy-data",
    "href": "assignments/labs/04_describing.html#principles-of-tidy-data",
    "title": "Describing Places",
    "section": "Principles of Tidy Data",
    "text": "Principles of Tidy Data\nIn the book R for Data Science, Hadley Wickam describes three principles for tidy data:\n\nEach variable must have its own column\nEach observation must have its own row\nEach value must have its own cell\n\n Much of the data we work with in the context of basic planning and policy analysis applications already conforms to this format (or is easily transformed into this format). This makes packages like tidyverse particularly useful for the common types of data manipulation that we perform.\nWhile we’ll occasionally use base r coding over the course of the semester, for the most part, we’ll rely upon the tidyverse suite to help us. Let’s explore some basic command syntax.\n\nLoad Example Data\nWe’re going to work with a dataset that describes those census tracts that were designated as Opportunity Zones as part of the federal Tax Cuts and Jobs Act. These incentives are designed to spur investment in low-income and undercapitalized cities, by providing investors with tax incentives to invest capital in these locations.\nThe specific dataset which we’ll work with was developed by the Urban Institute, and adds to basic identification of designated census tracts some additional analysis of the characteristics of those places.\n\n\nLoading Required Packages\nWe’re already learned how to use install.packages() and library() to (respectively) install and load packages that extend R and RStudio’s functionality. As a reminder, install.packages() downloads the package from a central server and installs it on your computer. You only have to install a package once. Using library() loads that package for use in your current RStudio session. If you plan to use that package in a given analysis, you’ll need to load it. To stay organized, you should load packages at the beginning of your script or markdown document.\n\n\nNote that to install the package, you need to treat the package name as a character vector \"tidyverse\", but when you load it in your R session, it does not need to be treated as a character vector tidyverse because it is an object that R recognizes after it is installed.\nWe are going to load the following packages:\n\ntidyverse contains tools which we’ll use to subset, filter, group, and summarize our data\nreadxl contains tools which will help us to read Excel files into R\ngt contains tools for making nicely formatted tables.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(gt)\n\n\nThe read_xlsx() command from the readxl package will read Microsoft Excel files into data tables. Let’s start by loading the Urban Institute Opportunity Zone dataset:\nLet’s read the Excel data and place it in an object called “ozs”:\n\n\nCode\nozs <- read_xlsx(\"04_describing/data/urbaninstitute_tractlevelozanalysis_update1242018.xlsx\")\n\n\nYou can either do a Google search for Readxl to find documentation, or you can use R’s built in documentation by typing ?readxl\nAs the documentation states, readxl imports excel files. Looking at the documentation, the read_excel() command will read a single excel sheet, or we can optionally select a sheet by name or number from an excel workbook with multiple sheets. In this case, the Urban Institute data is in a workbook with a single sheet, so we just need to tell R where the file is to load.\n\n\nDescribing Data\nOne of the first steps that we should do when we load an unfamiliar dataset is to get to know it using some basic description commands.\nLet’s use the str() command to analyze the dataset’s structure:\n\n\nCode\nstr(ozs)\n\n\ntibble [42,176 × 27] (S3: tbl_df/tbl/data.frame)\n $ geoid                : chr [1:42176] \"01001020200\" \"01001020300\" \"01001020700\" \"01001020802\" ...\n $ state                : chr [1:42176] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ Designated           : num [1:42176] NA NA 1 NA NA NA NA 1 NA 1 ...\n $ county               : chr [1:42176] \"Autauga County\" \"Autauga County\" \"Autauga County\" \"Autauga County\" ...\n $ Type                 : chr [1:42176] \"Low-Income Community\" \"Non-LIC Contiguous\" \"Low-Income Community\" \"Non-LIC Contiguous\" ...\n $ dec_score            : num [1:42176] 4 6 9 10 5 6 6 9 10 9 ...\n $ SE_Flag              : num [1:42176] NA NA NA NA NA NA NA NA NA NA ...\n $ Population           : num [1:42176] 2196 3136 3047 10743 2899 ...\n $ medhhincome2014_tract: num [1:42176] 41107 51250 45234 61242 49567 ...\n $ PovertyRate          : num [1:42176] 0.24 0.107 0.19 0.153 0.151 ...\n $ unemprate            : num [1:42176] 0.0775 0.051 0.1407 0.0459 0.0289 ...\n $ medvalue             : num [1:42176] 95300 113800 93500 160400 102900 ...\n $ medrent              : num [1:42176] 743 817 695 1018 546 ...\n $ pctown               : num [1:42176] 0.628 0.703 0.711 0.823 0.83 ...\n $ severerentburden     : num [1:42176] 0.3269 0.3223 0.3887 0.1994 0.0994 ...\n $ vacancyrate          : num [1:42176] 0.0584 0.1399 0.0619 0.0609 0.2182 ...\n $ pctwhitealone        : num [1:42176] 0.439 0.671 0.833 0.814 0.726 ...\n $ pctblackalone        : num [1:42176] 0.5187 0.205 0.0922 0.1572 0.2456 ...\n $ pctHispanic          : num [1:42176] 0.01275 0.0727 0.0338 0.01368 0.00448 ...\n $ pctAAPIalone         : num [1:42176] 0.01093 0.01052 0 0.00959 0 ...\n $ pctunder18           : num [1:42176] 0.218 0.224 0.249 0.27 0.245 ...\n $ pctover64            : num [1:42176] 0.124 0.175 0.149 0.122 0.156 ...\n $ HSorlower            : num [1:42176] 0.581 0.464 0.544 0.45 0.621 ...\n $ BAorhigher           : num [1:42176] 0.162 0.219 0.113 0.229 0.136 ...\n $ Metro                : num [1:42176] 1 1 1 1 1 1 1 1 1 1 ...\n $ Micro                : num [1:42176] NA NA NA NA NA NA NA NA NA NA ...\n $ NoCBSAType           : num [1:42176] NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe get a list where each row is a variable in the dataset. We also learn more about the format of the variable (e.g. character, numeric), the number of observations, and we see examples of the first few observations.\nLet’s next use summary() to get a statistical summary of each variable:\n\n\nCode\nsummary(ozs)\n\n\n    geoid              state             Designated       county         \n Length:42176       Length:42176       Min.   :1       Length:42176      \n Class :character   Class :character   1st Qu.:1       Class :character  \n Mode  :character   Mode  :character   Median :1       Mode  :character  \n                                       Mean   :1                         \n                                       3rd Qu.:1                         \n                                       Max.   :1                         \n                                       NA's   :33414                     \n     Type             dec_score         SE_Flag        Population   \n Length:42176       Min.   : 1.000   Min.   :1       Min.   :    0  \n Class :character   1st Qu.: 3.000   1st Qu.:1       1st Qu.: 2752  \n Mode  :character   Median : 5.000   Median :1       Median : 3897  \n                    Mean   : 5.495   Mean   :1       Mean   : 4147  \n                    3rd Qu.: 8.000   3rd Qu.:1       3rd Qu.: 5224  \n                    Max.   :10.000   Max.   :1       Max.   :40616  \n                    NA's   :1253     NA's   :41111   NA's   :112    \n medhhincome2014_tract  PovertyRate       unemprate          medvalue      \n Min.   :  2499        Min.   :0.0000   Min.   :0.00000   Min.   :   9999  \n 1st Qu.: 32014        1st Qu.:0.1380   1st Qu.:0.05900   1st Qu.:  85700  \n Median : 41094        Median :0.2055   Median :0.08734   Median : 122400  \n Mean   : 42153        Mean   :0.2331   Mean   :0.10063   Mean   : 165663  \n 3rd Qu.: 50833        3rd Qu.:0.2996   3rd Qu.:0.12600   3rd Qu.: 191300  \n Max.   :181406        Max.   :1.0000   Max.   :1.00000   Max.   :2000001  \n NA's   :249           NA's   :141      NA's   :141       NA's   :1106     \n    medrent           pctown       severerentburden  vacancyrate     \n Min.   :  99.0   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 655.0   1st Qu.:0.3833   1st Qu.:0.1662   1st Qu.:0.07115  \n Median : 800.0   Median :0.5728   Median :0.2403   Median :0.11658  \n Mean   : 860.9   Mean   :0.5436   Mean   :0.2476   Mean   :0.14120  \n 3rd Qu.:1010.0   3rd Qu.:0.7316   3rd Qu.:0.3206   3rd Qu.:0.18011  \n Max.   :3501.0   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :395      NA's   :1033     NA's   :189      NA's   :167      \n pctwhitealone    pctblackalone      pctHispanic       pctAAPIalone    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.2040   1st Qu.:0.01072   1st Qu.:0.02602   1st Qu.:0.00000  \n Median :0.5614   Median :0.06656   Median :0.09304   Median :0.00883  \n Mean   :0.5211   Mean   :0.18652   Mean   :0.22060   Mean   :0.03806  \n 3rd Qu.:0.8294   3rd Qu.:0.25000   3rd Qu.:0.32014   3rd Qu.:0.03533  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :0.91144  \n NA's   :131      NA's   :131       NA's   :131       NA's   :131      \n   pctunder18       pctover64         HSorlower        BAorhigher    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.1908   1st Qu.:0.09436   1st Qu.:0.4150   1st Qu.:0.1120  \n Median :0.2300   Median :0.13604   Median :0.5182   Median :0.1679  \n Mean   :0.2295   Mean   :0.14340   Mean   :0.5067   Mean   :0.2034  \n 3rd Qu.:0.2719   3rd Qu.:0.18057   3rd Qu.:0.6113   3rd Qu.:0.2536  \n Max.   :0.6468   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :131      NA's   :131       NA's   :132      NA's   :132     \n     Metro          Micro         NoCBSAType   \n Min.   :1      Min.   :1       Min.   :1      \n 1st Qu.:1      1st Qu.:1       1st Qu.:1      \n Median :1      Median :1       Median :1      \n Mean   :1      Mean   :1       Mean   :1      \n 3rd Qu.:1      3rd Qu.:1       3rd Qu.:1      \n Max.   :1      Max.   :1       Max.   :1      \n NA's   :9111   NA's   :37448   NA's   :37793  \n\n\nThis gives us a statistical summary including distribution and central tendency statistics, as well as information on the number of values that are NA.\nA few things to note after your preliminary inspection:\n\nThese data are at the census tract level and include geographic identifiers including geoid, the combined, state-county-tract FIPS code, state the state name, and county the county name.\nThese data include a field named Designated which is 1 when an eligible tract was designated as an opportunity zone, and NA where the tract was not designated.\nThe dataset also includes some other tract-level demographic measures, as well as additional geographic flags (variables that take the value 0 or 1)."
  },
  {
    "objectID": "assignments/labs/04_describing.html#query-and-describe-the-data",
    "href": "assignments/labs/04_describing.html#query-and-describe-the-data",
    "title": "Describing Places",
    "section": "Query and Describe the Data",
    "text": "Query and Describe the Data\nThe dataset we’re looking at is for the entire United States. We can easily summarize characteristics of the entire dataset."
  },
  {
    "objectID": "assignments/labs/04_describing.html#recoding-values",
    "href": "assignments/labs/04_describing.html#recoding-values",
    "title": "Describing Places",
    "section": "Recoding Values",
    "text": "Recoding Values\nOne of the characteristics tracked in the Urban Institute data is the median household income for each designated census tract. We might question whether there’s a difference in the median household income for designated and not-designated but eligible census tracts. This may help us understand something about whether the most needy tracts were selected from those that are eligible.\nHow would we do this? Conceptually…\n\nWe need to split our data into designated and not designated census tracts, and then calculate the average of the median income separately for these tracts.\nBefore we do this, let’s take care of one bit of housekeeping. The Urban Institute has coded the designated variable as either taking a value of 1 when designated or NA when not. Let’s recode those NA values to equal 0 instead.\nTo recode, we need to select those values from the Designated column in the ozs data frame where the value is NA and overwrite them with a new value of 0.\n\nThere’s lots of ways we could do this:\n\nStrategy 1 - Conditional Statement\nWe could use a conditional statement ifelse() to specify that if a value is NA in the Designated column we change it to 0.\n\n\nCode\nozs |> mutate(Designated = ifelse(is.na(Designated), 0, Designated))\n\n\n# A tibble: 42,176 × 27\n   geoid      state Desig…¹ county Type  dec_s…² SE_Flag Popul…³ medhh…⁴ Pover…⁵\n   <chr>      <chr>   <dbl> <chr>  <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 010010202… Alab…       0 Autau… Low-…       4      NA    2196   41107  0.240 \n 2 010010203… Alab…       0 Autau… Non-…       6      NA    3136   51250  0.107 \n 3 010010207… Alab…       1 Autau… Low-…       9      NA    3047   45234  0.190 \n 4 010010208… Alab…       0 Autau… Non-…      10      NA   10743   61242  0.153 \n 5 010010210… Alab…       0 Autau… Non-…       5      NA    2899   49567  0.151 \n 6 010010211… Alab…       0 Autau… Low-…       6      NA    3247   40801  0.194 \n 7 010030101… Alab…       0 Baldw… Non-…       6      NA    4013   45667  0.140 \n 8 010030102… Alab…       1 Baldw… Low-…       9      NA    3067   33333  0.272 \n 9 010030103… Alab…       0 Baldw… Non-…      10      NA    8079   47443  0.0684\n10 010030104… Alab…       1 Baldw… Non-…       9      NA    4578   46696  0.148 \n# … with 42,166 more rows, 17 more variables: unemprate <dbl>, medvalue <dbl>,\n#   medrent <dbl>, pctown <dbl>, severerentburden <dbl>, vacancyrate <dbl>,\n#   pctwhitealone <dbl>, pctblackalone <dbl>, pctHispanic <dbl>,\n#   pctAAPIalone <dbl>, pctunder18 <dbl>, pctover64 <dbl>, HSorlower <dbl>,\n#   BAorhigher <dbl>, Metro <dbl>, Micro <dbl>, NoCBSAType <dbl>, and\n#   abbreviated variable names ¹​Designated, ²​dec_score, ³​Population,\n#   ⁴​medhhincome2014_tract, ⁵​PovertyRate\n\n\nIn dplyr syntax, what we said here was with reference to the ozs dataset ozs |> let’s alter the dataset mutate(). Let’s alter the column named Designated mutate(Designated = ). Let’s alter the column named Designated conditionally mutate(Designated = ifelse()). If the value of Designated is equal to NA, replace it with 0, otherwise keep the value present in the Designated observation mutate(Designated = ifelse(is.na(Designated), 0, Designated)).\n\n\nLooking at this ifelse() statement, you might have been tempted to write something like Designated ==NA`which will not work.is.na()is the proper logical test to return whether a value is or is notNA`.\n\n\nStrategy 2: Use a Specialized Command\nWe could use a specialized command such as replace_na() from the tidyr package to replace our NA values:\n\n\nCode\nozs |> mutate(Designated = replace_na(Designated, 0))\n\n\n# A tibble: 42,176 × 27\n   geoid      state Desig…¹ county Type  dec_s…² SE_Flag Popul…³ medhh…⁴ Pover…⁵\n   <chr>      <chr>   <dbl> <chr>  <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 010010202… Alab…       0 Autau… Low-…       4      NA    2196   41107  0.240 \n 2 010010203… Alab…       0 Autau… Non-…       6      NA    3136   51250  0.107 \n 3 010010207… Alab…       1 Autau… Low-…       9      NA    3047   45234  0.190 \n 4 010010208… Alab…       0 Autau… Non-…      10      NA   10743   61242  0.153 \n 5 010010210… Alab…       0 Autau… Non-…       5      NA    2899   49567  0.151 \n 6 010010211… Alab…       0 Autau… Low-…       6      NA    3247   40801  0.194 \n 7 010030101… Alab…       0 Baldw… Non-…       6      NA    4013   45667  0.140 \n 8 010030102… Alab…       1 Baldw… Low-…       9      NA    3067   33333  0.272 \n 9 010030103… Alab…       0 Baldw… Non-…      10      NA    8079   47443  0.0684\n10 010030104… Alab…       1 Baldw… Non-…       9      NA    4578   46696  0.148 \n# … with 42,166 more rows, 17 more variables: unemprate <dbl>, medvalue <dbl>,\n#   medrent <dbl>, pctown <dbl>, severerentburden <dbl>, vacancyrate <dbl>,\n#   pctwhitealone <dbl>, pctblackalone <dbl>, pctHispanic <dbl>,\n#   pctAAPIalone <dbl>, pctunder18 <dbl>, pctover64 <dbl>, HSorlower <dbl>,\n#   BAorhigher <dbl>, Metro <dbl>, Micro <dbl>, NoCBSAType <dbl>, and\n#   abbreviated variable names ¹​Designated, ²​dec_score, ³​Population,\n#   ⁴​medhhincome2014_tract, ⁵​PovertyRate\n\n\nNote that in replace_na() we are specifying the column we want to replace the NA value in as well as the value we want to replace NA with.\n\n\nStrategy 3: Recode and Change Format\nDepending upon what we wanted to do with our Designated labels, we could simultaneously deal with recoding our NA values and relabeling the values for legibility. case_when() is useful for these more complex operations:\n\n\nCode\nozs |> mutate(\n  Designated = case_when(\n    Designated == 1 ~\"Designated\",\n    is.na(Designated) ~\"Not Designated\"\n))\n\n\n# A tibble: 42,176 × 27\n   geoid      state Desig…¹ county Type  dec_s…² SE_Flag Popul…³ medhh…⁴ Pover…⁵\n   <chr>      <chr> <chr>   <chr>  <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 010010202… Alab… Not De… Autau… Low-…       4      NA    2196   41107  0.240 \n 2 010010203… Alab… Not De… Autau… Non-…       6      NA    3136   51250  0.107 \n 3 010010207… Alab… Design… Autau… Low-…       9      NA    3047   45234  0.190 \n 4 010010208… Alab… Not De… Autau… Non-…      10      NA   10743   61242  0.153 \n 5 010010210… Alab… Not De… Autau… Non-…       5      NA    2899   49567  0.151 \n 6 010010211… Alab… Not De… Autau… Low-…       6      NA    3247   40801  0.194 \n 7 010030101… Alab… Not De… Baldw… Non-…       6      NA    4013   45667  0.140 \n 8 010030102… Alab… Design… Baldw… Low-…       9      NA    3067   33333  0.272 \n 9 010030103… Alab… Not De… Baldw… Non-…      10      NA    8079   47443  0.0684\n10 010030104… Alab… Design… Baldw… Non-…       9      NA    4578   46696  0.148 \n# … with 42,166 more rows, 17 more variables: unemprate <dbl>, medvalue <dbl>,\n#   medrent <dbl>, pctown <dbl>, severerentburden <dbl>, vacancyrate <dbl>,\n#   pctwhitealone <dbl>, pctblackalone <dbl>, pctHispanic <dbl>,\n#   pctAAPIalone <dbl>, pctunder18 <dbl>, pctover64 <dbl>, HSorlower <dbl>,\n#   BAorhigher <dbl>, Metro <dbl>, Micro <dbl>, NoCBSAType <dbl>, and\n#   abbreviated variable names ¹​Designated, ²​dec_score, ³​Population,\n#   ⁴​medhhincome2014_tract, ⁵​PovertyRate\n\n\nWhat’s going on here? case_when() allows us to conditionally recode values. We specify the condition and then what to do when that condition is met. For instance, we specify the condition Designated == 1 and then say when this condition is met, we want you to change that observation to Designated ~\"Designated\". We then say what to do if the value is NA - label it as “Not Designated”.\nFor the sake of legibility, let’s use the third strategy on our dataset:\n\n\nCode\nozs <- ozs |> mutate(\n  Designated = case_when(\n    Designated == 1 ~\"Designated\",\n    is.na(Designated) ~\"Not Designated\"\n))\n\n\nAnd here’s what our Designated column now looks like:\n\n\n\n\n\n\n  \n  \n    \n      geoid\n      state\n      county\n      Designated\n    \n  \n  \n    01001020200\nAlabama\nAutauga County\nNot Designated\n    01001020300\nAlabama\nAutauga County\nNot Designated\n    01001020700\nAlabama\nAutauga County\nDesignated\n    01001020802\nAlabama\nAutauga County\nNot Designated\n    01001021000\nAlabama\nAutauga County\nNot Designated\n    01001021100\nAlabama\nAutauga County\nNot Designated"
  },
  {
    "objectID": "assignments/labs/04_describing.html#lab-evaluation",
    "href": "assignments/labs/04_describing.html#lab-evaluation",
    "title": "Describing Places",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/04_describing.html#references",
    "href": "assignments/labs/04_describing.html#references",
    "title": "Describing Places",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/04_describing.html#making-nice-tables",
    "href": "assignments/labs/04_describing.html#making-nice-tables",
    "title": "Describing Places",
    "section": "Making Nice Tables",
    "text": "Making Nice Tables\nAs many of you have remarked in class, outputting “nice” tables is not R’s default. There are several packages that can help to clean up tables and make them presentable. Let’s learn how to use one such package, the gt package. Similar to how GGPlot describes a grammar of graphics for visualizations, gt similarly provides methods to shape elements of a table.\n\nTable Components in GT\nIn GT, there are numerous table components which you can format as you wish:\n\ngt’s documentation can help you become more familiar with these different components.\n\n\nMaking a First GT Table\nLet’s start off by taking the Illinois data we were previously working on and styling the table using gt:\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |> pivot_wider(names_from = Designated, values_from = Income) |> \n  mutate(Difference = Designated - `Not Designated`) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      Designated\n      Not Designated\n      Difference\n    \n  \n  \n    \n      Adams County\n    \n    26012.00\n39614.22\n-13602.2222\n    \n      Alexander County\n    \n    21500.00\n32809.00\n-11309.0000\n    \n      Bond County\n    \n    49590.00\n52310.00\n-2720.0000\n    \n      Boone County\n    \n    40599.00\n45742.00\n-5143.0000\n    \n      Bureau County\n    \n    48083.00\n60338.67\n-12255.6667\n    \n      Calhoun County\n    \n    NA\n55290.00\nNA\n    \n      Carroll County\n    \n    35184.00\n58942.00\n-23758.0000\n    \n      Cass County\n    \n    37679.00\n46840.50\n-9161.5000\n    \n      Champaign County\n    \n    13988.83\n45603.57\n-31614.7319\n    \n      Christian County\n    \n    36164.00\n45945.43\n-9781.4286\n    \n      Clark County\n    \n    44460.00\n50004.67\n-5544.6667\n    \n      Clay County\n    \n    39261.00\n50853.00\n-11592.0000\n    \n      Clinton County\n    \n    38047.00\n51269.00\n-13222.0000\n    \n      Coles County\n    \n    30682.00\n37615.00\n-6933.0000\n    \n      Cook County\n    \n    29154.50\n45208.69\n-16054.1904\n    \n      Crawford County\n    \n    41071.00\n47089.67\n-6018.6667\n    \n      Cumberland County\n    \n    41761.00\n43780.00\n-2019.0000\n    \n      De Witt County\n    \n    44181.00\n44048.00\n133.0000\n    \n      DeKalb County\n    \n    18462.67\n55294.50\n-36831.8333\n    \n      Douglas County\n    \n    54444.00\n51593.50\n2850.5000\n    \n      DuPage County\n    \n    47140.00\n60237.04\n-13097.0435\n    \n      Edgar County\n    \n    39306.00\n47314.00\n-8008.0000\n    \n      Edwards County\n    \n    39844.00\n51919.00\n-12075.0000\n    \n      Effingham County\n    \n    34028.00\n47879.67\n-13851.6667\n    \n      Fayette County\n    \n    34375.00\n44675.67\n-10300.6667\n    \n      Franklin County\n    \n    36469.00\n40014.18\n-3545.1818\n    \n      Fulton County\n    \n    38279.00\n45053.11\n-6774.1111\n    \n      Gallatin County\n    \n    38750.00\n45074.00\n-6324.0000\n    \n      Greene County\n    \n    35469.00\n43493.25\n-8024.2500\n    \n      Hamilton County\n    \n    54293.00\n47627.50\n6665.5000\n    \n      Hancock County\n    \n    50924.00\n49058.00\n1866.0000\n    \n      Hardin County\n    \n    34250.00\n43860.00\n-9610.0000\n    \n      Henderson County\n    \n    49453.00\n45447.50\n4005.5000\n    \n      Henry County\n    \n    31845.00\n45385.20\n-13540.2000\n    \n      Iroquois County\n    \n    34167.00\n47590.80\n-13423.8000\n    \n      Jackson County\n    \n    16960.00\n34503.50\n-17543.5000\n    \n      Jasper County\n    \n    39909.00\nNA\nNA\n    \n      Jefferson County\n    \n    18411.00\n45013.62\n-26602.6250\n    \n      Jersey County\n    \n    54435.00\n49565.25\n4869.7500\n    \n      Jo Daviess County\n    \n    46731.00\n49293.50\n-2562.5000\n    \n      Johnson County\n    \n    40578.00\n38164.67\n2413.3333\n    \n      Kane County\n    \n    47129.00\n50278.10\n-3149.1000\n    \n      Kankakee County\n    \n    23971.50\n48993.08\n-25021.5769\n    \n      Knox County\n    \n    23011.00\n36096.67\n-13085.6667\n    \n      Lake County\n    \n    29174.60\n48117.03\n-18942.4303\n    \n      LaSalle County\n    \n    38625.00\n46211.08\n-7586.0769\n    \n      Lawrence County\n    \n    37883.00\n42074.33\n-4191.3333\n    \n      Lee County\n    \n    43750.00\n53669.33\n-9919.3333\n    \n      Livingston County\n    \n    33250.00\nNA\nNA\n    \n      Logan County\n    \n    41824.00\n54024.25\n-12200.2500\n    \n      Macon County\n    \n    21405.20\n40130.44\n-18725.2375\n    \n      Macoupin County\n    \n    48000.00\n53369.14\n-5369.1429\n    \n      Madison County\n    \n    28699.80\n49991.43\n-21291.6286\n    \n      Marion County\n    \n    24932.00\n39650.29\n-14718.2857\n    \n      Marshall County\n    \n    49625.00\n56748.00\n-7123.0000\n    \n      Mason County\n    \n    35179.00\n46672.20\n-11493.2000\n    \n      Massac County\n    \n    32198.00\n44130.00\n-11932.0000\n    \n      McDonough County\n    \n    NaN\n44538.33\nNaN\n    \n      McHenry County\n    \n    42972.00\n49938.00\n-6966.0000\n    \n      McLean County\n    \n    11053.00\n43965.63\n-32912.6316\n    \n      Mercer County\n    \n    NA\n46705.00\nNA\n    \n      Montgomery County\n    \n    44438.00\n43921.67\n516.3333\n    \n      Morgan County\n    \n    34314.00\n38475.33\n-4161.3333\n    \n      Moultrie County\n    \n    40761.00\n59008.00\n-18247.0000\n    \n      Ogle County\n    \n    43393.00\n47580.00\n-4187.0000\n    \n      Peoria County\n    \n    20660.00\n39899.95\n-19239.9545\n    \n      Perry County\n    \n    31635.00\n47934.00\n-16299.0000\n    \n      Piatt County\n    \n    NA\n50185.00\nNA\n    \n      Pike County\n    \n    37837.00\n41462.75\n-3625.7500\n    \n      Pope County\n    \n    32396.00\n44375.00\n-11979.0000\n    \n      Pulaski County\n    \n    28884.00\n35643.00\n-6759.0000\n    \n      Randolph County\n    \n    36902.00\n48542.14\n-11640.1429\n    \n      Richland County\n    \n    24671.00\n48985.75\n-24314.7500\n    \n      Rock Island County\n    \n    31542.50\n40817.41\n-9274.9091\n    \n      Saline County\n    \n    32188.00\n43553.62\n-11365.6250\n    \n      Sangamon County\n    \n    22488.80\n40009.96\n-17521.1600\n    \n      Schuyler County\n    \n    52768.00\n48594.00\n4174.0000\n    \n      Scott County\n    \n    NA\n47024.00\nNA\n    \n      Shelby County\n    \n    37689.00\n49664.25\n-11975.2500\n    \n      St. Clair County\n    \n    20159.71\n32147.79\n-11988.0774\n    \n      Stark County\n    \n    NA\n52614.00\nNA\n    \n      Stephenson County\n    \n    26674.50\n46470.00\n-19795.5000\n    \n      Tazewell County\n    \n    47367.00\n47820.73\n-453.7273\n    \n      Union County\n    \n    40506.00\n48029.75\n-7523.7500\n    \n      Vermilion County\n    \n    28462.33\n39596.79\n-11134.4524\n    \n      Wabash County\n    \n    NA\n54207.00\nNA\n    \n      Warren County\n    \n    37185.00\n48814.67\n-11629.6667\n    \n      Washington County\n    \n    46406.00\n57554.00\n-11148.0000\n    \n      Wayne County\n    \n    47645.00\n45951.25\n1693.7500\n    \n      White County\n    \n    31250.00\n48981.00\n-17731.0000\n    \n      Whiteside County\n    \n    45106.00\n45458.10\n-352.1000\n    \n      Will County\n    \n    30419.00\n50754.27\n-20335.2667\n    \n      Williamson County\n    \n    38083.00\n42193.00\n-4110.0000\n    \n      Winnebago County\n    \n    20346.00\n37655.07\n-17309.0714\n    \n      Woodford County\n    \n    NA\n61875.00\nNA\n  \n  \n  \n\n\n\n\nWe simply added gt() as a final command after producing a summary table.\n\n\nFormatting Columns\nThere’s a range of formatting options we can take advantage of to style our table. most formatting options begin with fmt_ which makes it fairly easy to search for the format type you’re looking for. Let’s format the “Designated”, “Not Designated”, and “Difference” columns as currency with no decimal places. While we’re at it, let’s rename the “county” column so “county” is capitalized:\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |> pivot_wider(names_from = Designated, values_from = Income) |> \n  mutate(Difference = Designated - `Not Designated`) |> \n  ungroup() |> \n  gt() |> \n  fmt_currency(2:4, decimals = 0) |> \n  cols_label(county = \"County\")\n\n\n\n\n\n\n  \n  \n    \n      County\n      Designated\n      Not Designated\n      Difference\n    \n  \n  \n    Adams County\n$26,012\n$39,614\n−$13,602\n    Alexander County\n$21,500\n$32,809\n−$11,309\n    Bond County\n$49,590\n$52,310\n−$2,720\n    Boone County\n$40,599\n$45,742\n−$5,143\n    Bureau County\n$48,083\n$60,339\n−$12,256\n    Calhoun County\nNA\n$55,290\nNA\n    Carroll County\n$35,184\n$58,942\n−$23,758\n    Cass County\n$37,679\n$46,840\n−$9,162\n    Champaign County\n$13,989\n$45,604\n−$31,615\n    Christian County\n$36,164\n$45,945\n−$9,781\n    Clark County\n$44,460\n$50,005\n−$5,545\n    Clay County\n$39,261\n$50,853\n−$11,592\n    Clinton County\n$38,047\n$51,269\n−$13,222\n    Coles County\n$30,682\n$37,615\n−$6,933\n    Cook County\n$29,155\n$45,209\n−$16,054\n    Crawford County\n$41,071\n$47,090\n−$6,019\n    Cumberland County\n$41,761\n$43,780\n−$2,019\n    De Witt County\n$44,181\n$44,048\n$133\n    DeKalb County\n$18,463\n$55,294\n−$36,832\n    Douglas County\n$54,444\n$51,594\n$2,850\n    DuPage County\n$47,140\n$60,237\n−$13,097\n    Edgar County\n$39,306\n$47,314\n−$8,008\n    Edwards County\n$39,844\n$51,919\n−$12,075\n    Effingham County\n$34,028\n$47,880\n−$13,852\n    Fayette County\n$34,375\n$44,676\n−$10,301\n    Franklin County\n$36,469\n$40,014\n−$3,545\n    Fulton County\n$38,279\n$45,053\n−$6,774\n    Gallatin County\n$38,750\n$45,074\n−$6,324\n    Greene County\n$35,469\n$43,493\n−$8,024\n    Hamilton County\n$54,293\n$47,628\n$6,666\n    Hancock County\n$50,924\n$49,058\n$1,866\n    Hardin County\n$34,250\n$43,860\n−$9,610\n    Henderson County\n$49,453\n$45,448\n$4,006\n    Henry County\n$31,845\n$45,385\n−$13,540\n    Iroquois County\n$34,167\n$47,591\n−$13,424\n    Jackson County\n$16,960\n$34,504\n−$17,544\n    Jasper County\n$39,909\nNA\nNA\n    Jefferson County\n$18,411\n$45,014\n−$26,603\n    Jersey County\n$54,435\n$49,565\n$4,870\n    Jo Daviess County\n$46,731\n$49,294\n−$2,562\n    Johnson County\n$40,578\n$38,165\n$2,413\n    Kane County\n$47,129\n$50,278\n−$3,149\n    Kankakee County\n$23,972\n$48,993\n−$25,022\n    Knox County\n$23,011\n$36,097\n−$13,086\n    Lake County\n$29,175\n$48,117\n−$18,942\n    LaSalle County\n$38,625\n$46,211\n−$7,586\n    Lawrence County\n$37,883\n$42,074\n−$4,191\n    Lee County\n$43,750\n$53,669\n−$9,919\n    Livingston County\n$33,250\nNA\nNA\n    Logan County\n$41,824\n$54,024\n−$12,200\n    Macon County\n$21,405\n$40,130\n−$18,725\n    Macoupin County\n$48,000\n$53,369\n−$5,369\n    Madison County\n$28,700\n$49,991\n−$21,292\n    Marion County\n$24,932\n$39,650\n−$14,718\n    Marshall County\n$49,625\n$56,748\n−$7,123\n    Mason County\n$35,179\n$46,672\n−$11,493\n    Massac County\n$32,198\n$44,130\n−$11,932\n    McDonough County\nNaN\n$44,538\nNaN\n    McHenry County\n$42,972\n$49,938\n−$6,966\n    McLean County\n$11,053\n$43,966\n−$32,913\n    Mercer County\nNA\n$46,705\nNA\n    Montgomery County\n$44,438\n$43,922\n$516\n    Morgan County\n$34,314\n$38,475\n−$4,161\n    Moultrie County\n$40,761\n$59,008\n−$18,247\n    Ogle County\n$43,393\n$47,580\n−$4,187\n    Peoria County\n$20,660\n$39,900\n−$19,240\n    Perry County\n$31,635\n$47,934\n−$16,299\n    Piatt County\nNA\n$50,185\nNA\n    Pike County\n$37,837\n$41,463\n−$3,626\n    Pope County\n$32,396\n$44,375\n−$11,979\n    Pulaski County\n$28,884\n$35,643\n−$6,759\n    Randolph County\n$36,902\n$48,542\n−$11,640\n    Richland County\n$24,671\n$48,986\n−$24,315\n    Rock Island County\n$31,542\n$40,817\n−$9,275\n    Saline County\n$32,188\n$43,554\n−$11,366\n    Sangamon County\n$22,489\n$40,010\n−$17,521\n    Schuyler County\n$52,768\n$48,594\n$4,174\n    Scott County\nNA\n$47,024\nNA\n    Shelby County\n$37,689\n$49,664\n−$11,975\n    St. Clair County\n$20,160\n$32,148\n−$11,988\n    Stark County\nNA\n$52,614\nNA\n    Stephenson County\n$26,674\n$46,470\n−$19,796\n    Tazewell County\n$47,367\n$47,821\n−$454\n    Union County\n$40,506\n$48,030\n−$7,524\n    Vermilion County\n$28,462\n$39,597\n−$11,134\n    Wabash County\nNA\n$54,207\nNA\n    Warren County\n$37,185\n$48,815\n−$11,630\n    Washington County\n$46,406\n$57,554\n−$11,148\n    Wayne County\n$47,645\n$45,951\n$1,694\n    White County\n$31,250\n$48,981\n−$17,731\n    Whiteside County\n$45,106\n$45,458\n−$352\n    Will County\n$30,419\n$50,754\n−$20,335\n    Williamson County\n$38,083\n$42,193\n−$4,110\n    Winnebago County\n$20,346\n$37,655\n−$17,309\n    Woodford County\nNA\n$61,875\nNA\n  \n  \n  \n\n\n\n\nNote that cols_label allows us to adjust column labels by supplying the variable name and then the desired name of the column.\ngt provides a nice workflow example to show you step by step how you might apply formatting options to style a table. Also see the reference guide for specific formats.\n\n\nSaving a Table\nWe can easily use code to insert a table into our document, but what if you want to save it out as a separate file? The gtsave() command allows you to save your formatted table in a variety of formats.\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |> pivot_wider(names_from = Designated, values_from = Income) |> \n  mutate(Difference = Designated - `Not Designated`) |> \n  ungroup() |> \n  gt() |> \n  fmt_currency(2:4, decimals = 0) |> \n  cols_label(county = \"County\") |> \n  gtsave(\"il_difference.png\", \"04_describing/output\")\n\n\nAnd here’s the actual table that was produced:"
  },
  {
    "objectID": "assignments/labs/04_describing.html#summarizing-data",
    "href": "assignments/labs/04_describing.html#summarizing-data",
    "title": "Describing Places",
    "section": "Summarizing Data",
    "text": "Summarizing Data\nNow that we’ve recoded our designated column, let’s do some description of the characteristics of designated and not designated places.\nLet’s use a combination of group_by() and summarise() to produce a summary table showing the mean value for designated and not designated census tracts.\n\n\nCode\nozs |> \n  group_by(Designated) |> \n  summarise(Income = mean(medhhincome2014_tract))\n\n\n# A tibble: 2 × 2\n  Designated     Income\n  <chr>           <dbl>\n1 Designated         NA\n2 Not Designated     NA\n\n\nWe getting a table back, but why did we get NA insted of numbers here? If you’ve ever used the average mean() command in R, you probably understand what’s going on here. As a safety measure, when you average values, R will return NA if any value in that series is NA. If you’re not expecting any NA values, this is good, becuase you’ll quickly discover that there are unexpected NA values in your dataset. We might expect a few census tracts with missing income values coded as NA, so we will want to indicate na.rm = TRUE here so that R removes those NAs when calculating the mean.\n\n\nCode\nozs |> \n  group_by(Designated) |> \n  summarise(Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 2 × 2\n  Designated     Income\n  <chr>           <dbl>\n1 Designated     33345.\n2 Not Designated 44446.\n\n\nMuch better. We can see that that on average, the median household income for eligible designated census tracts is lower than that for eligible not designated census tracts. Since the Opportunity Zone legislation is designed to target distressed neighborhoods, this is a good sign that program targeting is focused on neighborhoods with greater need.\nWe might want to add some additional information to our summary table. One useful piece of information would be the number of census tracts that are designated or not designated.\n\n\nCode\nozs |> \n  group_by(Designated) |> \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 2 × 3\n  Designated     Tracts Income\n  <chr>           <int>  <dbl>\n1 Designated       8762 33345.\n2 Not Designated  33414 44446.\n\n\nWithin a summarise() statement, n() gives us a count of observations (rows) for each grouping. In this case, there are 8,762 census tracts designated as opportunity zones, and an additional 33,414 that were eligible based upon program criteria but not designated.\nWe could easily add other summaries to our summary table for this dataset, or further modify."
  },
  {
    "objectID": "assignments/labs/04_describing.html#filtering-data",
    "href": "assignments/labs/04_describing.html#filtering-data",
    "title": "Describing Places",
    "section": "Filtering Data",
    "text": "Filtering Data\nNow that we have some sense for how we might produce basic summaries of our data, how can we query out (filter) observations by row? How, for instance, would you modify the above code to produce the same table for counties in Illinois?\nWe can use a filter() statement to easily accomplish this. filter() allows us to specify one (or more) criteria for which we want to select rows from a larger dataset.\nLet’s take a step back and filter our base dataset to focus on observations in Illinois.\n\n\nCode\nozs |> filter(state == \"Illinois\")\n\n\n# A tibble: 1,659 × 27\n   geoid      state Desig…¹ county Type  dec_s…² SE_Flag Popul…³ medhh…⁴ Pover…⁵\n   <chr>      <chr> <chr>   <chr>  <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 170010002… Illi… Not De… Adams… Non-…       7      NA    1937   41538  0.167 \n 2 170010002… Illi… Not De… Adams… Low-…       1      NA    2563   40018  0.171 \n 3 170010004… Illi… Not De… Adams… Low-…       1      NA    3403   28819  0.316 \n 4 170010005… Illi… Not De… Adams… Low-…       1      NA    2298   32313  0.252 \n 5 170010007… Illi… Not De… Adams… Low-…       1      NA    1259   17850  0.369 \n 6 170010008… Illi… Design… Adams… Low-…       1      NA    2700   26012  0.344 \n 7 170010009… Illi… Not De… Adams… Low-…       5      NA    2671   40475  0.282 \n 8 170010101… Illi… Not De… Adams… Non-…       2      NA    4323   50156  0.0800\n 9 170010102… Illi… Not De… Adams… Low-…       2      NA    3436   46350  0.107 \n10 170010103… Illi… Not De… Adams… Non-…       8      NA    6038   59009  0.0617\n# … with 1,649 more rows, 17 more variables: unemprate <dbl>, medvalue <dbl>,\n#   medrent <dbl>, pctown <dbl>, severerentburden <dbl>, vacancyrate <dbl>,\n#   pctwhitealone <dbl>, pctblackalone <dbl>, pctHispanic <dbl>,\n#   pctAAPIalone <dbl>, pctunder18 <dbl>, pctover64 <dbl>, HSorlower <dbl>,\n#   BAorhigher <dbl>, Metro <dbl>, Micro <dbl>, NoCBSAType <dbl>, and\n#   abbreviated variable names ¹​Designated, ²​dec_score, ³​Population,\n#   ⁴​medhhincome2014_tract, ⁵​PovertyRate\n\n\nRecall that the ozs dataset has 42,176 observations (rows). We filtered the data using the criteria that the value of state is equal to “Illinois”, resulting in 1,659 observations (eligible census tracts in Illinois).\nFrom here, we can re-use our prior code to produce a summary table that is focused on Illinois.\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(Designated) |> \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 2 × 3\n  Designated     Tracts Income\n  <chr>           <int>  <dbl>\n1 Designated        327 30504.\n2 Not Designated   1332 45281.\n\n\nOk - but how do we summarise by county? We just need to add that as an additional grouping criteria in our group_by() statement:\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 181 × 4\n# Groups:   county [95]\n   county           Designated     Tracts Income\n   <chr>            <chr>           <int>  <dbl>\n 1 Adams County     Designated          1 26012 \n 2 Adams County     Not Designated      9 39614.\n 3 Alexander County Designated          1 21500 \n 4 Alexander County Not Designated      3 32809 \n 5 Bond County      Designated          1 49590 \n 6 Bond County      Not Designated      1 52310 \n 7 Boone County     Designated          1 40599 \n 8 Boone County     Not Designated      2 45742 \n 9 Bureau County    Designated          1 48083 \n10 Bureau County    Not Designated      3 60339.\n# … with 171 more rows\n\n\nWe are basically saying, group by both county and designated and then summarize for each.\nWith a few lines of code, we can produce very powerful and specific kinds of summaries for our data."
  },
  {
    "objectID": "assignments/labs/04_describing.html#pivoting-data",
    "href": "assignments/labs/04_describing.html#pivoting-data",
    "title": "Describing Places",
    "section": "Pivoting Data",
    "text": "Pivoting Data\nOur summary is getting more nuanced. We’ve used group_by() and summarise() to sumamrise data based upon certain characteristics. We’ve summarized in such a way where for our Illinois counties, we have two observations for each county - one that summarises values for designated tracts in that county, and one that summarises values for not designated tracts.\nIt might be useful for us to reshape our summary table so that there is one row for each county, with each row containing the summary value for both designated and not designated tracts.\nThe two commands pivot_wider() and pivot_longer() are useful for reshaping our data. pivot_wider() essentially adds columns to a dataset by transitioning content from rows to columns. pivot_longer() does the opposite - it makes a dataset longer by transitioning columns to rows.\nIn our case, let’s use pivot_wider() to transition our Designated and Not Designated rows into columns.\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |> pivot_wider(names_from = Designated, values_from = Income)\n\n\n# A tibble: 172 × 4\n# Groups:   county [95]\n   county           Tracts Designated `Not Designated`\n   <chr>             <int>      <dbl>            <dbl>\n 1 Adams County          1      26012              NA \n 2 Adams County          9         NA           39614.\n 3 Alexander County      1      21500              NA \n 4 Alexander County      3         NA           32809 \n 5 Bond County           1      49590           52310 \n 6 Boone County          1      40599              NA \n 7 Boone County          2         NA           45742 \n 8 Bureau County         1      48083              NA \n 9 Bureau County         3         NA           60339.\n10 Calhoun County        2         NA           55290 \n# … with 162 more rows\n\n\nWe start with our previous summary and pass two arguments to pivot_wider().\nWe use names_from to specify the column in our dataset contining row values that we want to become new columns. In this case we’d expect that our Desginated column would result in the creation of two new columns - one where values are Designated and one where values are Not Designated.\nWe use values_from to specify the column containing the values we want in our new columns, in this case, the average of tract income.\nOne problem though - our tract count column is still present and these values are not reshaped. To simplify things, let’s just get rid of this count so we can see what things look like:\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |> pivot_wider(names_from = Designated, values_from = Income)\n\n\n# A tibble: 95 × 3\n# Groups:   county [95]\n   county           Designated `Not Designated`\n   <chr>                 <dbl>            <dbl>\n 1 Adams County         26012            39614.\n 2 Alexander County     21500            32809 \n 3 Bond County          49590            52310 \n 4 Boone County         40599            45742 \n 5 Bureau County        48083            60339.\n 6 Calhoun County          NA            55290 \n 7 Carroll County       35184            58942 \n 8 Cass County          37679            46840.\n 9 Champaign County     13989.           45604.\n10 Christian County     36164            45945.\n# … with 85 more rows\n\n\nLooking good! To make things a bit more informative, let’s also show the difference in income between designated and not designated tracts:\n\n\nCode\nozs |> \n  filter(state == \"Illinois\") |> \n  group_by(county, Designated) |> \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |> pivot_wider(names_from = Designated, values_from = Income) |> \n  mutate(Difference = Designated - `Not Designated`)\n\n\n# A tibble: 95 × 4\n# Groups:   county [95]\n   county           Designated `Not Designated` Difference\n   <chr>                 <dbl>            <dbl>      <dbl>\n 1 Adams County         26012            39614.    -13602.\n 2 Alexander County     21500            32809     -11309 \n 3 Bond County          49590            52310      -2720 \n 4 Boone County         40599            45742      -5143 \n 5 Bureau County        48083            60339.    -12256.\n 6 Calhoun County          NA            55290         NA \n 7 Carroll County       35184            58942     -23758 \n 8 Cass County          37679            46840.     -9162.\n 9 Champaign County     13989.           45604.    -31615.\n10 Christian County     36164            45945.     -9781.\n# … with 85 more rows\n\n\nOne note here - in the last mutate() statement, you see that Not Designated has backticks around it. This is because there’s a space between “Not” and “Designated” which will be treated as separate variable names. The backticks allow this to be referenced as a column. We could change the name to something like Not_Designated, but backticks will allow us to appropriately reference it as well."
  },
  {
    "objectID": "assignments/labs/04_describing.html#joining-tables",
    "href": "assignments/labs/04_describing.html#joining-tables",
    "title": "Describing Places",
    "section": "Joining Tables",
    "text": "Joining Tables\nLinking together the place data to the ozs data might would give us some additional context regarding opportunity zones. Remember that the opportunity zones data itemizes those census tracts that were eligible for designation with the Designated column denoting which eligible census tracts actually became opportunity zones. If we link together information for census tracts which were not eligible for designation, we could learn something about the differences between undesignated, eligible not designated, and eligible designated census tracts.\nIn order to link together these two datasets, we need to learn about and apply relational joins to bring these two datasets together.\n\nJoins Overview\nJoins are methods to merge two datasets based on common attributes. It’s rare that one single dataset contains all of the information you wish to tell a story about, so it’s important to understand how joins work.\nA Venn diagram of join types.\n The tidyverse package which we’ve installed and loaded in the past can perform seven different types of relational joins. We’ll discuss six of them briefly, but focus on four key types. Joins require us to have two tables with some type of common identifier column present in both that we can match records based on.\n\n\nJoin Types\nLet’s assume we have two data frames named x and y, and we’re trying to match a column called key in both datasets.\n\nleft_join(): A left join returns every row in x and all the columns from x and y. If a value in the key column in x doesn’t exist in y, the row will contain NA values for all the y columns. If there are multiple key matches, all the possible combinations will be returned.\nright_join(): This is similar to a left join, but returns every row in y instead.\ninner_join(): An inner join returns all the rows in x where there is an key match in y, and all the columns in x and y.\nfull_join(): A full join returns all rows and all columns from x and y. If there isn’t a match in the x key column, all x columns will return NA. (The same is true for y.)\nsemi_join(): A semi-join returns the rows in x where there is an key match in y. It is different than an inner join in that it only returns the columns in x and doesn’t duplicate rows if there are multiple matches in y.\nanti_join(): An anti-join returns the rows in x where there is not a matching key in y. It only returns the columns in x.\n\nYou’ll notice that only the first four joins—left, right, inner, and full—merge two datasets. Those are going to be the most valuable ones to learn. Here are a couple of additional illustrations to illustrate how joins work.\n  The basic general syntax for the joins is the same:\n*_join(x, y, by = \"key name\")\nx and y are self-explanatory. The by attribute is used to name the key, or the variable that the two datasets have in common. If you’re lucky, they’ll have the same name. If you’re unlucky, you’ll have to type a bit more: by = c(\"name1\" = \"name2\"), assuming “name1” is the name of the key column in x and “name2” is the name of the key column in y.\n\n\nExample\nLet’s assume we have two data frames: fruit_1 that contains some characteristics about fruit, and fruit_2 that has some others. Here’s how they’re defined:\nNote that the code above is just another syntax for creating tables as we did in the past.\n\n\nCode\nprint(fruit_1)\n\n\n# A tibble: 4 × 2\n  fruit  color \n  <chr>  <chr> \n1 apple  red   \n2 orange orange\n3 banana yellow\n4 lime   green \n\n\nCode\nprint(fruit_2)\n\n\n# A tibble: 4 × 3\n  fruit  shape price\n  <chr>  <chr> <dbl>\n1 orange round  0.4 \n2 banana long   0.3 \n3 lime   oval   0.25\n4 durian spiky  8   \n\n\nWhat would be the result of a left join, right join, and inner join of fruit_1 and fruit 2?\nNote the following:\n\nThe left join includes all records from fruit_1, but excludes those measures from fruit_2 where there isn’t a match in the fruit column (durian). Note that even though there’s not shape and price information for apples in fruit_2, the rows are still included, but with NA where the data would be were it present.\nThe right join includes all records from fruit_2 but excludes those columns from fruit_1 where there isn’t a match in the fruit column. In this case, we’re missing color information for durian fruit.\nThe inner join includes only those records from fruit_1 and fruit_2 where there were matches in both datasets.\n\nThe powerful thing about these joins is that they allow us to bring together data with different shapes and we can control which elements of the data are joined. Joins will become far more intuitive as you use them more.\n\n\nJoining Together OZs and Place datasets\nNow that we have a sense for how joins work, let’s combine our oz and place into one larger table.\n\n\nCode\nplace <- read_csv(\"04_describing/data/place_name.csv\")\n\n\nRows: 73057 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): STATEFP, COUNTYFP, TRACTCE, GEOID, place_name\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nplace\n\n\n# A tibble: 73,057 × 5\n   STATEFP COUNTYFP TRACTCE GEOID       place_name\n   <chr>   <chr>    <chr>   <chr>       <chr>     \n 1 01      001      020100  01001020100 Prattville\n 2 01      001      020200  01001020200 Prattville\n 3 01      001      020300  01001020300 Prattville\n 4 01      001      020400  01001020400 Prattville\n 5 01      001      020500  01001020500 Prattville\n 6 01      001      020600  01001020600 Prattville\n 7 01      001      020700  01001020700 Prattville\n 8 01      001      020801  01001020801 <NA>      \n 9 01      001      020802  01001020802 <NA>      \n10 01      001      020900  01001020900 <NA>      \n# … with 73,047 more rows\n\n\nIf you take a look at both tables, you’ll note that they have a field in common called geoid. This represents a unique code that is assigned to each census tract geography. Technically, this is a FIPS (Federal Information Processing Standards) code. FIPS codes for tracts are hierarchical - the first two digits are unique to each state, the next three digits correspond to each county, and the remaining six digits are unique to each census tract in that county.\nBecause each tract is labelled with corresponding FIPS codes, we can join the two datasets together based upon this common field. This will become a fairly common action for you that you will repeat over the course of this class.\nNext, we should think carefully about what kind of join we want. We know we have ozs data for a subset of census tracts in the U.S. and we have the place data for a more expansive set of tracts. If we want to preserve the more extensive data (including those rows that do not match up with oz- eligible tracts), what type of join should we use and how would we construct it?\nJust to make sure we get this correct, I’m going to provide you with the way to complete your first join on real data. We have one more issue to deal with here to successfully join our data together. Recall that join takes three arguments - two table objects to join together and at least one common field to complete the join based on. These columns are both labelled geoid, but one is capitalized and one is not. We’ll need to tell our join function that these two columns with different names (different in that one is capitalized and one is not) should be joined to each other. We use the modified by=c(\"GEOID\" = \"geoid\") to denote that GEOID in the place data should be joined to geoid in the ozs data. If the names were the same (say, both were GEOID), we could simply say by=\"GEOID\" and this would work.\nOkay, with that out of the way, let’s join our data together:\n\n\nCode\ndataset<-left_join(place, ozs, by=c(\"GEOID\" = \"geoid\"))\n\n\nInto a new object called dataset, we joined all rows from place and those records from ozs that matched. Records from place without a match in ozs will have NA where there could be data.\nTake a look at the data:\n\n\nCode\nView(dataset)\n\n\nStart by looking at the number of rows in the data - 73,057 - the same number as in the place data - we have brought in all rows from the place data and have joined to in matching rows in the ozs data. It would be useful for us to start off by knowing how many rows fall into each of our three categories - ineligible for designated, eligible and undesignated, and eligible and designated. At this point, the NA values in the Designated column reflect ineligible, the 0’s in that column reflect eligible but not designated, and the 1’s represent eligible and designated.\nUse your new knowledge of dplyr’s group_by() and summarise() to create a summary table based upon the three values that we expect the Designated column to take. You’ve learned that you could define what type of summary you’d like to produce in your summarise() statement. Let’s use n() which counts the number of rows that meet each category specified in group_by():\n\n\nCode\ndataset %>% group_by(Designated) %>% summarise(n())\n\n\n# A tibble: 3 × 2\n  Designated     `n()`\n  <chr>          <int>\n1 Designated      7825\n2 Not Designated 33391\n3 <NA>           31841\n\n\nYou should see that we have 31,841 rows (census tracts) that were ineligible for designation, an additional 33,391 that were eligible but not designated, and 7,825 that were eligible and designated. Excellent!\nWhat might you want to do next to be able to properly label the three categories that now exist for designated?"
  },
  {
    "objectID": "assignments/labs/05_census.html",
    "href": "assignments/labs/05_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "This lab is designed to introduce you to some of the basic techniques for downloading data from the U.S. Census Bureau using R. Following elements from the Klosterman, et al. reading, we will replicate some of the tables present in that chapter.\nWe will also learn a new R package this week - the tidycensus package, which is designed to programmatically download and load census data directly into our R session."
  },
  {
    "objectID": "assignments/labs/05_census.html#goals",
    "href": "assignments/labs/05_census.html#goals",
    "title": "Population and the Census",
    "section": "Goals",
    "text": "Goals\nPlanners frequently rely upon census data to help establish baseline portraits of places. Given the frequency with which planners tend to use Census data, can R help us to more efficiently access this information, and integrate it into analysis processes?\nIn this section, we will examine how Census data is typically accessed, and explore how to do so with R. By the end of this section, you will:\n\nReview how to manually access census data from data.census.gov.\nLearn how to construct basic calls for census data using R and the tidycensus package\nLearn the basic principles of joining data, and apply these"
  },
  {
    "objectID": "assignments/labs/05_census.html#core-concepts",
    "href": "assignments/labs/05_census.html#core-concepts",
    "title": "Population and the Census",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nbind_rows()\nbind_cols()\nget_acs()\n%in% c()\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/05_census.html#github-lab-repository",
    "href": "assignments/labs/05_census.html#github-lab-repository",
    "title": "Population and the Census",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/05_census.html#initial-set-up",
    "href": "assignments/labs/05_census.html#initial-set-up",
    "title": "Population and the Census",
    "section": "Initial Set Up",
    "text": "Initial Set Up\nLet’s start by loading a few packages that will help us with our work. Please go ahead and load (library()) (and install (install.packages()), if necessary) the tidyverse and tidycensus packages.\n\n\nCode\n# Your Code Here\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(tidycensus)"
  },
  {
    "objectID": "assignments/labs/05_census.html#accessing-census-data-via-american-fact-finder",
    "href": "assignments/labs/05_census.html#accessing-census-data-via-american-fact-finder",
    "title": "Population and the Census",
    "section": "Accessing Census Data via American Fact Finder",
    "text": "Accessing Census Data via American Fact Finder\nLet’s start working with Census data! Our goal here is to begin creating tables similar to the ones we see in Klosterman Chapter 2. For the purposes of this exercise, we’ll describe how Champaign, Illinois compares to other cities within Champaign County. We’ll then think about how we might be able to describe smaller geographies from there.\nBefore we get around to downloading census data using R, we’ll start by learning about some of the common ways in which census data are accessed, and will then spend a little time learning how to download data at different summary levels.\nMost casual users of Census data typically start by visiting the data.census.gov website:\n\nFor those users looking to download detailed demographic tables covering more than one geography, “Use Advanced Search” is best option (and the option that we will choose).\nThe Advanced Search window includes a series of query selectors, or if you know the exact name or table number you are looking for, you can use the search dialog. Even if you know the table number, it makes sense to start off by selecting your survey and the geography at which you wish FactFinder to return results.\nThe Census bureau tabulates demographic information for a number of different geographic types, some which are hierarchical, and some which aren’t:\n The Klosterman chapter presents data for a city, and then compares it to its constituent county and region. This diagram tells us that cities (census places) are nested within (and contained by) states, which are nested within census divisions, and census regions which are all contained within the nation. Census places do not have any subgeographies, however, we may also use other sub county geography, especially census tracts and block groups to help us describe the characteristics of census places.\nThe most common geographies used by planners are as follows:\n\nState\nCounty\nPlace (incorporated cities)\nTract (approximate neighborhoods based upon physical boundaries)\n\nSome specialized geographies like Traffic Analysis Zones (TAZs) are also frequently used, as are Zip Code Tabulation Areas (ZCTAs) which approximate postal service Zone Improvement Plan (ZIP) codes.\nIn our FactFinder search, let’s start off by looking for data on the racial composition of census places in Illinois from the 2019 5-Year American Community Survey. This corresponds to data used to create the second column in Table 2.1 of Klosterman. We can add some filters to identify the census data table containing the appropriate data.\nLet’s start by filtering to the 5-Year American Community Survey data (Browse Filters -> Surveys -> American Community Survey)\n\nWe now have 2 main choices - 1-Year estimates and 5-year estimates. What might the difference be between these two data products?\n\n1-Year Estimates are based off of the survey sample collected within one year and reflect the characteristics of the population within that given year.\n5-Year Estimates are based off of a pooled survey sample collected over a 5-year period and reflect the characteristics of the population over those 5 years.\n\nThe trade offs between the two samples are time specificity and certainty around the accuracy of the estimates. Particularly when we are looking at smaller geographies, this certainty question can become more of an issue. More on this later in our documentation.\nFor now, let’s select 5 year estimates, and then select “Detailed Tables”.\n\nAfter adding this filter, add an additional filter under Years for 2021.\n\nWe might also want to add a geographic filter to indicate that the data we want is for census places.\n And then select all places in Illinois…\n\nWith these filters in place we can now search for the race table.\n\nWe’ll then get a list of tables matching our criteria. There are a lot of them! You will need to click “View All Tables” to see the entire list. Let’s take a closer look at Table B02001: Race. We can click on it to view the table’s contents.\n This is useful - we can see the structure of the table. Rows contain information associated with racial identification groups, columns correspond to census places (cities and local governments). Observations include counts of the population that identify with each racial group.\nWe could download this file and open it in a spreadsheet program. \nWe could download this file and open it in a spreadsheet program or R. If you download the .csv file, we’d need to do some reformatting to start using the data in any serious way.\nAs we start manipulating the data with more clicks (say to remove the double header row, to delete the GEO.id column, or to work with estimates) we introduce the potential for user error, especially if we want to exactly reproduce what we’ve done over and over again. Of course the potential for error grows once our data takes on more dimensions or involves multiple tables or sources.\nAs an alternative strategy, let’s look at how we’d do the same thing programmatically in R."
  },
  {
    "objectID": "assignments/labs/05_census.html#workflow-1-getting-census-data",
    "href": "assignments/labs/05_census.html#workflow-1-getting-census-data",
    "title": "Population and the Census",
    "section": "Workflow 1: Getting Census Data",
    "text": "Workflow 1: Getting Census Data"
  },
  {
    "objectID": "assignments/labs/05_census.html#lab-evaluation",
    "href": "assignments/labs/05_census.html#lab-evaluation",
    "title": "Population and the Census",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nUse of dplyr and tidyverse style formatting in your coding.\nRefined table output formatting using tools such as gt.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/05_census.html#references",
    "href": "assignments/labs/05_census.html#references",
    "title": "Population and the Census",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/05_census.html#workflow-downloading-census-data-programmatically-in-r",
    "href": "assignments/labs/05_census.html#workflow-downloading-census-data-programmatically-in-r",
    "title": "Population and the Census",
    "section": "Workflow: Downloading Census Data Programmatically in R",
    "text": "Workflow: Downloading Census Data Programmatically in R\nThe tidycensus package made by Kyle Walker downloads data directly from the U.S. Census Bureau’s application programming interface (API). While we reviewed using data.census.gov to manually select and download data from the Census, the benefit of using tidycensus and R is that the data pipeline is reproducible.\nIn order to use tidycensus to download data, you’ll need the following:\n\nA Census API key (if you don’t already have one, please request one from the census bureau)\nA desired geography to look up (for example, all census places in Illinois)\nThe census year you want data for (we’ll start with 2021)\nThe survey you’d like to use (Decennial Census, ACS 1 or 5 year data)\nThe number of the table we want to look up (in this case table B02001 which contains information on the total population by race from the American Community Survey)\n\nThere are two main functions in tidycensus:\n\nget_decennial() is designed to download data from the decennial census which is conducted every ten years by law (e.g. 1990, 2000, 2010, and soon 2020) for the purposes of allocating political representation in the US.\nget_acs() is designed to download data from the American Community Survey, which is conducted continuously, and which focuses on describing the characteristics of the population for the years in between the decennial census.\n\n\nInitial Set Up\nLet’s start by loading a few packages that will help us with our work.\n\n\nCode\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(tidycensus)\n\n\nEach time you run a tidycensus function, R constructs a request to the Census API for data. This call needs to include your API key to authenticate your access to the data.The first time you use tidycensus on a new computer or R installation, you’ll need to set up your R environment to load your Census API key. You have two options -\n\nManually set the API key for use in your R script each time you use tidycensus:\n\n\n\nCode\nlibrary(tidycensus)\ncensus_api_key(\"936c96236b979ae522c6cf67edb51923dd391fb3\")\n\n\n\nStore your API key locally on your computer so that it does not need to be loaded manually.\n\n\n\nCode\nlibrary(tidycensus)\ncensus_api_key(\"936c96236b979ae522c6cf67edb51923dd391fb3\", install = TRUE)\n\n\nAgain, this looks largely the same. The first option simply sets the API key for use in a given rstudio session. Adding install=TRUE “installs” the API key for use any time you load RStudio, meaning you can load and use the package without having to use census_api_key(). I recommend installing your API key for two reasons - first, it is convenient to not have to copy and paste your API key into a new script or notebook; and second, if you share your code publicly, others will not have access to your api key because it is not included in your code (you might include a note in your documentation that lets other users know they will need to supply their own API key).\nWith that set up, we can move on…\n\n\nOur First Table Download\nTo familiarize yourself with tidycensus, have a look at the following code chunk. This gets the total population by race (Table B02001) for each census place in Illinois, and places that data into place_race_2021:\n\n\nCode\nplace_race_2021<-get_acs(geography = \"place\", state = \"Illinois\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\")\n\n\nOur call says create an object names “place_race_2021” and place into it the results of a function called get_acs (which is the function to download ACS data from the census API). Within the get_acs() function, we specify several attributes:\n\ngeography = “place” allows us to specify the geography which we want returned - in this case estimates for census places.\nstate = “Illinois” allows us to specify that we want to download data for the state of Illinois.\ntable = “B02001” allows us to specify which table we want downloaded - in this case, we want to look in table B02001. As an alternate to using table = we can download specific variables (if we don’t need all of the table’s data using variables =.)\nyear = 2021 allows us to specify which vintage of ACS data we are downloading.\nsurvey=“acs5” specifies that we want to look at 5-year ACS data with year being the most recent endpoint (e.g. if we specify year = 2021 and survey = \"acs5\", we are specifying 2017-2021 5-year ACS data).\noutput=“wide” specifies that we want our data to be formatted with columns corresponding to different racial identification categories.\n\nLet’s have a look at what was just downloaded:\n\n\nCode\nplace_race_2021 |> \n  slice_head(n = 10) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      B02001_004E\n      B02001_004M\n      B02001_005E\n      B02001_005M\n      B02001_006E\n      B02001_006M\n      B02001_007E\n      B02001_007M\n      B02001_008E\n      B02001_008M\n      B02001_009E\n      B02001_009M\n      B02001_010E\n      B02001_010M\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n0\n12\n0\n12\n0\n12\n21\n33\n28\n42\n0\n12\n28\n42\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n11\n11\n3\n6\n8\n9\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n223\n181\n3022\n668\n0\n25\n5547\n884\n3082\n891\n2230\n734\n852\n458\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n0\n12\n0\n12\n0\n12\n3\n7\n19\n20\n4\n6\n15\n20\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n0\n12\n6\n7\n0\n12\n0\n12\n38\n55\n2\n6\n36\n54\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n33\n32\n15\n11\n0\n12\n8\n12\n33\n29\n0\n12\n33\n29\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n  \n  \n  \n\n\n\n\nExamining the place_race_2019 object, we have a table that includes 22 columns:\n\nGEOID a unique numeric code for each census place\nNAME the name of each census place\nEstimates estimates (in this case population estimates by race), with each column representing a row in the data table we pulled up on data.census.gov.\nMargin of Error the margin of error associated with each estimate.\n\nThat wasn’t so hard! There are several benefits of downloading census data this way: 1. Once we know what tables or variables we are looking for, we don’t have to interact with data.census.gov very much (or at all). 2. R downloads the data and then it is immediately available for analysis as a formatted data frame. 3. We can easily modify existing calls to download data for other geographies.\nLet’s say we want to transition to looking at racial identification for counties in Illinois. How would we modify our previous code to match those specifications? - geography needs to be changed from “place” to “county” (find more information on specifying geography in the tidycensus documentation). - We need to add a new specification, state = \"IL\" (you’ll note that in the documentation on specifying a geography a column called “available by” which tells you what must be specified for a given summary level). - We need to change the object name that we’re putting our downloaded data into. Let’s call in “county_race_2021.\n\n\nCode\ncounty_race_2021<-get_acs(geography = \"county\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\")\n\ncounty_race_2021 |> \n  slice_head(n = 10) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      B02001_004E\n      B02001_004M\n      B02001_005E\n      B02001_005M\n      B02001_006E\n      B02001_006M\n      B02001_007E\n      B02001_007M\n      B02001_008E\n      B02001_008M\n      B02001_009E\n      B02001_009M\n      B02001_010E\n      B02001_010M\n    \n  \n  \n    17001\nAdams County, Illinois\n65878\nNA\n60689\n144\n2615\n343\n51\n33\n420\n113\n8\n11\n153\n61\n1942\n382\n513\n191\n1429\n283\n    17003\nAlexander County, Illinois\n5488\nNA\n3502\n39\n1836\n70\n4\n9\n1\n3\n22\n32\n0\n17\n123\n58\n15\n18\n108\n53\n    17005\nBond County, Illinois\n16804\nNA\n15120\n135\n1184\n141\n112\n44\n165\n155\n0\n19\n36\n25\n187\n114\n88\n71\n99\n86\n    17007\nBoone County, Illinois\n53592\nNA\n44895\n968\n1437\n293\n160\n137\n523\n87\n0\n28\n4200\n1041\n2377\n515\n1243\n394\n1134\n321\n    17009\nBrown County, Illinois\n6330\nNA\n4876\n202\n1174\n170\n39\n33\n3\n5\n0\n17\n135\n70\n103\n58\n59\n46\n44\n37\n    17011\nBureau County, Illinois\n33338\nNA\n30452\n237\n189\n66\n66\n29\n298\n51\n0\n25\n1307\n256\n1026\n172\n564\n146\n462\n77\n    17013\nCalhoun County, Illinois\n4537\nNA\n4357\n45\n29\n35\n14\n18\n2\n4\n0\n12\n0\n12\n135\n52\n61\n49\n74\n29\n    17015\nCarroll County, Illinois\n15586\nNA\n14248\n276\n468\n128\n44\n22\n119\n73\n0\n19\n79\n50\n628\n240\n223\n117\n405\n212\n    17017\nCass County, Illinois\n13058\nNA\n11075\n281\n451\n150\n108\n112\n89\n25\n91\n135\n545\n251\n699\n253\n523\n240\n176\n75\n    17019\nChampaign County, Illinois\n206583\nNA\n144522\n757\n26996\n1074\n170\n82\n22204\n477\n115\n98\n2280\n513\n10296\n1300\n3005\n571\n7291\n1166\n  \n  \n  \n\n\n\n\nBy making those few changes, you’ll note that the newly downloaded data in county_race_2021 now includes names of counties in the NAME column, instead of places as in our first call. There are data for each of the 102 counties in Illinois.\nPretty cool, huh?\n\n\nBuilding Table 2.1: Population Breakdown by Race\nLet’s start by downloading data on population by racial identification for counties in Illinois from the 2021 5-year American Community Survey, and placing the data into a data frame called “place_race_2021”. To do so, we need to know some basic information to construct our data request to the census.\nSince we are downloading ACS data, we will use the get_acs() function. At a minimum, get_acs() needs to know the geography we want it to download, the name(s) of the variables we’d like to retrieve, and the year we want the data for. Check out the basic usage of tidycensus for more information on constructing calls for different geographies.\ntidycensus can make calls for most geographies programmatically.\nSo far, we have used table = to specify that we wanted all variables downloaded for each table. In some cases, we may not need all of that information. Instead, we can use the variables = specification to download specific columns from the larger census table.\nTable B02001 which we’ve been looking at is hierarchical. The first row contains the total population of all races. The second row contains the estimate for those individuals who identify as white alone. The eighth row contains the estimate for individuals of two or more races.\nTable 2.1 in the Klosterman book presents racial identification for White Alone, Black Alone, Other, and Hispanic ethnicity. Putting aside for a minute the Hispanic ethnicity statistic, how would we download and construct analogous data for Champaign, Illinois? Let’s think this through:\n\n\n\n\n\n\n\n\n\nIndicator\nRow Number\nTidycensus Variable\nConstructed Variable\n\n\n\n\nWhite Alone\n2\nB02001_002\nNA\n\n\nBlack Alone\n3\nB02001_003\nNA\n\n\nOther\n?\nNA\nB02001_001-(B02001_002+B02001_003)\n\n\n\nTo construct this table, we only need the variables corresponding to the first, second, and third rows.\n\nWhite alone and black alone are the second and third rows in the data.census.gov table.\nWe can construct our “Other” variable by subtracting from the total population, the number of white alone and black alone individuals.\n\nLet’s see this in practice:\n\n\nCode\nplace_race_2021<-get_acs(geography = \"place\", state = \"IL\", variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\"), year=2021, survey=\"acs5\", output=\"wide\")\n\nplace_race_2021 |> \n  slice_head(n = 10) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n  \n  \n  \n\n\n\n\nNote that our original place_race_2021 data frame had 22 variables. The one we downloaded with our updated call only has 8 variables. What’s going on in variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\")? The table is B02001, but then we also specify the specific row in the census data table, which actually gets represented as a column when downloaded here.\n\nB02001_001 contains the total population estimate\nB02001_002 contains the white alone population\nB02001_003 contains the total black alone population\n\nWe could use the tidyverse command mutate() to create our other category and place into a new column in our dataset:\n\n\nCode\nplace_race_2021 |> \n  mutate(p_other = B02001_001E-(B02001_002E + B02001_003E)) |> \n  slice_head(n = 10) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      p_other\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n49\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n0\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n0\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n11\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n11874\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n0\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n22\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n44\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n89\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n0\n  \n  \n  \n\n\n\n\nWe might also want to rename our other two variables so that they are more readable, and might want to get rid of some of the other data we don’t need:\n\n\nCode\nplace_race_2021 |> \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E)) |>  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E) |> \n  select(Name, pop_tot, pop_white, pop_black, pop_other) |> \n  slice_head(n = 10) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_other\n    \n  \n  \n    Abingdon city, Illinois\n3586\n3474\n63\n49\n    Adair CDP, Illinois\n210\n192\n18\n0\n    Adams CDP, Illinois\n47\n47\n0\n0\n    Addieville village, Illinois\n359\n348\n0\n11\n    Addison village, Illinois\n35999\n22423\n1702\n11874\n    Adeline village, Illinois\n95\n94\n1\n0\n    Albany village, Illinois\n771\n739\n10\n22\n    Albers village, Illinois\n1431\n1387\n0\n44\n    Albion city, Illinois\n2122\n2033\n0\n89\n    Alden CDP, Illinois\n188\n188\n0\n0\n  \n  \n  \n\n\n\n\nThe above code does the following:\n\nUses mutate() to create a new variable called pop_other that subtracts from the total population the white alone population and black alone population.\nPipes this through to a function which you haven’t seen before called rename() which allows us to rename columns by name with the specification rename(new name = old name)\nWe then pipe this through and use select() to select only the name of the place, and our four named population columns - pop_total, pop_white, pop_black, pop_other.\n\nLet’s filter out just the data for Champaign, Illinois:\n\n\nCode\nplace_race_2021 |>  \n  filter(NAME == \"Champaign city, Illinois\") |>  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E)) |>  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E) |>  \n  select(Name, pop_tot, pop_white, pop_black, pop_other) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n18487\n  \n  \n  \n\n\n\n\nNote that if you look at the Name column that the name for each census place includes some additional information regarding what type of place it is (e.g. city, village, CDP). How would we filter for a place like Champaign if we didn’t know it was incorporated as a city?\nSome possible strategies:\n\nWe could search through all 1,369 records to find the specific record for Champaign\nWe could search manually but first sort alphabetically by NAME which would make it a little easier to find the value for Champaign. The arrange() command allows us to sort:\n\n\n\nCode\nplace_race_2021 |>\n  arrange(NAME) |> \n  slice_head(n = 20) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n    1700646\nAledo city, Illinois\n3860\n84\n3685\n105\n69\n36\n    1700659\nAlexander CDP, Illinois\n263\n134\n257\n134\n0\n12\n    1700672\nAlexis village, Illinois\n735\n166\n693\n160\n0\n12\n    1700685\nAlgonquin village, Illinois\n29869\n55\n24997\n659\n789\n362\n    1700737\nAlhambra village, Illinois\n627\n154\n609\n151\n0\n12\n    1700815\nAllendale village, Illinois\n709\n156\n633\n143\n0\n12\n    1700867\nAllenville village, Illinois\n111\n59\n111\n59\n0\n12\n    1700880\nAllerton village, Illinois\n226\n56\n197\n50\n19\n23\n    1700919\nAlma village, Illinois\n304\n77\n299\n76\n0\n12\n    1700958\nAlorton village, Illinois\n1864\n388\n33\n44\n1806\n388\n  \n  \n  \n\n\n\n\nIf we wanted to sort this list in the opposite direction, we can use the desc() command to do so:\n\n\nCode\nplace_race_2021 |>\n  arrange(desc(NAME)) |> \n  slice_head(n = 20) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1784220\nZion city, Illinois\n24660\n74\n12130\n1175\n6447\n768\n    1784155\nZeigler city, Illinois\n1563\n200\n1512\n194\n7\n12\n    1784038\nYorkville city, Illinois\n20503\n638\n16005\n1706\n2367\n1259\n    1783817\nYates City village, Illinois\n711\n154\n683\n154\n5\n7\n    1783765\nYale village, Illinois\n84\n41\n76\n39\n0\n12\n    1783739\nXenia village, Illinois\n337\n97\n317\n95\n0\n12\n    1783687\nWyoming city, Illinois\n1428\n192\n1360\n187\n18\n18\n    1783622\nWyanet village, Illinois\n1015\n172\n969\n173\n2\n5\n    1783518\nWorth village, Illinois\n10909\n38\n8807\n702\n275\n139\n    1783505\nWorden village, Illinois\n800\n173\n762\n169\n0\n12\n    1783349\nWoodstock city, Illinois\n25829\n345\n21197\n1104\n558\n258\n    1783336\nWoodson village, Illinois\n570\n157\n554\n154\n0\n12\n    1783245\nWoodridge village, Illinois\n34161\n131\n22766\n976\n4097\n980\n    1783206\nWoodlawn village, Illinois\n647\n126\n597\n119\n7\n9\n    1783102\nWoodland village, Illinois\n286\n115\n263\n100\n5\n14\n    1783063\nWoodhull village, Illinois\n782\n147\n768\n145\n5\n8\n    1783271\nWood River city, Illinois\n10325\n28\n9324\n401\n499\n306\n    1782985\nWood Dale city, Illinois\n14034\n24\n10351\n912\n326\n247\n    1782855\nWonder Lake village, Illinois\n4032\n403\n3381\n651\n0\n12\n    1782725\nWitt city, Illinois\n638\n164\n629\n155\n4\n24\n  \n  \n  \n\n\n\n\n\nWe could call upon a flexible tool for identifying matches within a character vector from the stringr package called str_detect():\n\n\n\nCode\nplace_race_2021 |>\n  filter(str_detect(NAME, \"Champaign\")) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1712385\nChampaign city, Illinois\n88343\n45\n54115\n1105\n15741\n1104\n  \n  \n  \n\n\n\n\nThis says, look in the NAME column, and find those values where part of the row matches “Champaign”.\nLet’s look again at our cleaned up observations for Champaign:\n\n\nCode\nplace_race_2021 |>  \n  filter(NAME == \"Champaign city, Illinois\") |>  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E)) |> \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E) |> \n  select(Name, pop_tot, pop_white, pop_black, pop_other) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n18487\n  \n  \n  \n\n\n\n\nThere’s a fairly sizable population that falls into the “pop_other” category. We could not worry about this, but it probably makes sense to see if there’s another relatively large population group that we should itemize for in Champaign. Let’s go back and download the whole table (and overwrite our existing one) to be able to take a closer look:\n\n\nCode\nplace_race_2021<-get_acs(geography = \"place\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\")\n\nplace_race_2021 |> \n  filter(NAME == \"Champaign city, Illinois\") |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      B02001_004E\n      B02001_004M\n      B02001_005E\n      B02001_005M\n      B02001_006E\n      B02001_006M\n      B02001_007E\n      B02001_007M\n      B02001_008E\n      B02001_008M\n      B02001_009E\n      B02001_009M\n      B02001_010E\n      B02001_010M\n    \n  \n  \n    1712385\nChampaign city, Illinois\n88343\n45\n54115\n1105\n15741\n1104\n30\n29\n13310\n750\n24\n36\n1375\n464\n3748\n777\n950\n266\n2798\n684\n  \n  \n  \n\n\n\n\n\nTotal Population 88,343 (B02001_001E)\nWhite Population 54,115 (B02001_002E)\nBlack Population 15,741 (B02001_003E)\n\nThere’s a fairly sizable number of people in B02001_005E (13,310). What racial group is represented there? Given that large a number, let’s add this group into the racial summary table we’re working to develop:\n\n\nCode\nplace_race_2021 |> \n  filter(NAME == \"Champaign city, Illinois\") |>  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |> \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |> \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other) |> \n  gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n  \n  \n  \n\n\n\n\nThe missing group with a fairly large population in this case is the Asian population. By adding this group, we lower the proportion of the total population in the “other” category substantially. This now looks a bit more presentable (and informative). There’s no hard and fast rule for which groups to represent. However, it’s important to note that it is often beneficial to make use of an “other” category in instances where you have small numbers (unless there’s detail there which your audience needs to see).\nLet’s go ahead and write this cleaned information back into place_race_2021:\n\n\nCode\nplace_race_2021<-place_race_2021 |> \n  filter(NAME == \"Champaign city, Illinois\") |>  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |>  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |>  \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other)\n\nplace_race_2021 |> gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n  \n  \n  \n\n\n\n\nFrom here, we could do a few more things:\n\nConstruct an analogous table from our county data for Champaign County, Illinois.\nConstruct percentages for the values we’ve found, which can make comparison much easier.\n\n\n\nConstruct County Data\nPerform the same table cleaning and re-labeling for our county data that you did for our data for Champaign, and overwrite the existing data with this new data.\n\n\nCode\ncounty_race_2021 <-get_acs(geography = \"county\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\") |> \n  filter(NAME == \"Champaign County, Illinois\") |>  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |>  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |>  \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other)\n\ncounty_race_2021 |> gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n    \n  \n  \n    Champaign County, Illinois\n206583\n144522\n26996\n22204\n12861\n  \n  \n  \n\n\n\n\n\n\nConstruct Percentages\nNow that we have “clean” data for City of Champaign and Champaign County, calculate percentages of the total population for each racial category and add these as new columns in their respective data tables:\n\n\nCode\nplace_race_2021<-\n  place_race_2021 |>  \n  mutate(p_white = (pop_white/pop_tot)*100,\n         p_black = (pop_black/pop_tot)*100,\n         p_asian = (pop_asian / pop_tot)*100,\n         p_other = (pop_other/pop_tot)*100)\n\ncounty_race_2021<-\n  county_race_2021 %>% \n  mutate(p_white = (pop_white/pop_tot)*100,\n         p_black = (pop_black/pop_tot)*100,\n         p_asian = (pop_asian / pop_tot)*100,\n         p_other = (pop_other/pop_tot)*100)\n\n\nFrom here, we could copy and paste out values to construct a final table, or could combine values to create our own table:\n\n\nCode\nrace_2021<-bind_rows(place_race_2021, county_race_2021)\n\nrace_2021 |> gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n      p_white\n      p_black\n      p_asian\n      p_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n61.25556\n17.81805\n15.06628\n5.860113\n    Champaign County, Illinois\n206583\n144522\n26996\n22204\n12861\n69.95832\n13.06787\n10.74822\n6.225585\n  \n  \n  \n\n\n\n\nThe bind_rows() command allows us to take the rows in two separate data tables and combine them together based upon them having common column names (the columns don’t have to be in the same order). There’s also bind_columns() for the analogous operation on columns.\nThis puts us in a good position to make a final “pretty” table:\nFirst, for raw numbers:\n\n\nCode\nrace_2021 |> \n  select(Name, pop_white, pop_black, pop_asian, pop_other, pop_tot) |> \n  gt() |> \n  fmt_number(2:6, decimals = 0) |> \n  cols_label(\n    pop_white = \"White\",\n    pop_black = \"Black\",\n    pop_asian = \"Asian\",\n    pop_other = \"Other\",\n    pop_tot = \"Total Population\"\n    )\n\n\n\n\n\n\n  \n  \n    \n      Name\n      White\n      Black\n      Asian\n      Other\n      Total Population\n    \n  \n  \n    Champaign city, Illinois\n54,115\n15,741\n13,310\n5,177\n88,343\n    Champaign County, Illinois\n144,522\n26,996\n22,204\n12,861\n206,583\n  \n  \n  \n\n\n\n\nSecond, for percentages:\n\n\nCode\nrace_2021 |> \n  select(Name, p_white, p_black, p_asian, p_other) |> \n  gt() |> \n  fmt_percent(2:5, decimals = 1, scale_values = FALSE) |> \n  cols_label(\n    p_white = \"White\",\n    p_black = \"Black\",\n    p_asian = \"Asian\",\n    p_other = \"Other\"\n    )\n\n\n\n\n\n\n  \n  \n    \n      Name\n      White\n      Black\n      Asian\n      Other\n    \n  \n  \n    Champaign city, Illinois\n61.3%\n17.8%\n15.1%\n5.9%\n    Champaign County, Illinois\n70.0%\n13.1%\n10.7%\n6.2%\n  \n  \n  \n\n\n\n\nThe Klosterman book includes one more comparison - for the region. Champaign’s region includes Champaign, Ford, and Piatt Counties. We can construct regional numbers from the same county data which we used before. Unfortunately, we overwrote that existing data when we created our “clean” data just for Champaign County. Let’s download this again and construct our regional comparison:\n\n\nCode\nregion_race_2021<- get_acs(geography = \"county\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\") |>  \n  filter(NAME %in% c(\"Champaign County, Illinois\", \"Ford County, Illinois\", \"Piatt County, Illinois\")) |> summarise(B02001_001E = sum(B02001_001E),\n            B02001_002E = sum(B02001_002E),\n            B02001_003E = sum(B02001_003E),\n            B02001_005E = sum(B02001_005E)) |> \n  mutate(NAME = \"Region\") |>  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |>  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |>  \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other) |>  \n  mutate(\n  p_white = (pop_white / pop_tot)*100,\n  p_black = (pop_black / pop_tot)*100,\n  p_asian = (pop_asian / pop_tot)*100,\n  p_other = (pop_other / pop_tot)*100)\n\n\nNote that the above code uses pipes to take all of the steps we had done above - downloading the data, querying out the counties of interest (remember that %in% allows us to match on multiple elements of a list), calculating values, to put them into one step. We now have a data processing pipeline which we only need to make minor modifications to in order to create a summary for any census geography. If you are sure that you have robust code present here, this will work just fine. It is perfectly acceptable to break this code into multiple chunks and then to overwrite once you are certain that the code will give you a desired outcome.\nAlso, did you catch how we combined information for Champaign, Ford, and Piatt counties into one “Region”? The following lines of code handle these steps:\n\n\nCode\nfilter(NAME %in% c(\"Champaign County, Illinois\", \"Ford County, Illinois\", \"Piatt County, Illinois\")) |>  \n  summarise(B02001_001E = sum(B02001_001E),\n            B02001_002E = sum(B02001_002E),\n            B02001_003E = sum(B02001_003E),\n            B02001_005E = sum(B02001_005E)) |> \n  mutate(NAME = \"Region\")\n\n\nWhat’s going on here?\n\nThe filter() section filters the data to the three rows in the dataset where the NAME field is equal to Champaign, Ford, or Piatt county.\nThe summarise() section is a little different than what we’ve seen before. In the past we’ve used this with group_by() in order to develop summaries for groups. Because we don’t have a group_by() clause here, R, summarizes all of the observations as one group. In this case, because we’re dealing with count data, we simply sum up all of the observations in a summary table where I have named the columns with the same names as the raw data had.\n\nFrom there, we can re-use the code we had already created to process the place and county data. With these things done, we can add our region values into our race_2021 summary table:\n\n\nCode\nrace_2021<-bind_rows(race_2021, region_race_2021)\nrace_2021 |> gt()\n\n\n\n\n\n\n  \n  \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n      p_white\n      p_black\n      p_asian\n      p_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n61.25556\n17.81805\n15.066276\n5.860113\n    Champaign County, Illinois\n206583\n144522\n26996\n22204\n12861\n69.95832\n13.06787\n10.748222\n6.225585\n    Region\n236836\n173216\n27219\n22294\n14107\n73.13753\n11.49276\n9.413265\n5.956442\n  \n  \n  \n\n\n\n\nWhile the values here are transposed, we have created code which creates a table similar to table 2.1 in the Klosterman book."
  },
  {
    "objectID": "assignments/labs/05_census.html#closing-thoughts",
    "href": "assignments/labs/05_census.html#closing-thoughts",
    "title": "Population and the Census",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nThis tutorial has walked you through the steps necessary to start making basic calls to the Census API via tidycensus. There’s a lot more that you can do with tidycensus that we didn’t cover here, including accessing decennial census data, downloading census geometries along with data, and having tidycensus do some reshaping of your data. We will continue to explore these features as we add complexity in future labs."
  }
]